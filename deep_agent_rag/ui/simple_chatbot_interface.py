"""
Simple Chatbot Interface
ç°¡å–®çš„èŠå¤©æ©Ÿå™¨äººç•Œé¢ï¼Œä¸åŒ…å« RAG å’Œ Deep AI Agent åŠŸèƒ½
ç´”ç²¹çš„å°è©±å¼èŠå¤©æ©Ÿå™¨äºº
åŒ…å«å…§å®¹éæ¿¾ Guardrails åŠŸèƒ½
"""
import gradio as gr
from typing import List, Dict, Any
import time
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnableLambda
import jieba

from ..utils.llm_utils import get_llm_type, is_using_local_llm, get_llm


# ==================== Guardrails é…ç½® ====================
# æ•æ„Ÿé—œéµå­—åå–®
BLOCKED_KEYWORDS = [
    "ä¼Šæ–¯è˜­æ•™",
    "é˜¿æ‹‰",
    "å›æ•™å¾’",
    "é»˜ç½•é»˜å¾·",
    "Islam",
    "Allah",
    "Muslim",
    "Muhammad"
]

# æ””æˆªé–€æª»ï¼š5% çš„è©å½™å¯†åº¦
KEYWORD_DENSITY_THRESHOLD = 0.05

# é è¨­æ””æˆªè¨Šæ¯
DEFAULT_BLOCKED_MESSAGE = "æŠ±æ­‰ï¼Œæ‚¨çš„å•é¡ŒåŒ…å«æ•æ„Ÿå…§å®¹ï¼Œç„¡æ³•å›ç­”ã€‚è«‹æ›å€‹è©±é¡Œæˆ–é‡æ–°è¡¨è¿°æ‚¨çš„å•é¡Œã€‚"

# åˆå§‹åŒ– jieba è‡ªå®šç¾©è©å…¸ï¼ˆç¢ºä¿æº–ç¢ºè­˜åˆ¥æ•æ„Ÿè©ï¼‰
def _init_jieba_custom_dict():
    """åˆå§‹åŒ– jieba è‡ªå®šç¾©è©å…¸ï¼Œæ·»åŠ æ•æ„Ÿé—œéµå­—ä»¥æé«˜è­˜åˆ¥æº–ç¢ºåº¦"""
    for keyword in BLOCKED_KEYWORDS:
        jieba.add_word(keyword, freq=10000, tag='sensitive')

# åœ¨æ¨¡çµ„è¼‰å…¥æ™‚åˆå§‹åŒ–
_init_jieba_custom_dict()


def check_content_guardrails(text: str) -> tuple[bool, float]:
    """
    æª¢æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æ•æ„Ÿå…§å®¹
    
    ä½¿ç”¨ jieba é€²è¡Œæ–·è©ï¼Œè¨ˆç®—æ•æ„Ÿè©å½™å¯†åº¦
    æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ï¼ˆä¸å€åˆ†å¤§å°å¯«ï¼‰
    
    Args:
        text: è¦æª¢æŸ¥çš„æ–‡æœ¬
    
    Returns:
        tuple[bool, float]: (æ˜¯å¦æ‡‰è©²æ””æˆª, é—œéµå­—å¯†åº¦)
    """
    if not text or not text.strip():
        return False, 0.0
    
    # ä½¿ç”¨ jieba é€²è¡Œæ–·è©
    words = list(jieba.cut(text))
    total_words = len(words)
    
    if total_words == 0:
        return False, 0.0
    
    # å»ºç«‹å°å¯«æ•æ„Ÿè©é›†åˆä»¥ä¾¿å¿«é€Ÿæ¯”å°
    blocked_keywords_lower = {k.lower() for k in BLOCKED_KEYWORDS}
    
    # è¨ˆç®—æ•æ„Ÿè©æ•¸é‡
    sensitive_word_count = 0
    for word in words:
        # ç§»é™¤ç©ºç™½ä¸¦è½‰ç‚ºå°å¯«é€²è¡Œæ¯”å°
        clean_word = word.strip().lower()
        if clean_word and clean_word in blocked_keywords_lower:
            sensitive_word_count += 1
    
    # è¨ˆç®—é—œéµå­—å¯†åº¦
    keyword_density = sensitive_word_count / total_words
    
    # åˆ¤æ–·æ˜¯å¦è¶…éé–€æª»
    should_block = keyword_density >= KEYWORD_DENSITY_THRESHOLD
    
    return should_block, keyword_density


def guardrail_filter(response: str) -> str:
    """
    Guardrail éæ¿¾å‡½æ•¸ - ç”¨æ–¼ LangChain RunnableLambda
    
    Args:
        response: LLM çš„å›æ‡‰æ–‡æœ¬
    
    Returns:
        str: éæ¿¾å¾Œçš„æ–‡æœ¬ï¼ˆå¦‚æœè¶…éé–€æª»å‰‡è¿”å›é è¨­è¨Šæ¯ï¼‰
    """
    should_block, density = check_content_guardrails(response)
    
    if should_block:
        print(f"ğŸš« Guardrails æ””æˆªï¼šé—œéµå­—å¯†åº¦ {density:.2%} è¶…éé–€æª» {KEYWORD_DENSITY_THRESHOLD:.2%}")
        return DEFAULT_BLOCKED_MESSAGE
    else:
        print(f"ğŸŸ¢ Guardrails é€šéï¼šé—œéµå­—å¯†åº¦ {density:.2%} ä½æ–¼é–€æª» {KEYWORD_DENSITY_THRESHOLD:.2%}")
        
    return response


# å‰µå»º LangChain RunnableLambdaï¼ˆç”¨æ–¼ä¸²æ¥åœ¨ Chain æœ«ç«¯ï¼‰
guardrail_runnable = RunnableLambda(guardrail_filter)


def chat_with_llm_streaming(
    message: str,
    history: List[Dict[str, str]],
    system_prompt: str = "ä½ æ˜¯ä¸€å€‹æœ‰å¹«åŠ©çš„AIåŠ©æ‰‹ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚"
):
    """
    èˆ‡ LLM é€²è¡Œæµå¼å°è©±ï¼ˆé€å­—é¡¯ç¤ºï¼‰
    
    Args:
        message: ç”¨æˆ¶è¼¸å…¥çš„æ¶ˆæ¯
        history: å°è©±æ­·å² (å­—å…¸æ ¼å¼ï¼š[{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}, ...])
        system_prompt: ç³»çµ±æç¤ºè©
    
    Yields:
        List[Dict[str, str]]: æ›´æ–°ä¸­çš„æ­·å²è¨˜éŒ„
    """
    if not message or not message.strip():
        yield history
        return
    
    # ==================== ç«‹å³é¡¯ç¤ºç”¨æˆ¶æ¶ˆæ¯ ====================
    # å…ˆå°‡ç”¨æˆ¶æ¶ˆæ¯æ·»åŠ åˆ°æ­·å²ä¸¦ç«‹å³é¡¯ç¤º
    new_history = history + [{"role": "user", "content": message}]
    yield new_history
    
    try:
        # ç²å– LLM
        llm = get_llm()
        
        # æ§‹å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [SystemMessage(content=system_prompt)]
        
        # æ·»åŠ å°è©±æ­·å²
        for msg in history:
            if msg.get("role") == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg.get("role") == "assistant":
                messages.append(AIMessage(content=msg["content"]))
        
        # æ·»åŠ ç•¶å‰ç”¨æˆ¶æ¶ˆæ¯
        messages.append(HumanMessage(content=message))
        
        # èª¿ç”¨ LLM ç²å–å®Œæ•´å›æ‡‰
        response = llm.invoke(messages)
        full_response = response.content
        
        # ==================== æ‡‰ç”¨ Guardrails éæ¿¾ ====================
        # ä½¿ç”¨ RunnableLambda é€²è¡Œå…§å®¹éæ¿¾
        filtered_response = guardrail_runnable.invoke(full_response)
        
        # æ·»åŠ ç©ºçš„åŠ©æ‰‹å›æ‡‰ï¼ˆå°‡é€æ­¥å¡«å……ï¼‰
        new_history.append({"role": "assistant", "content": ""})
        
        # æŒ‰å­—ç¬¦é€æ­¥é¡¯ç¤ºï¼ˆä½¿ç”¨éæ¿¾å¾Œçš„å›æ‡‰ï¼‰
        for i in range(len(filtered_response)):
            # æ›´æ–°æœ€å¾Œä¸€æ¢æ­·å²è¨˜éŒ„çš„æ©Ÿå™¨äººå›æ‡‰
            new_history[-1] = {"role": "assistant", "content": filtered_response[:i+1]}
            yield new_history
            time.sleep(0.01)  # 10ms å»¶é²ï¼Œå‰µé€ æ‰“å­—æ•ˆæœ
        
        # ç¢ºä¿å®Œæ•´é¡¯ç¤º
        new_history[-1] = {"role": "assistant", "content": filtered_response}
        yield new_history
    
    except Exception as e:
        error_msg = f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        print(f"èŠå¤©éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        
        # æ·»åŠ éŒ¯èª¤å›æ‡‰åˆ°å·²é¡¯ç¤ºç”¨æˆ¶æ¶ˆæ¯çš„æ­·å²
        new_history.append({"role": "assistant", "content": error_msg})
        yield new_history


def get_llm_status() -> str:
    """ç²å–ç•¶å‰ LLM ç‹€æ…‹ä¿¡æ¯"""
    if is_using_local_llm():
        return "âš ï¸ **ç•¶å‰ä½¿ç”¨ï¼šæœ¬åœ° MLX æ¨¡å‹ (Qwen2.5)**\n\næœ¬åœ°æ¨¡å‹è™•ç†é€Ÿåº¦å¯èƒ½è¼ƒæ…¢ï¼Œè«‹è€å¿ƒç­‰å¾…ã€‚"
    else:
        llm_type = get_llm_type()
        if llm_type == "groq":
            return "âœ… **ç•¶å‰ä½¿ç”¨ï¼šGroq API (é«˜é€Ÿé›²ç«¯æ¨¡å‹)**"
        else:
            return "â„¹ï¸ **ç•¶å‰ä½¿ç”¨ï¼šæœ¬åœ° MLX æ¨¡å‹ (Qwen2.5)**"


def create_simple_chatbot_interface():
    """
    å‰µå»ºç°¡å–®èŠå¤©æ©Ÿå™¨äººç•Œé¢
    ç´”ç²¹çš„å°è©±å¼èŠå¤©ï¼Œä¸åŒ…å« RAG æˆ–å…¶ä»–è¤‡é›œåŠŸèƒ½
    """
    with gr.Blocks(title="Simple Chatbot") as demo:
        # æ¨™é¡Œ
        gr.Markdown(
            """
            <div style="text-align: center; padding: 20px;">
                <h1>ğŸ’¬ Simple Chatbot</h1>
                <p style="font-size: 16px; color: #666;">
                    ç°¡å–®çš„å°è©±å¼èŠå¤©æ©Ÿå™¨äºº - ç´”ç²¹çš„ AI å°è©±é«”é©—
                </p>
                <p style="font-size: 14px; color: #888;">
                    ä¸åŒ…å« RAGã€Deep AI Agent ç­‰è¤‡é›œåŠŸèƒ½ï¼Œåªå°ˆæ³¨æ–¼è‡ªç„¶å°è©±
                </p>
            </div>
            """
        )
        
        # LLM ç‹€æ…‹é¡¯ç¤º
        llm_status = gr.Markdown(
            value=get_llm_status(),
            elem_classes=["warning-box"]
        )
        
        # ç³»çµ±æç¤ºè©è¨­å®šï¼ˆå¯é¸ï¼‰
        with gr.Accordion("âš™ï¸ é€²éšè¨­å®š", open=False):
            system_prompt = gr.Textbox(
                label="ç³»çµ±æç¤ºè© (System Prompt)",
                value="ä½ æ˜¯ä¸€å€‹æœ‰å¹«åŠ©çš„AIåŠ©æ‰‹ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚",
                lines=3,
                placeholder="è¨­å®š AI çš„è§’è‰²å’Œè¡Œç‚ºæ–¹å¼..."
            )
            
            gr.Markdown(
                """
                **æç¤ºè©ç¯„ä¾‹ï¼š**
                - å°ˆæ¥­åŠ©æ‰‹ï¼šã€Œä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æŠ€è¡“é¡§å•ï¼Œæ“…é•·è§£é‡‹è¤‡é›œçš„æŠ€è¡“æ¦‚å¿µã€‚ã€
                - å‰µæ„å¯«ä½œï¼šã€Œä½ æ˜¯ä¸€ä½å¯Œæœ‰å‰µæ„çš„ä½œå®¶ï¼Œæ“…é•·å¯«ä½œæ•…äº‹å’Œè©©æ­Œã€‚ã€
                - å­¸ç¿’è¼”å°ï¼šã€Œä½ æ˜¯ä¸€ä½è€å¿ƒçš„è€å¸«ï¼Œæ“…é•·ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹è¤‡é›œçš„æ¦‚å¿µã€‚ã€
                """
            )
        
        # Guardrails è¨­å®šé¡¯ç¤º
        with gr.Accordion("ğŸ›¡ï¸ å…§å®¹éæ¿¾ Guardrails", open=False):
            gr.Markdown(
                f"""
                **Guardrails å·²å•Ÿç”¨** âœ…
                
                æœ¬ç³»çµ±ä½¿ç”¨ `jieba` é€²è¡Œä¸­è‹±æ–‡æ–·è©èˆ‡å…§å®¹éæ¿¾ï¼š
                
                - **æ””æˆªé–€æª»**ï¼š{KEYWORD_DENSITY_THRESHOLD:.1%} é—œéµå­—å¯†åº¦
                - **éæ¿¾æ©Ÿåˆ¶**ï¼šæ•æ„Ÿè©æ•¸ / ç¸½è©æ•¸ â‰¥ {KEYWORD_DENSITY_THRESHOLD:.1%}
                - **è™•ç†æ–¹å¼**ï¼šè¶…éé–€æª»æ™‚ï¼Œå›æ‡‰å°‡è¢«æ›¿æ›ç‚ºé è¨­è¨Šæ¯
                - **æŠ€è¡“å¯¦ç¾**ï¼šä½¿ç”¨ LangChain `RunnableLambda` ä¸²æ¥åœ¨ Chain æœ«ç«¯
                - **æ”¯æ´èªè¨€**ï¼šç¹é«”ä¸­æ–‡ã€è‹±æ–‡ï¼ˆä¸å€åˆ†å¤§å°å¯«ï¼‰
                
                **ç•¶å‰éæ¿¾é—œéµå­—åˆ—è¡¨**ï¼š
                {', '.join([f'ã€Œ{kw}ã€' for kw in BLOCKED_KEYWORDS])}
                
                â„¹ï¸ ç³»çµ±æœƒè‡ªå‹•åˆ†æ AI å›æ‡‰å…§å®¹ï¼Œç¢ºä¿ç¬¦åˆä½¿ç”¨è¦ç¯„ã€‚
                """
            )
        
        # èŠå¤©ç•Œé¢ï¼ˆGradio 5.x+ é»˜èªä½¿ç”¨å­—å…¸æ ¼å¼ï¼‰
        chatbot = gr.Chatbot(
            label="å°è©±è¨˜éŒ„",
            height=500,
            show_label=True
        )
        
        # è¼¸å…¥å€åŸŸ
        msg = gr.Textbox(
            label="è¨Šæ¯",
            placeholder="åœ¨é€™è£¡è¼¸å…¥æ‚¨çš„è¨Šæ¯...",
            lines=2,
            show_label=False
        )
        
        # æ§åˆ¶æŒ‰éˆ•
        with gr.Row():
            submit_btn = gr.Button("ğŸ“¤ ç™¼é€", variant="primary")
            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å°è©±", variant="secondary")
            refresh_status_btn = gr.Button("ğŸ”„ æ›´æ–°ç‹€æ…‹", variant="secondary")
        
        # ç¤ºä¾‹å•é¡Œ
        gr.Examples(
            examples=[
                "ä½ å¥½ï¼è«‹ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
                "è«‹å¹«æˆ‘è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ",
                "èƒ½çµ¦æˆ‘ä¸€äº›å­¸ç¿’ Python çš„å»ºè­°å—ï¼Ÿ",
                "è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹é‡å­è¨ˆç®—ã€‚",
                "å¯«ä¸€é¦–é—œæ–¼æ˜¥å¤©çš„çŸ­è©©ã€‚"
            ],
            inputs=msg,
            label="ğŸ’¡ å¿«é€Ÿè©¦ç”¨ç¯„ä¾‹"
        )
        
        # äº‹ä»¶ç¶å®š
        def clear_chat():
            """æ¸…é™¤å°è©±"""
            return [], ""
        
        def refresh_status():
            """æ›´æ–° LLM ç‹€æ…‹"""
            return get_llm_status()
        
        # ç™¼é€æ¶ˆæ¯äº‹ä»¶
        msg.submit(
            fn=chat_with_llm_streaming,
            inputs=[msg, chatbot, system_prompt],
            outputs=[chatbot],
            queue=True
        ).then(
            fn=lambda: "",
            outputs=[msg],
            queue=False
        )
        
        submit_btn.click(
            fn=chat_with_llm_streaming,
            inputs=[msg, chatbot, system_prompt],
            outputs=[chatbot],
            queue=True
        ).then(
            fn=lambda: "",
            outputs=[msg],
            queue=False
        )
        
        clear_btn.click(
            fn=clear_chat,
            outputs=[chatbot, msg],
            queue=False
        )
        
        refresh_status_btn.click(
            fn=refresh_status,
            outputs=[llm_status],
            queue=False
        )
        
        # é è…³
        gr.Markdown(
            """
            ---
            ### ğŸ“ ä½¿ç”¨èªªæ˜
            
            1. **é–‹å§‹å°è©±**ï¼šåœ¨è¼¸å…¥æ¡†ä¸­è¼¸å…¥è¨Šæ¯ï¼Œé»æ“Šã€Œç™¼é€ã€æˆ–æŒ‰ Enter
            2. **ç³»çµ±æç¤ºè©**ï¼šå±•é–‹ã€Œé€²éšè¨­å®šã€å¯è‡ªè¨‚ AI çš„è§’è‰²å’Œè¡Œç‚º
            3. **æ¸…é™¤å°è©±**ï¼šé»æ“Šã€Œæ¸…é™¤å°è©±ã€å¯é‡æ–°é–‹å§‹
            4. **å¿«é€Ÿè©¦ç”¨**ï¼šé»æ“Šä¸‹æ–¹çš„ç¯„ä¾‹å•é¡Œå¿«é€Ÿæ¸¬è©¦
            
            **ç‰¹è‰²åŠŸèƒ½ï¼š**
            - ğŸ¯ ç´”ç²¹çš„å°è©±é«”é©—ï¼Œç„¡è¤‡é›œåŠŸèƒ½
            - ğŸ’« æµå¼è¼¸å‡ºï¼Œé€å­—é¡¯ç¤ºå›æ‡‰
            - ğŸ”§ å¯è‡ªè¨‚ç³»çµ±æç¤ºè©
            - ğŸ“ ä¿ç•™å®Œæ•´å°è©±æ­·å²
            - ğŸš€ æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œé›²ç«¯ API
            - ğŸ›¡ï¸ å…§å»º Guardrails å…§å®¹éæ¿¾æ©Ÿåˆ¶ï¼ˆä½¿ç”¨ jieba ä¸­æ–‡æ–·è©ï¼‰
            """
        )
    
    return demo


# å¦‚æœç›´æ¥åŸ·è¡Œæ­¤æ–‡ä»¶ï¼Œå•Ÿå‹•ç•Œé¢
if __name__ == "__main__":
    demo = create_simple_chatbot_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7861,
        share=False,
        show_error=True
    )

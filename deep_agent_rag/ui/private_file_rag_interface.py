# deep_agent_rag/ui/private_file_rag_interface.py

import gradio as gr
import re
import json
import os
import time

from ..rag.private_file_rag import get_private_rag_instance, reset_private_rag_instance
# Assuming is_using_local_llm might be used for warnings/status, similar to email_interface
# from ..utils.llm_utils import is_using_local_llm 

# Agent log path for debugging (if needed)
log_path = "/Users/matthuang/Desktop/Deep_Agentic_AI_Tool/.cursor/debug.log"

def _create_private_file_rag_interface():
    """å‰µå»ºç§æœ‰æ–‡ä»¶ RAG ç•Œé¢ï¼ˆå°è©±å¼ Chatbotï¼‰"""
    gr.Markdown(
        """
        ### ğŸ“š ç§æœ‰æ–‡ä»¶ RAG å°è©±ç³»çµ±
        
        ä¸Šå‚³æ‚¨çš„ç§æœ‰æ–‡ä»¶ï¼ˆPDFã€DOCXã€TXTï¼‰ï¼Œç³»çµ±æœƒè‡ªå‹•å»ºç«‹ RAG çŸ¥è­˜åº«ï¼Œè®“ AI å¯ä»¥å›ç­”é—œæ–¼é€™äº›æ–‡ä»¶çš„å•é¡Œã€‚
        æ”¯æŒå¤šè¼ªå°è©±ï¼ŒAI æœƒè¨˜ä½ä¹‹å‰çš„å°è©±å…§å®¹ï¼Œæä¾›æ›´é€£è²«çš„å›ç­”ã€‚
        
        **ä½¿ç”¨æ–¹å¼ï¼š**
        1. ä¸Šå‚³ä¸€å€‹æˆ–å¤šå€‹æ–‡ä»¶ï¼ˆPDFã€DOCXã€TXTï¼‰
        2. é»æ“Šã€Œè™•ç†æ–‡ä»¶ã€æŒ‰éˆ•ï¼Œç³»çµ±æœƒè‡ªå‹•è™•ç†æ–‡ä»¶ä¸¦å»ºç«‹ RAG ç³»çµ±
        3. åœ¨å°è©±æ¡†ä¸­è¼¸å…¥æ‚¨çš„å•é¡Œï¼ŒæŒ‰ Enter æˆ–é»æ“Šã€Œç™¼é€ã€æŒ‰éˆ•
        4. AI æœƒåŸºæ–¼ä¸Šå‚³çš„æ–‡ä»¶å›ç­”å•é¡Œï¼Œæ”¯æŒå¤šè¼ªå°è©±
        
        **åŠŸèƒ½ç‰¹è‰²ï¼š**
        - ğŸ’¬ **å°è©±å¼ç•Œé¢** ï¼šé¡ä¼¼ Gemini çš„å°è©±é«”é©—ï¼Œæ”¯æŒå¤šè¼ªå°è©±
        - ğŸ“„ æ”¯æŒå¤šç¨®æ–‡ä»¶æ ¼å¼ï¼šPDFã€DOCXã€TXT
        - ğŸ” ä½¿ç”¨æ··åˆæœå°‹ï¼ˆBM25 + å‘é‡æª¢ç´¢ï¼‰æå‡æª¢ç´¢æº–ç¢ºåº¦
        - ğŸ¯ å¯é¸é‡æ’åºåŠŸèƒ½ï¼Œé€²ä¸€æ­¥å„ªåŒ–çµæœ
        - ğŸ§  æ”¯æŒèªç¾©åˆ†å¡Šï¼Œä¿æŒèªç¾©å®Œæ•´æ€§
        - ğŸŒ è‡ªå‹•æª¢æ¸¬æ–‡æª”é¡å‹ä¸¦èª¿æ•´å›ç­”é¢¨æ ¼
        
        **LLM ä½¿ç”¨ç­–ç•¥ï¼š**
        - ğŸ¥‡ **å„ªå…ˆä½¿ç”¨ Groq API** ï¼šå¦‚æœé…ç½®äº† API é‡‘é‘°ï¼Œå„ªå…ˆä½¿ç”¨ Groqï¼ˆé€Ÿåº¦å¿«ã€è³ªé‡é«˜ï¼‰
        - ğŸ¥ˆ **å…¶æ¬¡ä½¿ç”¨ Ollama** ï¼šå¦‚æœ Groq ä¸å¯ç”¨ï¼Œè‡ªå‹•åˆ‡æ›åˆ° Ollama æœ¬åœ°æ¨¡å‹
        - ğŸ¥‰ **æœ€å¾Œä½¿ç”¨ MLX** ï¼šå¦‚æœå‰å…©è€…éƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨ MLX æœ¬åœ°æ¨¡å‹ä½œç‚ºå‚™é¸
        - ğŸ’¡ **è‡ªå‹•åˆ‡æ›** ï¼šç³»çµ±æœƒæ ¹æ“š API é¡åº¦ã€æœå‹™ç‹€æ…‹ç­‰è‡ªå‹•é¸æ“‡æœ€åˆé©çš„ LLM
        
        **æ³¨æ„ï¼š** æ­¤åŠŸèƒ½éœ€è¦ Learn_RAG é …ç›®åœ¨æ­£ç¢ºçš„ä½ç½®
        """
    )
    
    # å°è©±æ­·å²ç‹€æ…‹
    chat_history = gr.State(value=[])
    
    with gr.Row():
        # å·¦å´ï¼šæ–‡ä»¶ä¸Šå‚³å’Œè¨­ç½®
        with gr.Column(scale=1):
            # æ–‡ä»¶ä¸Šå‚³å€åŸŸ
            file_upload = gr.File(
                label="ğŸ“ ä¸Šå‚³æ–‡ä»¶ï¼ˆPDFã€DOCXã€TXTï¼‰",
                file_count="multiple",
                file_types=[ ".pdf", ".docx", ".doc", ".txt"]
            )
            
            # è™•ç†æŒ‰éˆ•
            with gr.Row():
                process_btn = gr.Button("ğŸ“ è™•ç†æ–‡ä»¶", variant="primary", scale=1)
                clear_files_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰", variant="secondary", scale=1)
            
            # è™•ç†ç‹€æ…‹
            process_status = gr.Textbox(
                label="ğŸ“Š è™•ç†ç‹€æ…‹",
                value="ç­‰å¾…ä¸Šå‚³æ–‡ä»¶...",
                interactive=False,
                lines=2
            )
            
            # è¨­ç½®å€åŸŸï¼ˆä½¿ç”¨ Accordion æ‘ºç–Šï¼‰
            with gr.Accordion("âš™ï¸ é€²éšè¨­ç½®", open=False):
                # è™•ç†é¸é …
                use_semantic_chunking = gr.Checkbox(
                    label="ä½¿ç”¨èªç¾©åˆ†å¡Šï¼ˆæ¨è–¦ï¼‰",
                    value=False,
                    info="èªç¾©åˆ†å¡Šèƒ½ä¿æŒèªç¾©å®Œæ•´æ€§ï¼Œä½†è™•ç†æ™‚é–“è¼ƒé•·"
                )
                
                # åˆ†å¡Šåƒæ•¸èª¿æ•´ï¼ˆå­—ç¬¦åˆ†å¡Šæ¨¡å¼ï¼‰
                gr.Markdown("**ğŸ“ å­—ç¬¦åˆ†å¡Šåƒæ•¸ï¼ˆåƒ…åœ¨æœªä½¿ç”¨èªç¾©åˆ†å¡Šæ™‚æœ‰æ•ˆï¼‰**")
                chunk_size_slider = gr.Slider(
                    minimum=200,
                    maximum=1500,
                    value=500,
                    step=50,
                    label="åˆ†å¡Šå¤§å°ï¼ˆå­—ç¬¦æ•¸ï¼‰",
                    info="å»ºè­°ï¼š300-800"
                )
                chunk_overlap_slider = gr.Slider(
                    minimum=0,
                    maximum=300,
                    value=100,
                    step=25,
                    label="åˆ†å¡Šé‡ç–Šï¼ˆå­—ç¬¦æ•¸ï¼‰",
                    info="å»ºè­°ï¼šchunk_size çš„ 15-25%"
                )
                
                # èªç¾©åˆ†å¡Šåƒæ•¸èª¿æ•´ï¼ˆåƒ…åœ¨ä½¿ç”¨èªç¾©åˆ†å¡Šæ™‚æœ‰æ•ˆï¼‰
                gr.Markdown("**ğŸ”¬ èªç¾©åˆ†å¡Šåƒæ•¸ï¼ˆåƒ…åœ¨ä½¿ç”¨èªç¾©åˆ†å¡Šæ™‚æœ‰æ•ˆï¼‰**")
                semantic_threshold_slider = gr.Slider(
                    minimum=0.5,
                    maximum=2.5,
                    value=1.0,
                    step=0.1,
                    label="èªç¾©åˆ†å¡Šé–¾å€¼ï¼ˆæ•æ„Ÿåº¦ï¼‰",
                    info="å»ºè­°ï¼š0.8-1.2ï¼ˆç´°ç²’åº¦ï¼‰"
                )
                semantic_min_chunk_slider = gr.Slider(
                    minimum=50,
                    maximum=300,
                    value=100,
                    step=25,
                    label="æœ€å°åˆ†å¡Šå¤§å°ï¼ˆå­—ç¬¦æ•¸ï¼‰",
                    info="å»ºè­°ï¼š50-200"
                )
                
                # RAG æ–¹æ³•é¸æ“‡
                gr.Markdown("**ğŸ¯ RAG æ–¹æ³•é¸æ“‡**")
                enable_adaptive_selection = gr.Checkbox(
                    label="è‡ªå‹•é¸æ“‡æœ€ä½³ RAG æ–¹æ³•ï¼ˆæ¨è–¦ï¼‰",
                    value=True,
                    info="ç³»çµ±æœƒæ ¹æ“šæŸ¥è©¢å’Œæ–‡ä»¶ç‰¹å¾è‡ªå‹•é¸æ“‡æœ€åˆé©çš„ RAG æ–¹æ³•"
                )
                manual_rag_method = gr.Dropdown(
                    choices=[
                        "basic",
                        "subquery",
                        "hyde",
                        "step_back",
                        "hybrid_subquery_hyde",
                        "triple_hybrid"
                    ],
                    value="basic",
                    label="æ‰‹å‹•é¸æ“‡ RAG æ–¹æ³•",
                    info="åƒ…åœ¨è‡ªå‹•é¸æ“‡é—œé–‰æ™‚ç”Ÿæ•ˆ",
                    visible=False
                )
                
                # æŸ¥è©¢é¸é …
                top_k_slider = gr.Slider(
                    minimum=1,
                    maximum=10,
                    value=3,
                    step=1,
                    label="è¿”å›çµæœæ•¸é‡"
                )
                use_llm_checkbox = gr.Checkbox(
                    label="ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”",
                    value=True
                )
        
        # å³å´ï¼šå°è©±ç•Œé¢
        with gr.Column(scale=2):
            # Chatbot çµ„ä»¶
            # #region agent log
            try:
                with open(log_path, "a", encoding="utf-8") as f:
                    log_entry = {
                        "sessionId": "debug-session",
                        "runId": "run1",
                        "hypothesisId": "A",
                        "location": "private_file_rag_interface.py:1409", # Adjusted line number
                        "message": "Before Chatbot creation",
                        "data": {
                            "gradio_version": gr.__version__ if hasattr(gr, '__version__') else "unknown"
                        },
                        "timestamp": int(time.time() * 1000)
                    }
                    f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
            except:
                pass
            # #endregion
            
            # å‰µå»º Chatbotï¼ˆç§»é™¤ä¸æ”¯æŒçš„åƒæ•¸ï¼šshow_copy_button å’Œ avatar_imagesï¼‰
            # #region agent log
            try:
                with open(log_path, "a", encoding="utf-8") as f:
                    log_entry = {
                        "sessionId": "debug-session",
                        "runId": "run1",
                        "hypothesisId": "A",
                        "location": "private_file_rag_interface.py:1430", # Adjusted line number
                        "message": "Creating Chatbot with minimal params",
                        "data": {"params": ["label", "height"]},
                        "timestamp": int(time.time() * 1000)
                    }
                    f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
            except:
                pass
            # #endregion
            
            try:
                chatbot = gr.Chatbot(
                    label="ğŸ’¬ å°è©±",
                    height=500
                )
                # #region agent log
                try:
                    with open(log_path, "a", encoding="utf-8") as f:
                        log_entry = {
                            "sessionId": "debug-session",
                            "runId": "run1",
                            "hypothesisId": "A",
                            "location": "private_file_rag_interface.py:1448", # Adjusted line number
                            "message": "Chatbot created successfully",
                            "data": {"success": True},
                            "timestamp": int(time.time() * 1000)
                        }
                        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
                except:
                    pass
                # #endregion
            except Exception as e:
                # #region agent log
                try:
                    with open(log_path, "a", encoding="utf-8") as f:
                        log_entry = {
                            "sessionId": "debug-session",
                            "runId": "run1",
                            "hypothesisId": "A",
                            "location": "private_file_rag_interface.py:1460", # Adjusted line number
                            "message": "Chatbot creation failed",
                            "data": {
                                "error_type": type(e).__name__,
                                "error_message": str(e)
                            },
                            "timestamp": int(time.time() * 1000)
                        }
                        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
                except:
                    pass
                # #endregion
                raise
            
            # è¼¸å…¥æ¡†
            msg = gr.Textbox(
                label="è¼¸å…¥å•é¡Œ",
                placeholder="è¼¸å…¥æ‚¨çš„å•é¡Œï¼ŒæŒ‰ Enter ç™¼é€...",
                lines=2,
                scale=4
            )
            
            # æŒ‰éˆ•å€åŸŸ
            with gr.Row():
                submit_btn = gr.Button("ğŸ“¤ ç™¼é€", variant="primary", scale=1)
                clear_chat_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å°è©±", variant="secondary", scale=1)
            
            # æŸ¥è©¢ç‹€æ…‹
            query_status = gr.Textbox(
                label="ğŸ“Š ç‹€æ…‹",
                value="ç­‰å¾…æŸ¥è©¢...",
                interactive=False,
                lines=1
            )
    
    # è¼”åŠ©å‡½æ•¸ï¼šè½‰æ› Gradio æ­·å²æ ¼å¼ï¼ˆdictï¼‰å’Œ RAG æ­·å²æ ¼å¼ï¼ˆtupleï¼‰
    def history_dict_to_tuple(history_dict):
        """
        å°‡ Gradio æ­·å²æ ¼å¼ï¼ˆList[Dict]ï¼‰è½‰æ›ç‚º RAG æ­·å²æ ¼å¼ï¼ˆList[Tuple[str, str]]ï¼‰
        
        Args:
            history_dict: Gradio æ ¼å¼çš„æ­·å²ï¼Œæ¯å€‹å…ƒç´ ç‚º {"role": "user"/"assistant", "content": "..."}
        
        Returns:
            RAG æ ¼å¼çš„æ­·å²ï¼Œæ¯å€‹å…ƒç´ ç‚º (user_message, assistant_message)
        """
        if not history_dict:
            return []
        
        conversation_history = []
        current_user_msg = None
        
        for msg in history_dict:
            if isinstance(msg, dict):
                role = msg.get("role", "")
                content = msg.get("content", "")
                
                if role == "user":
                    current_user_msg = content
                elif role == "assistant" and current_user_msg is not None:
                    conversation_history.append((current_user_msg, content))
                    current_user_msg = None
            elif isinstance(msg, tuple) and len(msg) == 2:
                # å¦‚æœå·²ç¶“æ˜¯ tuple æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
                conversation_history.append(msg)
        
        return conversation_history
    
    def history_tuple_to_dict(history_tuple):
        """
        å°‡ RAG æ­·å²æ ¼å¼ï¼ˆList[Tuple[str, str]]ï¼‰è½‰æ›ç‚º Gradio æ­·å²æ ¼å¼ï¼ˆList[Dict]ï¼‰
        
        Args:
            history_tuple: RAG æ ¼å¼çš„æ­·å²ï¼Œæ¯å€‹å…ƒç´ ç‚º (user_message, assistant_message)
        
        Returns:
            Gradio æ ¼å¼çš„æ­·å²ï¼Œæ¯å€‹å…ƒç´ ç‚º {"role": "user"/"assistant", "content": "..."}
        """
        if not history_tuple:
            return []
        
        history_dict = []
        for msg in history_tuple:
            if isinstance(msg, dict):
                # å¦‚æœå·²ç¶“æ˜¯ dict æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                history_dict.append(msg)
            elif isinstance(msg, tuple) and len(msg) == 2:
                # è½‰æ› tuple ç‚º dict æ ¼å¼
                user_msg, assistant_msg = msg
                history_dict.append({"role": "user", "content": user_msg})
                history_dict.append({"role": "assistant", "content": assistant_msg})
        
        return history_dict
    
    def ensure_dict_format(history):
        """
        ç¢ºä¿æ­·å²æ˜¯ Gradio dict æ ¼å¼
        
        Args:
            history: æ­·å²åˆ—è¡¨ï¼ˆå¯èƒ½æ˜¯ dict æˆ– tuple æ ¼å¼ï¼Œä¹Ÿå¯èƒ½æ˜¯ Noneï¼‰
        
        Returns:
            Gradio æ ¼å¼çš„æ­·å²ï¼ˆList[Dict]ï¼‰
        """
        if not history:
            return []
        
        # æª¢æŸ¥ç¬¬ä¸€å€‹å…ƒç´ çš„é¡å‹ä¾†åˆ¤æ–·æ ¼å¼
        try:
            if isinstance(history[0], dict):
                return history
            elif isinstance(history[0], tuple):
                return history_tuple_to_dict(history)
            else:
                # æœªçŸ¥æ ¼å¼ï¼Œè¿”å›ç©ºåˆ—è¡¨
                return []
        except (IndexError, TypeError):
            # å¦‚æœ history ç‚ºç©ºæˆ–ç„¡æ³•ç´¢å¼•ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []
    
    # äº‹ä»¶è™•ç†å‡½æ•¸
    def process_files(files, use_semantic, chunk_size, chunk_overlap, semantic_threshold, semantic_min_chunk):
        """
        è™•ç†ä¸Šå‚³çš„æ–‡ä»¶
        
        Args:
            files: ä¸Šå‚³çš„æ–‡ä»¶åˆ—è¡¨
            use_semantic: æ˜¯å¦ä½¿ç”¨èªç¾©åˆ†å¡Š
            chunk_size: å­—ç¬¦åˆ†å¡Šå¤§å°ï¼ˆåƒ…ç”¨æ–¼å­—ç¬¦åˆ†å¡Šæ¨¡å¼ï¼‰
            chunk_overlap: å­—ç¬¦åˆ†å¡Šé‡ç–Šå¤§å°ï¼ˆåƒ…ç”¨æ–¼å­—ç¬¦åˆ†å¡Šæ¨¡å¼ï¼‰
            semantic_threshold: èªç¾©åˆ†å¡Šé–¾å€¼ï¼ˆåƒ…ç”¨æ–¼èªç¾©åˆ†å¡Šæ¨¡å¼ï¼‰
            semantic_min_chunk: èªç¾©åˆ†å¡Šæœ€å° chunk å¤§å°ï¼ˆåƒ…ç”¨æ–¼èªç¾©åˆ†å¡Šæ¨¡å¼ï¼‰
        """
        if not files:
            return "âŒ è«‹å…ˆä¸Šå‚³æ–‡ä»¶", "ç­‰å¾…ä¸Šå‚³æ–‡ä»¶..."
        
        try:
            # ç²å– RAG å¯¦ä¾‹
            rag = get_private_rag_instance()
            
            # æ›´æ–°é…ç½®
            rag.use_semantic_chunking = use_semantic
            
            # æ›´æ–°åˆ†å¡Šåƒæ•¸ï¼ˆæ ¹æ“šåˆ†å¡Šæ¨¡å¼é¸æ“‡ï¼‰
            if not use_semantic:
                # å­—ç¬¦åˆ†å¡Šæ¨¡å¼ï¼šæ›´æ–°å­—ç¬¦åˆ†å¡Šåƒæ•¸
                rag.chunk_size = int(chunk_size)
                rag.chunk_overlap = int(chunk_overlap)
                print(f"ğŸ“ ä½¿ç”¨å­—ç¬¦åˆ†å¡Šï¼šchunk_size={rag.chunk_size}, chunk_overlap={rag.chunk_overlap}")
            else:
                # èªç¾©åˆ†å¡Šæ¨¡å¼ï¼šæ›´æ–°èªç¾©åˆ†å¡Šåƒæ•¸
                rag.semantic_threshold = float(semantic_threshold)
                rag.semantic_min_chunk_size = int(semantic_min_chunk)
                print(f"ğŸ“ ä½¿ç”¨èªç¾©åˆ†å¡Šï¼šthreshold={rag.semantic_threshold}, min_chunk_size={rag.semantic_min_chunk_size}")
            
            # è™•ç†ä¸Šå‚³çš„æ–‡ä»¶ï¼ˆGradio æœƒè‡ªå‹•ä¿å­˜åˆ°è‡¨æ™‚ç›®éŒ„ï¼‰
            # Gradio 6.x è¿”å›çš„æ˜¯æ–‡ä»¶è·¯å¾‘å­—ç¬¦ä¸²åˆ—è¡¨
            file_paths = []
            
            for file in files:
                # Gradio 6.x è¿”å›å­—ç¬¦ä¸²è·¯å¾‘ï¼ŒèˆŠç‰ˆæœ¬å¯èƒ½è¿”å›æ–‡ä»¶å°è±¡
                if isinstance(file, str):
                    file_path = file
                elif hasattr(file, 'name'):
                    # èˆŠç‰ˆæœ¬ Gradio æ–‡ä»¶å°è±¡
                    file_path = file.name
                else:
                    # å˜—è©¦è½‰æ›ç‚ºå­—ç¬¦ä¸²
                    file_path = str(file)
                
                if os.path.exists(file_path):
                    file_paths.append(file_path)
                else:
                    return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}", "è™•ç†å¤±æ•—"
            
            if not file_paths:
                return "âŒ æ²’æœ‰æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾‘", "è™•ç†å¤±æ•—"
            
            # è™•ç†æ–‡ä»¶
            documents, status_msg = rag.process_files(file_paths)
            
            if documents:
                return status_msg, "âœ… æ–‡ä»¶è™•ç†å®Œæˆï¼Œå¯ä»¥é–‹å§‹æŸ¥è©¢"
            else:
                return status_msg, "âŒ è™•ç†å¤±æ•—"
                
        except Exception as e:
            error_msg = f"âŒ è™•ç†æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return error_msg, "âŒ è™•ç†å¤±æ•—"
    
    def query_rag_stream(message, history, top_k, use_llm, enable_adaptive, manual_method):
        """
        æŸ¥è©¢ RAG ç³»çµ±ï¼ˆå°è©±å¼ï¼Œæµå¼è¼¸å‡ºï¼‰
        
        Args:
            message: ç•¶å‰ç”¨æˆ¶æ¶ˆæ¯
            history: å°è©±æ­·å²ï¼ˆGradio æ ¼å¼ï¼šList[Dict] æˆ– List[Tuple[str, str]]ï¼‰
            top_k: è¿”å›çµæœæ•¸é‡
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
            enable_adaptive: æ˜¯å¦å•Ÿç”¨è‡ªå‹•é¸æ“‡
            manual_method: æ‰‹å‹•é¸æ“‡çš„æ–¹æ³•ï¼ˆåƒ…åœ¨è‡ªå‹•é¸æ“‡é—œé–‰æ™‚ç”Ÿæ•ˆï¼‰
        
        Yields:
            Tuple[history, status_msg]: é€æ­¥æ›´æ–°çš„å°è©±æ­·å²å’Œç‹€æ…‹è¨Šæ¯
        """
        if not message or not message.strip():
            yield history, "âŒ è«‹è¼¸å…¥å•é¡Œ"
            return
        
        try:
            # ç²å– RAG å¯¦ä¾‹
            rag = get_private_rag_instance()
            
            if not rag.is_initialized:
                error_msg = "âŒ RAG ç³»çµ±å°šæœªåˆå§‹åŒ–ï¼Œè«‹å…ˆè™•ç†æ–‡ä»¶"
                # ç¢ºä¿ history æ˜¯ dict æ ¼å¼
                history = ensure_dict_format(history)
                history.append({"role": "user", "content": message})
                history.append({"role": "assistant", "content": error_msg})
                yield history, error_msg
                return
            
            # è¨­ç½® RAG æ–¹æ³•é¸æ“‡åƒæ•¸
            rag.enable_adaptive_selection = enable_adaptive
            if not enable_adaptive:
                rag.selected_rag_method = manual_method
            else:
                rag.selected_rag_method = None
            
            # æº–å‚™å°è©±æ­·å²ï¼šè½‰æ›ç‚º RAG éœ€è¦çš„ tuple æ ¼å¼
            conversation_history = history_dict_to_tuple(history) if history else []
            
            # ç¢ºä¿ history æ˜¯ dict æ ¼å¼ä¸¦æ·»åŠ ç”¨æˆ¶æ¶ˆæ¯
            history = ensure_dict_format(history)
            history.append({"role": "user", "content": message})
            
            # åŸ·è¡ŒæŸ¥è©¢ï¼ˆå‚³å…¥å°è©±æ­·å²ï¼Œä½¿ç”¨æµå¼è¼¸å‡ºï¼‰
            if use_llm:
                # ä½¿ç”¨æµå¼æŸ¥è©¢
                answer_generator = rag.query_stream(
                    query=message,
                    top_k=int(top_k),
                    conversation_history=conversation_history
                )
                
                # åˆå§‹åŒ–å›ç­”
                accumulated_answer = ""
                history_with_user = history.copy()
                final_result = {}
                
                # é€æ­¥æ¥æ”¶æµå¼å›ç­”
                for chunk in answer_generator:
                    if chunk.get("success") is False:
                        error = chunk.get("error", "æœªçŸ¥éŒ¯èª¤")
                        error_msg = f"âŒ æŸ¥è©¢å¤±æ•—: {error}"
                        history_with_user.append({"role": "assistant", "content": error_msg})
                        yield history_with_user, error_msg
                        return
                    
                    # ä¿å­˜æœ€å¾Œä¸€å€‹ chunk ä½œç‚ºæœ€çµ‚çµæœ
                    final_result = chunk
                    
                    # ç²å–æ–°çš„å›ç­”ç‰‡æ®µ
                    new_answer = chunk.get("answer", "")
                    if new_answer:
                        # ç´¯ç©å›ç­”
                        accumulated_answer = new_answer
                        # æ›´æ–°æ­·å²
                        history_with_answer = history_with_user.copy()
                        history_with_answer.append({"role": "assistant", "content": accumulated_answer})
                        yield history_with_answer, "ğŸ”„ æ­£åœ¨ç”Ÿæˆå›ç­”..."
                
                # ç²å–æœ€çµ‚çµæœï¼ˆåŒ…å«çµ±è¨ˆä¿¡æ¯ï¼‰
                rag_method = final_result.get("rag_method", "basic")
                stats = final_result.get("stats", {})
                status_msg = f"âœ… æŸ¥è©¢å®Œæˆï¼ˆæ–¹æ³•: {rag_method.upper()}ï¼‰"
                if stats:
                    total_time = stats.get("total_time", 0)
                    if total_time > 0:
                        status_msg += f" | è€—æ™‚: {total_time:.2f}ç§’"
                
                # ç¢ºä¿æœ€çµ‚å›ç­”å®Œæ•´
                if accumulated_answer:
                    history_with_answer = history_with_user.copy()
                    history_with_answer.append({"role": "assistant", "content": accumulated_answer})
                    yield history_with_answer, status_msg
                else:
                    error_msg = "âš ï¸ LLM æœªç”Ÿæˆå›ç­”ï¼ˆå¯èƒ½ LLM æœå‹™æœªå•Ÿå‹•ï¼‰"
                    history_with_answer = history_with_user.copy()
                    history_with_answer.append({"role": "assistant", "content": error_msg})
                    yield history_with_answer, status_msg
            else:
                # ä¸ä½¿ç”¨ LLMï¼Œç›´æ¥è¿”å›æª¢ç´¢çµæœ
                result = rag.query(
                    query=message,
                    top_k=int(top_k),
                    use_llm=False,
                    conversation_history=conversation_history
                )
                
                if not result.get("success"):
                    error = result.get("error", "æœªçŸ¥éŒ¯èª¤")
                    error_msg = f"âŒ æŸ¥è©¢å¤±æ•—: {error}"
                    history.append({"role": "assistant", "content": error_msg})
                    yield history, error_msg
                    return
                
                # æ ¼å¼åŒ–æª¢ç´¢çµæœ
                formatted_context = result.get("formatted_context", "")
                answer = f"ğŸ“„ æª¢ç´¢åˆ°çš„ç›¸é—œå…§å®¹ï¼š\n\n{formatted_context}"
                
                # ç²å– RAG æ–¹æ³•ä¿¡æ¯
                rag_method = result.get("rag_method", "basic")
                stats = result.get("stats", {})
                status_msg = f"âœ… æŸ¥è©¢å®Œæˆï¼ˆæ–¹æ³•: {rag_method.upper()}ï¼‰"
                if stats:
                    total_time = stats.get("total_time", 0)
                    if total_time > 0:
                        status_msg += f" | è€—æ™‚: {total_time:.2f}ç§’"
                
                history.append({"role": "assistant", "content": answer})
                yield history, status_msg
            
        except Exception as e:
            error_msg = f"âŒ æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            # ç¢ºä¿ history æ˜¯ dict æ ¼å¼
            history = ensure_dict_format(history)
            if not any(msg.get("role") == "user" and msg.get("content") == message for msg in history):
                history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": error_msg})
            yield history, error_msg
    
    def clear_chat():
        """æ¸…é™¤å°è©±æ­·å²ï¼ˆä¸é‡ç½® RAG ç³»çµ±ï¼‰"""
        return [], "å°è©±å·²æ¸…é™¤"
    
    def clear_all():
        """æ¸…é™¤æ‰€æœ‰å…§å®¹ï¼ˆåŒ…æ‹¬ RAG ç³»çµ±ï¼‰"""
        reset_private_rag_instance()
        empty_history = []
        return (
            None,  # file_upload
            False,  # use_semantic_chunking
            500,  # chunk_size_slider
            100,  # chunk_overlap_slider
            1.0,  # semantic_threshold_slider
            100,  # semantic_min_chunk_slider
            True,  # enable_adaptive_selection
            "basic",  # manual_rag_method
            "ç­‰å¾…ä¸Šå‚³æ–‡ä»¶...",  # process_status
            empty_history,  # chatbot (å°è©±æ­·å²)
            empty_history,  # chat_history (ç‹€æ…‹)
            "ç­‰å¾…æŸ¥è©¢...",  # query_status
        )
    
    # ç¶å®šäº‹ä»¶
    process_btn.click(
        fn=process_files,
        inputs=[
            file_upload, 
            use_semantic_chunking, 
            chunk_size_slider, 
            chunk_overlap_slider,
            semantic_threshold_slider,
            semantic_min_chunk_slider
        ],
        outputs=[process_status, query_status]
    )
    
    # è‡ªå‹•é¸æ“‡é–‹é—œæ™‚é¡¯ç¤º/éš±è—æ‰‹å‹•é¸æ“‡ä¸‹æ‹‰èœå–®
    def toggle_manual_method(enable_adaptive):
        return gr.update(visible=not enable_adaptive)
    
    enable_adaptive_selection.change(
        fn=toggle_manual_method,
        inputs=[enable_adaptive_selection],
        outputs=[manual_rag_method]
    )
    
    # æäº¤æ¶ˆæ¯ï¼ˆæŒ‰éˆ•é»æ“Šæˆ– Enter éµï¼‰
    def submit_message(message, history, top_k, use_llm, enable_adaptive, manual_method):
        """æäº¤æ¶ˆæ¯ä¸¦æ›´æ–°å°è©±æ­·å²ï¼ˆæµå¼è¼¸å‡ºï¼‰"""
        if not message or not message.strip():
            # ç¢ºä¿ history æ˜¯ dict æ ¼å¼
            history = ensure_dict_format(history)
            return history, history, "", "ç­‰å¾…æŸ¥è©¢..."
        # æ¸…ç©ºè¼¸å…¥æ¡†ä¸¦åŸ·è¡Œæµå¼æŸ¥è©¢
        for new_history, status in query_rag_stream(message, history, top_k, use_llm, enable_adaptive, manual_method):
            yield new_history, new_history, "", status
    
    # ç¶å®šæäº¤æŒ‰éˆ•å’Œ Enter éµ
    submit_btn.click(
        fn=submit_message,
        inputs=[msg, chat_history, top_k_slider, use_llm_checkbox, enable_adaptive_selection, manual_rag_method],
        outputs=[chatbot, chat_history, msg, query_status]
    )
    
    msg.submit(
        fn=submit_message,
        inputs=[msg, chat_history, top_k_slider, use_llm_checkbox, enable_adaptive_selection, manual_rag_method],
        outputs=[chatbot, chat_history, msg, query_status]
    )
    
    # æ¸…é™¤å°è©±æŒ‰éˆ•ï¼ˆéœ€è¦æ›´æ–° chat_history ç‹€æ…‹ï¼‰
    def clear_chat_with_state():
        """æ¸…é™¤å°è©±æ­·å²ä¸¦æ›´æ–°ç‹€æ…‹"""
        empty_history = []
        return empty_history, empty_history, "å°è©±å·²æ¸…é™¤"
    
    clear_chat_btn.click(
        fn=clear_chat_with_state,
        outputs=[chatbot, chat_history, query_status]
    )
    
    # æ¸…é™¤æ‰€æœ‰æŒ‰éˆ•
    clear_files_btn.click(
        fn=clear_all,
        outputs=[
            file_upload,
            use_semantic_chunking,
            chunk_size_slider,
            chunk_overlap_slider,
            semantic_threshold_slider,
            semantic_min_chunk_slider,
            enable_adaptive_selection,
            manual_rag_method,
            process_status,
            chatbot,  # æ›´æ–° chatbot é¡¯ç¤º
            chat_history,  # æ›´æ–° chat_history ç‹€æ…‹
            query_status
        ]
    )

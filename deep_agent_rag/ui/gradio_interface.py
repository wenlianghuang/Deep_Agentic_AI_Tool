"""
Gradio ç•Œé¢æ¨¡çµ„
æä¾› Web UI å’Œæµå¼æ›´æ–°åŠŸèƒ½
"""
import uuid
import re
import time
import json
import os
from typing import Iterator, Tuple
import gradio as gr
from langchain_core.messages import HumanMessage

# graph å’Œ rag_retriever å°‡å¾å¤–éƒ¨å‚³å…¥ï¼Œä¸åœ¨é€™è£¡å°å…¥
from ..utils.llm_utils import get_llm_type, is_using_local_llm
from .email_interface import _create_email_interface
from .calendar_interface import _create_calendar_interface
from .private_file_rag_interface import _create_private_file_rag_interface
from .simple_chatbot_interface import create_simple_chatbot_interface
from .image_analysis_interface import _create_image_analysis_interface


def run_research_agent(query: str, graph, thread_id: str = None) -> Iterator[Tuple[str, str, str, str, str]]:
    """
    åŸ·è¡Œç ”ç©¶ä»£ç†ä¸¦å¯¦æ™‚è¿”å›ç‹€æ…‹ï¼ˆç”¨æ–¼ Gradio æµå¼æ›´æ–°ï¼‰
    
    ã€Gradio æ•´åˆã€‘è¿”å›ç”Ÿæˆå™¨ï¼Œè®“ Gradio å¯ä»¥å¯¦æ™‚æ›´æ–° UI
    è¿”å›æ ¼å¼: (ç•¶å‰ç¯€é»ç‹€æ…‹, ä»»å‹™åˆ—è¡¨, ç ”ç©¶ç­†è¨˜, æœ€çµ‚å ±å‘Š, è­¦å‘Šè¨Šæ¯)
    
    Args:
        query: ç”¨æˆ¶è¼¸å…¥çš„ç ”ç©¶å•é¡Œ
        graph: ç·¨è­¯å¾Œçš„ Agent åœ–è¡¨
        thread_id: å¯é¸çš„æœƒè©± IDï¼Œç”¨æ–¼å€åˆ†ä¸åŒçš„æŸ¥è©¢æœƒè©±
    
    Yields:
        Tuple[str, str, str, str, str]: (ç‹€æ…‹, ä»»å‹™åˆ—è¡¨, ç ”ç©¶ç­†è¨˜, å ±å‘Š, è­¦å‘Šè¨Šæ¯)
    """
    if not query or not query.strip():
        yield "âŒ è«‹è¼¸å…¥å•é¡Œ", "", "", "", ""
        return
    
    # æª¢æŸ¥ LLM é¡å‹ä¸¦ç”Ÿæˆè­¦å‘Šè¨Šæ¯
    warning_msg = ""
    if is_using_local_llm():
        warning_msg = "âš ï¸ **è­¦å‘Šï¼šGroq API é¡åº¦å·²ç”¨å®Œï¼Œå·²åˆ‡æ›åˆ°æœ¬åœ° MLX æ¨¡å‹ (Qwen2.5)**\n\næœ¬åœ°æ¨¡å‹è™•ç†é€Ÿåº¦å¯èƒ½è¼ƒæ…¢ï¼Œè«‹è€å¿ƒç­‰å¾…ã€‚"
    else:
        llm_type = get_llm_type()
        if llm_type == "groq":
            warning_msg = "âœ… **ç•¶å‰ä½¿ç”¨ï¼šGroq API**"
        else:
            warning_msg = "â„¹ï¸ **ç•¶å‰ä½¿ç”¨ï¼šæœ¬åœ° MLX æ¨¡å‹ (Qwen2.5)**"
    
    # ç”Ÿæˆå”¯ä¸€çš„ thread_idï¼ˆå¦‚æœæœªæä¾›ï¼‰
    if not thread_id:
        thread_id = f"deep-research-{uuid.uuid4().hex[:8]}"
    
    config = {"configurable": {"thread_id": thread_id}}
    
    # åˆå§‹åŒ–å®Œæ•´ç‹€æ…‹
    initial_state = {
        "query": query,
        "messages": [HumanMessage(content=query)],
        "tasks": [],
        "completed_tasks": [],
        "research_notes": [],
        "iteration": 0
    }
    
    # åˆå§‹åŒ–é¡¯ç¤ºè®Šæ•¸
    current_node = "ğŸ”„ åˆå§‹åŒ–ä¸­..."
    tasks_display = ""
    notes_display = ""
    report_display = ""
    full_report = ""  # å„²å­˜å®Œæ•´å ±å‘Šï¼Œç”¨æ–¼é€æ­¥é¡¯ç¤º
    
    # åœ¨é–‹å§‹æ™‚é¡¯ç¤ºè­¦å‘Šè¨Šæ¯
    yield current_node, tasks_display, notes_display, report_display, warning_msg
    
    try:
        # é–‹å§‹åŸ·è¡Œåœ–è¡¨
        events = graph.stream(
            initial_state,
            config,
            stream_mode="updates"
        )
        
        # éæ­·äº‹ä»¶æµï¼Œå¯¦æ™‚æ›´æ–° UI
        for event in events:
            for node, data in event.items():
                # æ›´æ–°ç•¶å‰ç¯€é»ç‹€æ…‹
                node_emoji = {
                    "planner": "ğŸ“",
                    "research_agent": "ğŸ•µï¸",
                    "tools": "ğŸ”§",
                    "note_taking": "ğŸ“Œ",
                    "final_report": "ğŸ“Š"
                }.get(node, "ğŸ”„")
                
                current_node = f"{node_emoji} æ­£åœ¨åŸ·è¡Œ: {node}"
                
                # æª¢æŸ¥ LLM ç‹€æ…‹è®ŠåŒ–ï¼ˆå¯èƒ½åœ¨åŸ·è¡Œéç¨‹ä¸­åˆ‡æ›ï¼‰
                if is_using_local_llm():
                    warning_msg = "âš ï¸ **è­¦å‘Šï¼šGroq API é¡åº¦å·²ç”¨å®Œï¼Œå·²åˆ‡æ›åˆ°æœ¬åœ° MLX æ¨¡å‹ (Qwen2.5)**\n\næœ¬åœ°æ¨¡å‹è™•ç†é€Ÿåº¦å¯èƒ½è¼ƒæ…¢ï¼Œè«‹è€å¿ƒç­‰å¾…ã€‚"
                else:
                    llm_type = get_llm_type()
                    if llm_type == "groq":
                        warning_msg = "âœ… **ç•¶å‰ä½¿ç”¨ï¼šGroq API**"
                    else:
                        warning_msg = "â„¹ï¸ **ç•¶å‰ä½¿ç”¨ï¼šæœ¬åœ° MLX æ¨¡å‹ (Qwen2.5)**"
                
                # æ›´æ–°ä»»å‹™åˆ—è¡¨é¡¯ç¤º
                if "tasks" in data:
                    tasks = data.get("tasks", [])
                    if tasks:
                        tasks_display = "\n".join([f"{i+1}. {task}" for i, task in enumerate(tasks)])
                
                # æ›´æ–°å®Œæˆä»»å‹™è¨ˆæ•¸
                if "completed_tasks" in data:
                    completed = data.get("completed_tasks", [])
                    tasks = data.get("tasks", [])
                    if completed and tasks:
                        completed_count = len(completed)
                        total_count = len(tasks)
                        progress = f"\n\nâœ… é€²åº¦: {completed_count}/{total_count} å€‹ä»»å‹™å·²å®Œæˆ"
                        tasks_display = "\n".join([f"{i+1}. {task}" for i, task in enumerate(tasks)]) + progress
                
                # æ›´æ–°ç ”ç©¶ç­†è¨˜é¡¯ç¤ºï¼ˆåªé¡¯ç¤ºæœ€è¿‘5æ¢ï¼Œé¿å…éé•·ï¼‰
                if "research_notes" in data:
                    notes = data.get("research_notes", [])
                    if notes:
                        # åªå–æœ€è¿‘5æ¢ç­†è¨˜
                        recent_notes = notes[-5:] if len(notes) > 5 else notes
                        notes_display = "\n\n" + "="*50 + "\n\n".join(recent_notes)
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯æœ€çµ‚å ±å‘Š
                if node == "final_report" and "messages" in data:
                    full_report = data["messages"][-1].content
                    current_node = "ğŸ“Š æ­£åœ¨ç”Ÿæˆå ±å‘Š..."
                    
                    # æŒ‰å¥å­åˆ†å‰²ä¸¦é€æ­¥é¡¯ç¤ºï¼ˆæ”¯æŒä¸­è‹±æ–‡æ¨™é»ï¼‰
                    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åˆ†å‰²å¥å­ï¼ˆæ”¯æŒä¸­æ–‡æ¨™é»ï¼šã€‚ï¼ï¼Ÿå’Œè‹±æ–‡æ¨™é»ï¼š. ! ?ï¼‰
                    sentence_pattern = r'([ã€‚ï¼ï¼Ÿ\n\n]+|\.\s+|!\s+|\?\s+)'
                    parts = re.split(sentence_pattern, full_report)
                    
                    # é‡æ–°çµ„åˆå¥å­ï¼ˆä¿ç•™æ¨™é»ï¼‰
                    sentence_parts = []
                    i = 0
                    while i < len(parts):
                        if i + 1 < len(parts) and re.match(sentence_pattern, parts[i + 1]):
                            # å¥å­ + æ¨™é»
                            sentence_parts.append(parts[i] + parts[i + 1])
                            i += 2
                        else:
                            # å–®ç¨çš„å¥å­æˆ–æ¨™é»
                            if parts[i].strip():
                                sentence_parts.append(parts[i])
                            i += 1
                    
                    # å¦‚æœåˆ†å‰²å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®çš„å­—ç¬¦å¡Šæ–¹å¼
                    if not sentence_parts or len(sentence_parts) == 1:
                        # æŒ‰å­—ç¬¦å¡Šé€æ­¥é¡¯ç¤ºï¼ˆæ¯20å€‹å­—ç¬¦ï¼‰
                        chunk_size = 20
                        accumulated_text = ""
                        for i in range(0, len(full_report), chunk_size):
                            accumulated_text = full_report[:i + chunk_size]
                            report_display = accumulated_text
                            yield current_node, tasks_display, notes_display, report_display, warning_msg
                            time.sleep(0.03)  # æ¯å¡Šä¹‹é–“çš„å»¶é²ï¼ˆ30æ¯«ç§’ï¼‰
                    else:
                        # é€æ­¥é¡¯ç¤ºæ¯å€‹å¥å­
                        accumulated_text = ""
                        for sentence in sentence_parts:
                            accumulated_text += sentence
                            report_display = accumulated_text
                            yield current_node, tasks_display, notes_display, report_display, warning_msg
                            time.sleep(0.1)  # æ¯å¥ä¹‹é–“çš„å»¶é²ï¼ˆ100æ¯«ç§’ï¼‰
                    
                    # ç¢ºä¿å®Œæ•´å ±å‘Šé¡¯ç¤º
                    report_display = full_report
                    current_node = "âœ… å ±å‘Šç”Ÿæˆå®Œæˆï¼"
                    yield current_node, tasks_display, notes_display, report_display, warning_msg
                    continue  # è·³éå¾Œé¢çš„ yieldï¼Œé¿å…é‡è¤‡
                
                # å¯¦æ™‚è¿”å›ç‹€æ…‹ï¼ˆè®“ Gradio æ›´æ–° UIï¼‰
                yield current_node, tasks_display, notes_display, report_display, warning_msg
        
        # æœ€çµ‚ç‹€æ…‹
        yield "âœ… ç ”ç©¶å®Œæˆï¼", tasks_display, notes_display, report_display, warning_msg
        
    except Exception as e:
        error_msg = f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        print(f"éŒ¯èª¤è©³æƒ…: {e}")
        import traceback
        traceback.print_exc()
        # æª¢æŸ¥æ˜¯å¦æ˜¯å› ç‚º Groq é¡åº¦å•é¡Œ
        if is_using_local_llm():
            warning_msg = "âš ï¸ **è­¦å‘Šï¼šGroq API é¡åº¦å·²ç”¨å®Œï¼Œå·²åˆ‡æ›åˆ°æœ¬åœ° MLX æ¨¡å‹ (Qwen2.5)**\n\næœ¬åœ°æ¨¡å‹è™•ç†é€Ÿåº¦å¯èƒ½è¼ƒæ…¢ï¼Œè«‹è€å¿ƒç­‰å¾…ã€‚"
        yield error_msg, tasks_display, notes_display, report_display, warning_msg


def create_gradio_interface(graph):
    """
    å‰µå»º Gradio ç•Œé¢
    
    ã€Gradio 6.x å…¼å®¹ã€‘ä½¿ç”¨æœ€æ–°çš„ Gradio API å‰µå»ºç¾è§€çš„ Web ç•Œé¢
    """
    with gr.Blocks(
        title="Deep Research Agent with RAG (Local MLX)"
    ) as demo:
        # æ¨™é¡Œå€åŸŸ
        gr.Markdown(
            """
            <div class="header">
            <h1>ğŸš€ Deep Research Agent with RAG</h1>
            <p><strong>åŠŸèƒ½ç‰¹è‰²ï¼š</strong></p>
            <p>ğŸ’¬ ç°¡å–®èŠå¤©æ©Ÿå™¨äºº | ğŸ” Deep Research Agent | ğŸ“§ æ™ºèƒ½éƒµä»¶åŠ©æ‰‹ | ğŸ“… æ™ºèƒ½è¡Œäº‹æ›†ç®¡ç† | ğŸ“„ ç§æœ‰æ–‡ä»¶ RAG å•ç­” | ğŸ–¼ï¸ æ™ºèƒ½åœ–ç‰‡åˆ†æ</p>
            <p><strong>æ™ºèƒ½è¦åŠƒï¼š</strong> ç³»çµ±æœƒæ ¹æ“šå•é¡Œé¡å‹è‡ªå‹•é¸æ“‡åˆé©çš„ç ”ç©¶å·¥å…·</p>
            <p><strong>æœ¬åœ°æ¨¡å‹ï¼š</strong> ä½¿ç”¨ MLX æœ¬åœ°æ¨¡å‹ï¼Œä¿è­·éš±ç§ï¼Œç„¡éœ€ API é‡‘é‘°</p>
            </div>
            """,
            elem_classes=["header"]
        )
        
        # ä½¿ç”¨ Tabs åˆ†é›¢ä¸åŒåŠŸèƒ½
        with gr.Tabs() as tabs:
            # Tab 1: Simple Chatbot
            with gr.Tab("ğŸ’¬ Simple Chatbot"):
                _create_simple_chatbot_tab()
            
            # Tab 2: Deep Research Agent
            with gr.Tab("ğŸ” Deep Research Agent"):
                _create_research_interface(graph)
            
            # Tab 3: Email Tool
            with gr.Tab("ğŸ“§ Email Tool"):
                _create_email_interface()
            
            # Tab 4: Calendar Tool
            with gr.Tab("ğŸ“… Calendar Tool"):
                _create_calendar_interface()
            
            # Tab 5: Private File RAG
            with gr.Tab("ğŸ“š Private File RAG"):
                _create_private_file_rag_interface()
            
            # Tab 6: Image Analysis
            with gr.Tab("ğŸ–¼ï¸ Image Analysis"):
                _create_image_analysis_interface()
    
    return demo


def _create_research_interface(graph):
    """å‰µå»º Deep Research Agent ç•Œé¢"""
    with gr.Row():
        with gr.Column(scale=2):
            # è¼¸å…¥å€åŸŸ
            query_input = gr.Textbox(
                label="ğŸ“ è«‹è¼¸å…¥æ‚¨çš„ç ”ç©¶å•é¡Œ",
                placeholder="ä¾‹å¦‚ï¼šèªªæ˜Tree of Thoughtsï¼Œä¸¦æ·±åº¦æ¯”è¼ƒä»–è·ŸChain of Thoughtçš„å·®è·åœ¨å“ªè£¡ï¼Ÿ",
                lines=3,
                value="æ¯”è¼ƒå¾®è»Ÿ(MSFT)å’Œè°·æ­Œ(GOOGL)åœ¨AIé ˜åŸŸçš„ä½ˆå±€ï¼Œä¸¦çµåˆ Tree of Thoughts è«–æ–‡ä¸­çš„æ–¹æ³•è«–é€²è¡Œåˆ†æ"
            )
            
            # æŒ‰éˆ•å€åŸŸ
            with gr.Row():
                submit_btn = gr.Button("ğŸ” é–‹å§‹ç ”ç©¶", variant="primary", scale=1)
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤", variant="secondary", scale=1)
            
            # ç‹€æ…‹é¡¯ç¤º
            status_display = gr.Textbox(
                label="ğŸ“Š ç•¶å‰ç‹€æ…‹",
                value="ç­‰å¾…é–‹å§‹...",
                interactive=False,
                lines=2
            )
            
            # è­¦å‘Šè¨Šæ¯é¡¯ç¤º
            warning_display = gr.Markdown(
                value="",
                elem_classes=["warning-box"]
            )
        
        with gr.Column(scale=1):
            # ä»»å‹™åˆ—è¡¨
            tasks_display = gr.Textbox(
                label="ğŸ“‹ ç ”ç©¶ä»»å‹™åˆ—è¡¨",
                lines=12,
                interactive=False
            )
    
    with gr.Row():
        # ç ”ç©¶ç­†è¨˜ï¼ˆå¯¦æ™‚æ›´æ–°ï¼‰
        notes_display = gr.Textbox(
            label="ğŸ“Œ ç ”ç©¶ç­†è¨˜ï¼ˆå¯¦æ™‚æ›´æ–°ï¼‰",
            lines=15,
            interactive=False
        )
    
    with gr.Row():
        # æœ€çµ‚å ±å‘Š
        report_display = gr.Textbox(
            label="ğŸ“„ æœ€çµ‚æ·±åº¦å ±å‘Š",
            lines=20,
            interactive=False
        )
    
    # äº‹ä»¶è™•ç†å‡½æ•¸
    def process_query(query):
        """è™•ç†æŸ¥è©¢ä¸¦è¿”å›æµå¼æ›´æ–°"""
        if not query or not query.strip():
            return "âŒ è«‹è¼¸å…¥å•é¡Œ", "", "", "", ""
        
        # ä½¿ç”¨ç”Ÿæˆå™¨å‡½æ•¸å¯¦æ™‚æ›´æ–°ï¼ˆGradio 6.x æ”¯æŒæµå¼è¼¸å‡ºï¼‰
        for status, tasks, notes, report, warning in run_research_agent(query, graph):
            yield status, tasks, notes, report, warning
    
    def clear_all():
        """æ¸…é™¤æ‰€æœ‰è¼¸å…¥å’Œè¼¸å‡º"""
        # æª¢æŸ¥ç•¶å‰ LLM ç‹€æ…‹
        warning_msg = ""
        if is_using_local_llm():
            warning_msg = "âš ï¸ **è­¦å‘Šï¼šGroq API é¡åº¦å·²ç”¨å®Œï¼Œå·²åˆ‡æ›åˆ°æœ¬åœ° MLX æ¨¡å‹ (Qwen2.5)**\n\næœ¬åœ°æ¨¡å‹è™•ç†é€Ÿåº¦å¯èƒ½è¼ƒæ…¢ï¼Œè«‹è€å¿ƒç­‰å¾…ã€‚"
        else:
            llm_type = get_llm_type()
            if llm_type == "groq":
                warning_msg = "âœ… **ç•¶å‰ä½¿ç”¨ï¼šGroq API**"
            else:
                warning_msg = "â„¹ï¸ **ç•¶å‰ä½¿ç”¨ï¼šæœ¬åœ° MLX æ¨¡å‹ (Qwen2.5)**"
        return "", "", "", "", "ç­‰å¾…é–‹å§‹...", warning_msg
    
    # ç¶å®šäº‹ä»¶
    submit_btn.click(
        fn=process_query,
        inputs=query_input,
        outputs=[status_display, tasks_display, notes_display, report_display, warning_display]
    )
    
    clear_btn.click(
        fn=clear_all,
        outputs=[query_input, tasks_display, notes_display, report_display, status_display, warning_display]
    )
    
    # åˆå§‹åŒ–æ™‚é¡¯ç¤ºç•¶å‰ LLM ç‹€æ…‹
    def get_initial_warning():
        warning_msg = ""
        if is_using_local_llm():
            warning_msg = "âš ï¸ **è­¦å‘Šï¼šGroq API é¡åº¦å·²ç”¨å®Œï¼Œå·²åˆ‡æ›åˆ°æœ¬åœ° MLX æ¨¡å‹ (Qwen2.5)**\n\næœ¬åœ°æ¨¡å‹è™•ç†é€Ÿåº¦å¯èƒ½è¼ƒæ…¢ï¼Œè«‹è€å¿ƒç­‰å¾…ã€‚"
        else:
            llm_type = get_llm_type()
            if llm_type == "groq":
                warning_msg = "âœ… **ç•¶å‰ä½¿ç”¨ï¼šGroq API**"
            else:
                warning_msg = "â„¹ï¸ **ç•¶å‰ä½¿ç”¨ï¼šæœ¬åœ° MLX æ¨¡å‹ (Qwen2.5)**"
        return warning_msg
    
    # åœ¨ç•Œé¢è¼‰å…¥æ™‚é¡¯ç¤ºåˆå§‹è­¦å‘Š
    warning_display.value = get_initial_warning()
    
    # ç¤ºä¾‹å•é¡Œï¼ˆå¿«é€Ÿæ¸¬è©¦ï¼‰
    gr.Examples(
        examples=[
            "èªªæ˜Tree of Thoughtsï¼Œä¸¦æ·±åº¦æ¯”è¼ƒä»–è·ŸChain of Thoughtçš„å·®è·åœ¨å“ªè£¡ï¼Ÿ",
            "æ¯”è¼ƒå¾®è»Ÿ(MSFT)å’Œè°·æ­Œ(GOOGL)åœ¨AIé ˜åŸŸçš„ä½ˆå±€",
            "åˆ†æ Tree of Thoughts æ–¹æ³•çš„å„ªç¼ºé»å’Œæ‡‰ç”¨å ´æ™¯",
            "æŸ¥è©¢è˜‹æœ(AAPL)çš„è²¡å‹™ç‹€æ³å’Œè¿‘æœŸå‹•æ…‹"
        ],
        inputs=query_input
    )
    
    # é è…³èªªæ˜
    gr.Markdown(
        """
        ---
        **ä½¿ç”¨èªªæ˜ï¼š**
        1. åœ¨è¼¸å…¥æ¡†ä¸­è¼¸å…¥æ‚¨çš„ç ”ç©¶å•é¡Œ
        2. é»æ“Šã€Œé–‹å§‹ç ”ç©¶ã€æŒ‰éˆ•
        3. ç³»çµ±æœƒè‡ªå‹•è¦åŠƒç ”ç©¶æ­¥é©Ÿä¸¦åŸ·è¡Œ
        4. æ‚¨å¯ä»¥å¯¦æ™‚æŸ¥çœ‹ä»»å‹™é€²åº¦ã€ç ”ç©¶ç­†è¨˜å’Œæœ€çµ‚å ±å‘Š
        5. é»æ“Šã€Œæ¸…é™¤ã€æŒ‰éˆ•å¯ä»¥é‡ç½®æ‰€æœ‰å…§å®¹
        """
    )


def _create_simple_chatbot_tab():
    """å‰µå»ºç°¡å–®èŠå¤©æ©Ÿå™¨äººæ¨™ç±¤é å…§å®¹"""
    from .simple_chatbot_interface import chat_with_llm_streaming, get_llm_status
    
    # æ¨™é¡Œèªªæ˜
    gr.Markdown(
        """
        ### ğŸ’¬ Simple Chatbot - ç´”ç²¹çš„å°è©±é«”é©—
        
        é€™æ˜¯ä¸€å€‹ç°¡å–®çš„èŠå¤©æ©Ÿå™¨äººï¼Œä¸åŒ…å« RAGã€Deep AI Agent ç­‰è¤‡é›œåŠŸèƒ½ã€‚
        åªå°ˆæ³¨æ–¼è‡ªç„¶å°è©±ï¼Œè®“æ‚¨èˆ‡ AI è¼•é¬†äº¤æµã€‚
        """
    )
    
    # LLM ç‹€æ…‹é¡¯ç¤º
    llm_status = gr.Markdown(
        value=get_llm_status(),
        elem_classes=["warning-box"]
    )
    
    # Guardrails å•Ÿç”¨é–‹é—œ
    with gr.Row():
        enable_guardrails_checkbox = gr.Checkbox(
            label="ğŸ›¡ï¸ å•Ÿç”¨ Guardrails å…§å®¹éæ¿¾",
            value=True,
            info="å•Ÿç”¨å¾Œå°‡æª¢æŸ¥è¼¸å…¥å’Œè¼¸å‡ºå…§å®¹ï¼Œé˜»æ“‹æ•æ„Ÿè©±é¡Œ"
        )
    
    # ç³»çµ±æç¤ºè©è¨­å®š
    with gr.Accordion("âš™ï¸ é€²éšè¨­å®š", open=False):
        system_prompt = gr.Textbox(
            label="ç³»çµ±æç¤ºè© (System Prompt)",
            value="ä½ æ˜¯ä¸€å€‹æœ‰å¹«åŠ©çš„AIåŠ©æ‰‹ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚",
            lines=3,
            placeholder="è¨­å®š AI çš„è§’è‰²å’Œè¡Œç‚ºæ–¹å¼..."
        )
        
        gr.Markdown(
            """
            **æç¤ºè©ç¯„ä¾‹ï¼š**
            - å°ˆæ¥­åŠ©æ‰‹ï¼šã€Œä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æŠ€è¡“é¡§å•ï¼Œæ“…é•·è§£é‡‹è¤‡é›œçš„æŠ€è¡“æ¦‚å¿µã€‚ã€
            - å‰µæ„å¯«ä½œï¼šã€Œä½ æ˜¯ä¸€ä½å¯Œæœ‰å‰µæ„çš„ä½œå®¶ï¼Œæ“…é•·å¯«ä½œæ•…äº‹å’Œè©©æ­Œã€‚ã€
            - å­¸ç¿’è¼”å°ï¼šã€Œä½ æ˜¯ä¸€ä½è€å¿ƒçš„è€å¸«ï¼Œæ“…é•·ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹è¤‡é›œçš„æ¦‚å¿µã€‚ã€
            """
        )
    
    # èŠå¤©ç•Œé¢
    chatbot = gr.Chatbot(
        label="å°è©±è¨˜éŒ„",
        height=400,
        show_label=True
    )
    
    # è¼¸å…¥å€åŸŸ
    msg = gr.Textbox(
        label="è¨Šæ¯",
        placeholder="åœ¨é€™è£¡è¼¸å…¥æ‚¨çš„è¨Šæ¯...",
        lines=2,
        show_label=False
    )
    
    # æ§åˆ¶æŒ‰éˆ•
    with gr.Row():
        submit_btn = gr.Button("ğŸ“¤ ç™¼é€", variant="primary")
        clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å°è©±", variant="secondary")
        refresh_status_btn = gr.Button("ğŸ”„ æ›´æ–°ç‹€æ…‹", variant="secondary")
    
    # ç¤ºä¾‹å•é¡Œ
    gr.Examples(
        examples=[
            "ä½ å¥½ï¼è«‹ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
            "è«‹å¹«æˆ‘è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ",
            "èƒ½çµ¦æˆ‘ä¸€äº›å­¸ç¿’ Python çš„å»ºè­°å—ï¼Ÿ",
            "è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹é‡å­è¨ˆç®—ã€‚",
            "å¯«ä¸€é¦–é—œæ–¼æ˜¥å¤©çš„çŸ­è©©ã€‚"
        ],
        inputs=msg,
        label="ğŸ’¡ å¿«é€Ÿè©¦ç”¨ç¯„ä¾‹"
    )
    
    # äº‹ä»¶ç¶å®š
    def clear_chat():
        """æ¸…é™¤å°è©±"""
        return [], ""
    
    def refresh_status():
        """æ›´æ–° LLM ç‹€æ…‹"""
        return get_llm_status()
    
    # ç™¼é€æ¶ˆæ¯äº‹ä»¶
    msg.submit(
        fn=chat_with_llm_streaming,
        inputs=[msg, chatbot, system_prompt, enable_guardrails_checkbox],
        outputs=[chatbot],
        queue=True
    ).then(
        fn=lambda: "",
        outputs=[msg],
        queue=False
    )
    
    submit_btn.click(
        fn=chat_with_llm_streaming,
        inputs=[msg, chatbot, system_prompt, enable_guardrails_checkbox],
        outputs=[chatbot],
        queue=True
    ).then(
        fn=lambda: "",
        outputs=[msg],
        queue=False
    )
    
    clear_btn.click(
        fn=clear_chat,
        outputs=[chatbot, msg],
        queue=False
    )
    
    refresh_status_btn.click(
        fn=refresh_status,
        outputs=[llm_status],
        queue=False
    )



    
    
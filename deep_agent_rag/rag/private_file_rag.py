"""
ç§æœ‰æ–‡ä»¶ RAG ç³»çµ±
é›†æˆ Learn_RAG çš„åŠŸèƒ½ï¼Œæ”¯æŒä¸Šå‚³ç§æœ‰æ–‡ä»¶ï¼ˆPDFã€DOCXã€TXTï¼‰ä¸¦ä½¿ç”¨ RAG å›ç­”å•é¡Œ

LLM ä½¿ç”¨ç­–ç•¥ï¼š
- å„ªå…ˆä½¿ç”¨ Groq APIï¼ˆå¦‚æœé…ç½®äº† API é‡‘é‘°ï¼‰
- å…¶æ¬¡ä½¿ç”¨ Ollamaï¼ˆå¦‚æœæœå‹™æ­£åœ¨é‹è¡Œï¼‰
- æœ€å¾Œä½¿ç”¨ MLX æœ¬åœ°æ¨¡å‹ï¼ˆä½œç‚ºå‚™é¸æ–¹æ¡ˆï¼‰
"""
import os
import sys
import time
from pathlib import Path
from typing import Optional, Dict, List, Tuple
import tempfile
import shutil

# å°å…¥ Deep_Agentic_AI_Tool çš„ LLM å·¥å…·
# é€™æ¨£å¯ä»¥ä½¿ç”¨çµ±ä¸€çš„ LLM å„ªå…ˆé †åºç­–ç•¥ï¼ˆGroq -> Ollama -> MLXï¼‰
from ..utils.llm_utils import get_llm
from langchain_core.messages import HumanMessage

# å°å…¥ LLM é©é…å™¨å’Œæ™ºèƒ½é¸æ“‡å™¨
from .llm_adapter import LangChainLLMAdapter
from .adaptive_rag_selector import AdaptiveRAGSelector, RAGMethod

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘ï¼ˆé€™æ¨£å¯ä»¥å°å…¥ src æ¨¡çµ„ï¼‰
# å¾ deep_agent_rag/rag/private_file_rag.py å‘ä¸Šæ‰¾åˆ° Deep_Agentic_AI_Tool æ ¹ç›®éŒ„
current_file = Path(__file__).resolve()
# å¾ deep_agent_rag/rag/private_file_rag.py å‘ä¸Šæ‰¾åˆ° Deep_Agentic_AI_Tool æ ¹ç›®éŒ„
# private_file_rag.py -> rag/ -> deep_agent_rag/ -> Deep_Agentic_AI_Tool/
# éœ€è¦å‘ä¸Š3å±‚ï¼šrag -> deep_agent_rag -> Deep_Agentic_AI_Tool
deep_agent_root = current_file.parent.parent.parent

# æª¢æŸ¥ src ç›®éŒ„æ˜¯å¦å­˜åœ¨ï¼ˆæ‡‰è©²åœ¨é …ç›®æ ¹ç›®éŒ„ä¸‹ï¼‰
src_path = deep_agent_root / "src"
if src_path.exists() and src_path.is_dir():
    # å°‡é …ç›®æ ¹ç›®éŒ„æ·»åŠ åˆ° Python è·¯å¾‘ï¼ˆä¸æ˜¯ src ç›®éŒ„æœ¬èº«ï¼‰
    # é€™æ¨£å¯ä»¥é€šé from src.xxx import xxx å°å…¥
    if str(deep_agent_root) not in sys.path:
        sys.path.insert(0, str(deep_agent_root))
    print(f"âœ“ æ‰¾åˆ°æœ¬åœ° src æ¨¡çµ„: {src_path}")
    print(f"  é …ç›®æ ¹ç›®éŒ„å·²æ·»åŠ åˆ° Python è·¯å¾‘: {deep_agent_root}")
else:
    print(f"âš ï¸ ç„¡æ³•æ‰¾åˆ° src ç›®éŒ„")
    print(f"   é æœŸè·¯å¾‘: {src_path}")
    print(f"   é …ç›®æ ¹ç›®éŒ„: {deep_agent_root}")

# å˜—è©¦å°å…¥ Learn_RAG æ¨¡çµ„
# æ³¨æ„ï¼šdocument_processor.py åœ¨é ‚å±¤å°å…¥äº† arxivï¼Œæ‰€ä»¥éœ€è¦å…ˆå®‰è£ä¾è³´
try:
    # å…ˆæª¢æŸ¥å¿…è¦çš„ä¾è³´æ˜¯å¦å·²å®‰è£
    import importlib
    
    required_deps = {
        "arxiv": "arxiv",
        "langchain_community": "langchain-community",
        "langchain_text_splitters": "langchain-text-splitters",
        "chromadb": "chromadb",
        "sentence_transformers": "sentence-transformers",
        "rank_bm25": "rank-bm25",
        "pypdf": "pypdf",
    }
    
    missing_deps = []
    for module_name, package_name in required_deps.items():
        try:
            importlib.import_module(module_name)
        except ImportError:
            missing_deps.append(package_name)
    
    if missing_deps:
        print(f"âš ï¸ ç¼ºå°‘ä»¥ä¸‹ä¾è³´åŒ…: {', '.join(missing_deps)}")
        print(f"\nğŸ’¡ è«‹å®‰è£ RAG ç³»çµ±æ‰€éœ€çš„ä¾è³´:")
        print(f"   æ–¹æ³• 1: ä½¿ç”¨ pip")
        print(f"   pip install {' '.join(missing_deps)}")
        print(f"\n   æ–¹æ³• 2: ä½¿ç”¨ uv (æ¨è–¦)")
        print(f"   cd {deep_agent_root}")
        print(f"   uv sync")
        print(f"\n   æ–¹æ³• 3: å®‰è£æ‰€æœ‰ä¾è³´")
        print(f"   pip install arxiv langchain-community langchain-text-splitters chromadb sentence-transformers rank-bm25 pypdf docx2txt langchain-experimental")
        LEARN_RAG_AVAILABLE = False
    else:
        # æ‰€æœ‰ä¾è³´éƒ½å·²å®‰è£ï¼Œå˜—è©¦å°å…¥æ¨¡çµ„
        from src.document_processor import DocumentProcessor
        from src.retrievers.bm25_retriever import BM25Retriever
        from src.retrievers.vector_retriever import VectorRetriever
        from src.retrievers.hybrid_search import HybridSearch
        from src.retrievers.reranker import Reranker, RAGPipeline
        from src.prompt_formatter import PromptFormatter
        # å°å…¥é€²éš RAG æ–¹æ³•
        from src.subquery_rag import SubQueryDecompositionRAG
        from src.hyde_rag import HyDERAG
        from src.step_back_rag import StepBackRAG
        from src.hybrid_subquery_hyde_rag import HybridSubqueryHyDERAG
        from src.triple_hybrid_rag import TripleHybridRAG
        # ä¸å†éœ€è¦å°å…¥ OllamaLLMï¼Œå› ç‚ºæˆ‘å€‘ä½¿ç”¨ Deep_Agentic_AI_Tool çš„çµ±ä¸€ LLM ç³»çµ±ï¼ˆget_llm()ï¼‰
        # from src.llm_integration import OllamaLLM
        LEARN_RAG_AVAILABLE = True
        print("âœ“ æˆåŠŸå°å…¥ RAG æ¨¡çµ„ï¼ˆæœ¬åœ°é›†æˆç‰ˆæœ¬ï¼ŒåŒ…å«é€²éš RAG æ–¹æ³•ï¼‰")
        
except ImportError as e:
    error_msg = str(e)
    print(f"âš ï¸ ç„¡æ³•å°å…¥ RAG æ¨¡çµ„: {error_msg}")
    print(f"\nğŸ’¡ è«‹å®‰è£ RAG ç³»çµ±æ‰€éœ€çš„ä¾è³´:")
    print(f"   pip install arxiv langchain-community langchain-text-splitters chromadb sentence-transformers rank-bm25 pypdf docx2txt langchain-experimental")
    print(f"\n   æˆ–è€…:")
    print(f"   cd {deep_agent_root}")
    print(f"   uv sync")
    LEARN_RAG_AVAILABLE = False
except Exception as e:
    error_msg = str(e)
    print(f"âš ï¸ å°å…¥ RAG æ¨¡çµ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {error_msg}")
    print(f"   ç•¶å‰ Python è·¯å¾‘: {sys.path[:3]}")
    print(f"   é …ç›®æ ¹ç›®éŒ„: {deep_agent_root}")
    print(f"   src ç›®éŒ„: {src_path}")
    LEARN_RAG_AVAILABLE = False


class PrivateFileRAG:
    """
    ç§æœ‰æ–‡ä»¶ RAG ç³»çµ±ç®¡ç†å™¨
    
    é€™å€‹é¡è² è²¬ç®¡ç†ç§æœ‰æ–‡ä»¶çš„ RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰ç³»çµ±ï¼ŒåŒ…æ‹¬ï¼š
    - æ–‡ä»¶è™•ç†å’Œåˆ†å¡Šï¼ˆæ”¯æŒå­—ç¬¦åˆ†å¡Šå’Œèªç¾©åˆ†å¡Šï¼‰
    - æª¢ç´¢ç³»çµ±åˆå§‹åŒ–ï¼ˆBM25 + å‘é‡æª¢ç´¢ + æ··åˆæœå°‹ï¼‰
    - RAG æŸ¥è©¢å’Œå›ç­”ç”Ÿæˆ
    
    LLM ä½¿ç”¨ç­–ç•¥ï¼š
    - ä½¿ç”¨ Deep_Agentic_AI_Tool çš„çµ±ä¸€ LLM ç³»çµ±ï¼ˆget_llm()ï¼‰
    - è‡ªå‹•éµå¾ªå„ªå…ˆé †åºï¼šGroq API > Ollama > MLX æœ¬åœ°æ¨¡å‹
    - ç„¡éœ€æ‰‹å‹•æŒ‡å®š LLM é¡å‹ï¼Œç³»çµ±æœƒæ ¹æ“šé…ç½®å’Œå¯ç”¨æ€§è‡ªå‹•é¸æ“‡æœ€åˆé©çš„ LLM
    - å¦‚æœ Groq API é¡åº¦ç”¨å®Œæˆ–æœå‹™ä¸å¯ç”¨ï¼Œæœƒè‡ªå‹•åˆ‡æ›åˆ°å‚™é¸ LLM
    """
    
    def __init__(
        self,
        use_semantic_chunking: bool = False,
        chunk_size: int = 500,  # é è¨­æ”¹ç‚º 500ï¼Œæä¾›æ›´ç´°çš„ç²’åº¦
        chunk_overlap: int = 100,  # é è¨­æ”¹ç‚º 100ï¼Œä¿æŒ 20% çš„é‡ç–Šæ¯”ä¾‹
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        persist_directory: str = "./chroma_db_private",
        # èªç¾©åˆ†å¡Šåƒæ•¸
        semantic_threshold: float = 1.0,  # é è¨­æ”¹ç‚º 1.0ï¼Œæä¾›æ›´ç´°çš„ç²’åº¦ï¼ˆåŸç‚º 1.5ï¼‰
        semantic_min_chunk_size: int = 100,  # èªç¾©åˆ†å¡Šçš„æœ€å° chunk å¤§å°ï¼ˆå­—ç¬¦æ•¸ï¼‰
        # é€²éš RAG æ–¹æ³•åƒæ•¸
        enable_adaptive_selection: bool = True,  # æ˜¯å¦å•Ÿç”¨è‡ªå‹•é¸æ“‡æœ€ä½³ RAG æ–¹æ³•
        selected_rag_method: Optional[str] = None,  # æ‰‹å‹•æŒ‡å®šæ–¹æ³•ï¼ˆå¯é¸ï¼Œå¦‚æœè¨­ç½®å‰‡è¦†è“‹è‡ªå‹•é¸æ“‡ï¼‰
        # å°è©±æ­·å²æŒä¹…åŒ–åƒæ•¸
        enable_conversation_persistence: bool = True,  # æ˜¯å¦å•Ÿç”¨å°è©±æ­·å²æŒä¹…åŒ–
        conversation_db_path: str = "./conversation_history.db",  # å°è©±æ­·å²æ•¸æ“šåº«è·¯å¾‘
        use_smart_history_retrieval: bool = True,  # æ˜¯å¦ä½¿ç”¨æ™ºèƒ½æ­·å²æª¢ç´¢ï¼ˆæ ¹æ“šæŸ¥è©¢æª¢ç´¢ç›¸é—œæ­·å²ï¼‰
        max_history_rounds: int = 10  # æœ€å¤§æ­·å²è¼ªæ•¸ï¼ˆç”¨æ–¼ prompt æ§‹å»ºï¼‰
    ):
        """
        åˆå§‹åŒ–ç§æœ‰æ–‡ä»¶ RAG ç³»çµ±
        
        Args:
            use_semantic_chunking: æ˜¯å¦ä½¿ç”¨èªç¾©åˆ†å¡Š
                                  True: ä½¿ç”¨èªç¾©åˆ†å¡Šï¼ˆä¿æŒèªç¾©å®Œæ•´æ€§ï¼Œä¸æœƒåœ¨å¥å­ä¸­é–“åˆ‡æ–·ï¼Œä½†è™•ç†æ™‚é–“è¼ƒé•·ï¼‰
                                  False: ä½¿ç”¨å­—ç¬¦åˆ†å¡Šï¼ˆå¿«é€Ÿï¼Œä½†å¯èƒ½åœ¨å¥å­ä¸­é–“åˆ‡æ–·ï¼Œé è¨­ï¼‰
            chunk_size: å­—ç¬¦åˆ†å¡Šå¤§å°ï¼ˆåƒ…ç”¨æ–¼å­—ç¬¦åˆ†å¡Šæ¨¡å¼ï¼‰
                       æ¯å€‹ chunk çš„å­—ç¬¦æ•¸ï¼Œé è¨­ 500ï¼ˆè¼ƒç´°çš„ç²’åº¦ï¼‰
                       å»ºè­°å€¼ï¼š
                       - 300-500ï¼šç´°ç²’åº¦ï¼Œé©åˆç²¾ç¢ºæª¢ç´¢ï¼Œä½†å¯èƒ½éºæ¼ä¸Šä¸‹æ–‡
                       - 500-800ï¼šä¸­ç­‰ç²’åº¦ï¼Œå¹³è¡¡ç²¾ç¢ºåº¦å’Œä¸Šä¸‹æ–‡ï¼ˆæ¨è–¦ï¼‰
                       - 800-1200ï¼šç²—ç²’åº¦ï¼ŒåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†å¯èƒ½åŒ…å«ä¸ç›¸é—œå…§å®¹
                       è¼ƒå¤§çš„å€¼å¯ä»¥åŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†å¯èƒ½åŒ…å«ä¸ç›¸é—œå…§å®¹
                       è¼ƒå°çš„å€¼æ›´ç²¾ç¢ºï¼Œä½†å¯èƒ½éºæ¼é‡è¦ä¿¡æ¯
            chunk_overlap: å­—ç¬¦åˆ†å¡Šé‡ç–Šå¤§å°ï¼ˆåƒ…ç”¨æ–¼å­—ç¬¦åˆ†å¡Šæ¨¡å¼ï¼‰
                          ç›¸é„° chunks ä¹‹é–“çš„é‡ç–Šå­—ç¬¦æ•¸ï¼Œé è¨­ 100ï¼ˆç´„ç‚º chunk_size çš„ 20%ï¼‰
                          å»ºè­°å€¼ï¼šchunk_size çš„ 15-25%
                          é‡ç–Šå¯ä»¥å¹«åŠ©ä¿æŒä¸Šä¸‹æ–‡é€£è²«æ€§ï¼Œé¿å…åœ¨é‡è¦ä¿¡æ¯é‚Šç•Œè™•åˆ‡æ–·
                          è¼ƒå¤§çš„é‡ç–Šå¯ä»¥æ›´å¥½åœ°ä¿æŒä¸Šä¸‹æ–‡ï¼Œä½†æœƒå¢åŠ  chunks æ•¸é‡
            embedding_model: Embedding æ¨¡å‹åç¨±
                            ç”¨æ–¼å‘é‡æª¢ç´¢å’Œèªç¾©åˆ†å¡Šçš„ embedding æ¨¡å‹
                            é è¨­ä½¿ç”¨ "sentence-transformers/all-MiniLM-L6-v2"ï¼ˆè¼•é‡ç´šã€å¿«é€Ÿã€æ•ˆæœå¥½ï¼‰
                            å¦‚æœéœ€è¦æ›´å¥½çš„æ•ˆæœï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼Œä½†æœƒå¢åŠ è¨ˆç®—æ™‚é–“å’Œå…§å­˜ä½¿ç”¨
            persist_directory: å‘é‡è³‡æ–™åº«æŒä¹…åŒ–ç›®éŒ„
                               ChromaDB æœƒå°‡å‘é‡è³‡æ–™åº«ä¿å­˜åœ¨æ­¤ç›®éŒ„ï¼Œä¸‹æ¬¡ä½¿ç”¨æ™‚å¯ä»¥ç›´æ¥è¼‰å…¥
                               é è¨­ç‚º "./chroma_db_private"
                               å¦‚æœç›®éŒ„å·²å­˜åœ¨ï¼Œæœƒè‡ªå‹•è¼‰å…¥å·²æœ‰çš„å‘é‡è³‡æ–™åº«
            semantic_threshold: èªç¾©åˆ†å¡Šçš„æ•æ„Ÿåº¦é–¾å€¼ï¼ˆæ¨™æº–å·®å€æ•¸ï¼‰
                               æ•¸å€¼è¶Šå°ï¼Œåˆ†å¡Šè¶Šå¤šã€è¶Šç´°ï¼ˆchunks è¶Šå°ï¼‰
                               æ•¸å€¼è¶Šå¤§ï¼Œåˆ†å¡Šè¶Šå°‘ã€è¶Šç²—ï¼ˆchunks è¶Šå¤§ï¼‰
                               å»ºè­°ç¯„åœï¼š
                               - 0.8-1.2ï¼šç´°ç²’åº¦ï¼Œé©åˆéœ€è¦ç²¾ç¢ºæª¢ç´¢çš„å ´æ™¯ï¼ˆæ¨è–¦ï¼‰
                               - 1.2-1.8ï¼šä¸­ç­‰ç²’åº¦ï¼Œå¹³è¡¡ç²¾ç¢ºåº¦å’Œä¸Šä¸‹æ–‡
                               - 1.8-2.5ï¼šç²—ç²’åº¦ï¼ŒåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†å¯èƒ½åŒ…å«ä¸ç›¸é—œå…§å®¹
                               é è¨­å€¼ï¼š1.0ï¼ˆå·²å„ªåŒ–ç‚ºæ›´ç´°çš„ç²’åº¦ï¼ŒåŸç‚º 1.5ï¼‰
            semantic_min_chunk_size: èªç¾©åˆ†å¡Šçš„æœ€å° chunk å¤§å°ï¼ˆå­—ç¬¦æ•¸ï¼‰
                                    å°æ–¼æ­¤å¤§å°çš„ chunks æœƒè¢«åˆä½µåˆ°ç›¸é„°çš„ chunks
                                    é è¨­å€¼ï¼š100 å­—ç¬¦
                                    å»ºè­°å€¼ï¼š50-200ï¼Œæ ¹æ“šæ–‡æª”é¡å‹èª¿æ•´
                                    è¼ƒå°çš„å€¼å¯ä»¥ä¿ç•™æ›´å¤šç´°ç¯€ï¼Œä½†å¯èƒ½ç”¢ç”Ÿéå¤šçš„å° chunks
        """
        if not LEARN_RAG_AVAILABLE:
            raise ImportError("Learn_RAG æ¨¡çµ„ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥å®‰è£")
        
        self.use_semantic_chunking = use_semantic_chunking
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.embedding_model = embedding_model
        self.persist_directory = persist_directory
        # èªç¾©åˆ†å¡Šåƒæ•¸
        self.semantic_threshold = semantic_threshold
        self.semantic_min_chunk_size = semantic_min_chunk_size
        # é€²éš RAG æ–¹æ³•åƒæ•¸
        self.enable_adaptive_selection = enable_adaptive_selection
        self.selected_rag_method = selected_rag_method
        
        # å°è©±æ­·å²æŒä¹…åŒ–åƒæ•¸
        # #region agent log
        try:
            with open('/Users/matthuang/Desktop/Deep_Agentic_AI_Tool/.cursor/debug.log', 'a', encoding='utf-8') as f:
                import json
                log_entry = {
                    "sessionId": "debug-session",
                    "runId": "run1",
                    "hypothesisId": "A",
                    "location": "private_file_rag.py:205",
                    "message": "__init__: é–‹å§‹åˆå§‹åŒ–å°è©±æ­·å²æŒä¹…åŒ–åƒæ•¸",
                    "data": {
                        "enable_conversation_persistence": enable_conversation_persistence,
                        "conversation_db_path": conversation_db_path,
                        "use_smart_history_retrieval": use_smart_history_retrieval,
                        "max_history_rounds": max_history_rounds
                    },
                    "timestamp": int(__import__('time').time() * 1000)
                }
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        except:
            pass
        # #endregion
        self.enable_conversation_persistence = enable_conversation_persistence
        self.conversation_db_path = conversation_db_path
        self.use_smart_history_retrieval = use_smart_history_retrieval
        self.max_history_rounds = max_history_rounds
        
        # å°è©±æ­·å²æ•¸æ“šåº«ï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰
        # #region agent log
        try:
            with open('/Users/matthuang/Desktop/Deep_Agentic_AI_Tool/.cursor/debug.log', 'a', encoding='utf-8') as f:
                import json
                log_entry = {
                    "sessionId": "debug-session",
                    "runId": "run1",
                    "hypothesisId": "B",
                    "location": "private_file_rag.py:220",
                    "message": "__init__: é–‹å§‹åˆå§‹åŒ–å°è©±æ­·å²æ•¸æ“šåº«",
                    "data": {
                        "enable_conversation_persistence": self.enable_conversation_persistence
                    },
                    "timestamp": int(__import__('time').time() * 1000)
                }
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        except:
            pass
        # #endregion
        self.conversation_db = None
        if self.enable_conversation_persistence:
            try:
                from .conversation_db import ConversationDB
                self.conversation_db = ConversationDB(db_path=self.conversation_db_path)
                # #region agent log
                try:
                    with open('/Users/matthuang/Desktop/Deep_Agentic_AI_Tool/.cursor/debug.log', 'a', encoding='utf-8') as f:
                        import json
                        log_entry = {
                            "sessionId": "debug-session",
                            "runId": "run1",
                            "hypothesisId": "B",
                            "location": "private_file_rag.py:228",
                            "message": "__init__: å°è©±æ­·å²æ•¸æ“šåº«åˆå§‹åŒ–æˆåŠŸ",
                            "data": {"db_path": self.conversation_db_path},
                            "timestamp": int(__import__('time').time() * 1000)
                        }
                        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
                except:
                    pass
                # #endregion
                print("âœ“ å°è©±æ­·å²æŒä¹…åŒ–å·²å•Ÿç”¨")
            except Exception as e:
                # #region agent log
                try:
                    with open('/Users/matthuang/Desktop/Deep_Agentic_AI_Tool/.cursor/debug.log', 'a', encoding='utf-8') as f:
                        import json
                        log_entry = {
                            "sessionId": "debug-session",
                            "runId": "run1",
                            "hypothesisId": "D",
                            "location": "private_file_rag.py:240",
                            "message": "__init__: å°è©±æ­·å²æ•¸æ“šåº«åˆå§‹åŒ–å¤±æ•—",
                            "data": {"error": str(e)},
                            "timestamp": int(__import__('time').time() * 1000)
                        }
                        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
                except:
                    pass
                # #endregion
                print(f"âš ï¸ å°è©±æ­·å²æ•¸æ“šåº«åˆå§‹åŒ–å¤±æ•—: {e}ï¼Œå°‡ä½¿ç”¨å…§å­˜æ¨¡å¼")
                self.enable_conversation_persistence = False
        
        # ç•¶å‰æœƒè©± IDï¼ˆç”¨æ–¼æŒä¹…åŒ–ï¼‰
        self.current_session_id = None
        
        # çµ„ä»¶
        self.processor = None
        self.bm25_retriever = None
        self.vector_retriever = None
        self.hybrid_search = None
        self.reranker = None
        self.rag_pipeline = None
        self.formatter = None
        self.shared_embeddings = None
        
        # é€²éš RAG æ–¹æ³•çµ„ä»¶
        self.llm_adapter = None  # LLM é©é…å™¨
        self.rag_selector = AdaptiveRAGSelector()  # æ™ºèƒ½é¸æ“‡å™¨
        self.subquery_rag = None
        self.hyde_rag = None
        self.step_back_rag = None
        self.hybrid_subquery_hyde_rag = None
        self.triple_hybrid_rag = None
        
        # ç•¶å‰è¼‰å…¥çš„æ–‡ä»¶
        self.current_files = []
        self.is_initialized = False
    
    def _init_embeddings(self):
        """
        åˆå§‹åŒ–å…±ç”¨çš„ Embedding æ¨¡å‹ï¼ˆç”¨æ–¼èªç¾©åˆ†å¡Šï¼‰
        
        é€™å€‹æ–¹æ³•æœƒå‰µå»ºä¸€å€‹ HuggingFace Embeddings æ¨¡å‹ï¼Œç”¨æ–¼ï¼š
        - èªç¾©åˆ†å¡Šï¼šè¨ˆç®—æ–‡æœ¬çš„èªç¾©ç›¸ä¼¼åº¦ï¼Œåœ¨èªç¾©é‚Šç•Œè™•åˆ‡åˆ†
        - å‘é‡æª¢ç´¢ï¼šå°‡æ–‡æª”è½‰æ›ç‚ºå‘é‡ï¼Œç”¨æ–¼èªç¾©æœå°‹
        
        å¦‚æœåˆå§‹åŒ–å¤±æ•—ï¼Œæœƒè‡ªå‹•å›é€€åˆ°å­—ç¬¦åˆ†å¡Šæ¨¡å¼ã€‚
        
        Returns:
            HuggingFaceEmbeddings å¯¦ä¾‹ï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› None
        """
        # å¦‚æœä¸éœ€è¦èªç¾©åˆ†å¡Šï¼Œç›´æ¥è¿”å› None
        if not self.use_semantic_chunking:
            return None
        
        try:
            from langchain_community.embeddings import HuggingFaceEmbeddings
            from src.retrievers.vector_retriever import get_device
            
            # ç²å– Hugging Face æ¨¡å‹ç·©å­˜ç›®éŒ„ï¼ˆå¦‚æœè¨­ç½®äº†ç’°å¢ƒè®Šæ•¸ï¼‰
            # é€™å°æ–¼ä½¿ç”¨å¤–æ¥ç¡¬ç¢Ÿå­˜å„²æ¨¡å‹å¾ˆæœ‰ç”¨
            hf_cache_dir = os.getenv("HF_CACHE_DIR", None)
            
            # è‡ªå‹•æª¢æ¸¬å¯ç”¨çš„è¨­å‚™ï¼ˆMPS/CUDA/CPUï¼‰
            # MPS: macOS GPU, CUDA: NVIDIA GPU, CPU: å‚™é¸
            device = get_device()
            
            # æ§‹å»ºæ¨¡å‹åƒæ•¸
            model_kwargs = {'device': device}
            if hf_cache_dir:
                model_kwargs['cache_dir'] = hf_cache_dir
            
            # å‰µå»º HuggingFace Embeddings æ¨¡å‹
            # normalize_embeddings=True æœƒå°‡å‘é‡æ­£è¦åŒ–ï¼Œæœ‰åŠ©æ–¼æå‡æª¢ç´¢æ•ˆæœ
            self.shared_embeddings = HuggingFaceEmbeddings(
                model_name=self.embedding_model,
                model_kwargs=model_kwargs,
                encode_kwargs={'normalize_embeddings': True}
            )
            return self.shared_embeddings
        except Exception as e:
            # å¦‚æœåˆå§‹åŒ–å¤±æ•—ï¼Œè¨˜éŒ„éŒ¯èª¤ä¸¦å›é€€åˆ°å­—ç¬¦åˆ†å¡Šæ¨¡å¼
            print(f"âš ï¸ åˆå§‹åŒ– Embedding æ¨¡å‹å¤±æ•—: {e}")
            print("   å°‡å›é€€åˆ°å­—ç¬¦åˆ†å¡Šæ¨¡å¼")
            self.use_semantic_chunking = False
            return None
    
    def process_files(self, file_paths: List[str]) -> Tuple[List[Dict], str]:
        """
        è™•ç†ä¸Šå‚³çš„æ–‡ä»¶
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾‘åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²è·¯å¾‘æˆ– Gradio æ–‡ä»¶å°è±¡ï¼‰
            
        Returns:
            (documents, status_message) å…ƒçµ„
        """
        if not file_paths:
            return [], "âŒ æœªæä¾›æ–‡ä»¶è·¯å¾‘"
        
        try:
            # è™•ç†æ–‡ä»¶è·¯å¾‘ï¼ˆå¯èƒ½æ˜¯ Gradio æ–‡ä»¶å°è±¡ï¼‰
            actual_paths = []
            for file_path in file_paths:
                if hasattr(file_path, 'name'):
                    # Gradio æ–‡ä»¶å°è±¡
                    actual_path = file_path.name
                else:
                    # å­—ç¬¦ä¸²è·¯å¾‘
                    actual_path = file_path
                
                if os.path.exists(actual_path):
                    actual_paths.append(actual_path)
                else:
                    print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {actual_path}")
            
            if not actual_paths:
                return [], "âŒ æ²’æœ‰æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾‘"
            
            # åˆå§‹åŒ– Embeddingï¼ˆå¦‚æœéœ€è¦èªç¾©åˆ†å¡Šï¼‰
            if self.use_semantic_chunking:
                self._init_embeddings()
            
            # åˆå§‹åŒ–æ–‡æª”è™•ç†å™¨
            if self.use_semantic_chunking and self.shared_embeddings:
                # ä½¿ç”¨å¯èª¿æ•´çš„èªç¾©åˆ†å¡Šåƒæ•¸
                self.processor = DocumentProcessor(
                    embeddings=self.shared_embeddings,
                    use_semantic_chunking=True,
                    breakpoint_threshold_amount=self.semantic_threshold,  # ä½¿ç”¨å¯èª¿æ•´çš„é–¾å€¼
                    min_chunk_size=self.semantic_min_chunk_size  # ä½¿ç”¨å¯èª¿æ•´çš„æœ€å° chunk å¤§å°
                )
                print(f"ğŸ“ ä½¿ç”¨èªç¾©åˆ†å¡Šï¼šthreshold={self.semantic_threshold}, min_chunk_size={self.semantic_min_chunk_size}")
            else:
                self.processor = DocumentProcessor(
                    chunk_size=self.chunk_size,
                    chunk_overlap=self.chunk_overlap
                )
            
            # è™•ç†æ‰€æœ‰æ–‡ä»¶
            all_documents = []
            for file_path in actual_paths:
                print(f"è™•ç†æ–‡ä»¶: {file_path}")
                documents = self.processor.process_file(file_path)
                all_documents.extend(documents)
                print(f"  âœ“ å‰µå»ºäº† {len(documents)} å€‹ chunks")
            
            if not all_documents:
                return [], "âŒ è™•ç†å¾Œæ²’æœ‰æ–‡æª”å…§å®¹"
            
            self.current_files = actual_paths
            
            # åˆå§‹åŒ–æˆ–æ›´æ–°æœƒè©±ï¼ˆå¦‚æœå•Ÿç”¨æŒä¹…åŒ–ï¼‰
            # #region agent log
            try:
                with open('/Users/matthuang/Desktop/Deep_Agentic_AI_Tool/.cursor/debug.log', 'a', encoding='utf-8') as f:
                    import json
                    log_entry = {
                        "sessionId": "debug-session",
                        "runId": "run1",
                        "hypothesisId": "A",
                        "location": "private_file_rag.py:344",
                        "message": "process_files: æª¢æŸ¥å°è©±æ­·å²æŒä¹…åŒ–å±¬æ€§",
                        "data": {
                            "has_enable_conversation_persistence": hasattr(self, 'enable_conversation_persistence'),
                            "has_conversation_db": hasattr(self, 'conversation_db'),
                            "enable_conversation_persistence_value": getattr(self, 'enable_conversation_persistence', 'NOT_FOUND'),
                            "conversation_db_value": str(getattr(self, 'conversation_db', 'NOT_FOUND'))
                        },
                        "timestamp": int(__import__('time').time() * 1000)
                    }
                    f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
            except Exception as log_err:
                print(f"Log error: {log_err}")
            # #endregion
            if self.enable_conversation_persistence and self.conversation_db:
                self.current_session_id = self.conversation_db.get_or_create_session(actual_paths)
                print(f"âœ“ æœƒè©± ID: {self.current_session_id}")
            
            # åˆå§‹åŒ–æª¢ç´¢ç³»çµ±
            status_msg = self._init_retrievers(all_documents)
            
            return all_documents, status_msg
            
        except Exception as e:
            error_msg = f"âŒ è™•ç†æ–‡ä»¶å¤±æ•—: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return [], error_msg
    
    def _init_retrievers(self, documents: List[Dict]) -> str:
        """
        åˆå§‹åŒ–æª¢ç´¢å™¨
        
        Args:
            documents: æ–‡æª”åˆ—è¡¨
            
        Returns:
            ç‹€æ…‹è¨Šæ¯
        """
        try:
            # åˆå§‹åŒ– BM25 æª¢ç´¢å™¨
            print("  - åˆå§‹åŒ– BM25 æª¢ç´¢å™¨...")
            self.bm25_retriever = BM25Retriever(documents)
            
            # åˆå§‹åŒ–å‘é‡æª¢ç´¢å™¨
            print("  - åˆå§‹åŒ–å‘é‡æª¢ç´¢å™¨...")
            self.vector_retriever = VectorRetriever(
                documents,
                embedding_model=self.embedding_model,
                persist_directory=self.persist_directory,
                embeddings=self.shared_embeddings
            )
            
            # åˆå§‹åŒ–æ··åˆæœå°‹
            print("  - åˆå§‹åŒ–æ··åˆæœå°‹...")
            self.hybrid_search = HybridSearch(
                sparse_retriever=self.bm25_retriever,
                dense_retriever=self.vector_retriever,
                fusion_method="rrf",
                rrf_k=60
            )
            
            # å˜—è©¦åˆå§‹åŒ–é‡æ’åºå™¨ï¼ˆå¯é¸ï¼‰
            try:
                print("  - åˆå§‹åŒ–é‡æ’åºå™¨...")
                self.reranker = Reranker(
                    model_name="BAAI/bge-reranker-base",
                    batch_size=16
                )
                
                # åˆå§‹åŒ– RAG ç®¡ç·š
                print("  - åˆå§‹åŒ– RAG ç®¡ç·š...")
                self.rag_pipeline = RAGPipeline(
                    hybrid_search=self.hybrid_search,
                    reranker=self.reranker,
                    recall_k=20,
                    adaptive_recall=True
                )
                
                # åˆå§‹åŒ– Prompt æ ¼å¼åŒ–å™¨
                self.formatter = PromptFormatter(format_style="detailed")
                
                # åˆå§‹åŒ–é€²éš RAG æ–¹æ³•ï¼ˆå»¶é²åˆå§‹åŒ–ï¼Œåªåœ¨éœ€è¦æ™‚å‰µå»ºï¼‰
                self._init_advanced_rag_methods()
                
                self.is_initialized = True
                return f"âœ… æˆåŠŸè™•ç† {len(self.current_files)} å€‹æ–‡ä»¶ï¼Œå‰µå»ºäº† {len(documents)} å€‹ chunksï¼ŒRAG ç³»çµ±å·²åˆå§‹åŒ–ï¼ˆåŒ…å«é‡æ’åºï¼‰"
                
            except Exception as e:
                print(f"  âš ï¸ é‡æ’åºå™¨åˆå§‹åŒ–å¤±æ•—: {e}")
                print("   å°‡ä½¿ç”¨æ··åˆæœå°‹ï¼ˆä¸é€²è¡Œé‡æ’åºï¼‰")
                self.formatter = PromptFormatter(format_style="detailed")
                
                # å³ä½¿é‡æ’åºå¤±æ•—ï¼Œä¹Ÿåˆå§‹åŒ–é€²éš RAG æ–¹æ³•
                self._init_advanced_rag_methods()
                
                self.is_initialized = True
                return f"âœ… æˆåŠŸè™•ç† {len(self.current_files)} å€‹æ–‡ä»¶ï¼Œå‰µå»ºäº† {len(documents)} å€‹ chunksï¼ŒRAG ç³»çµ±å·²åˆå§‹åŒ–ï¼ˆç„¡é‡æ’åºï¼‰"
                
        except Exception as e:
            error_msg = f"âŒ æª¢ç´¢ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return error_msg
    
    def _init_advanced_rag_methods(self):
        """
        åˆå§‹åŒ–æ‰€æœ‰é€²éš RAG æ–¹æ³•
        
        é€™å€‹æ–¹æ³•æœƒå‰µå»º LLM é©é…å™¨ä¸¦åˆå§‹åŒ–æ‰€æœ‰ 5 ç¨®é€²éš RAG æ–¹æ³•å¯¦ä¾‹
        ä½¿ç”¨å»¶é²åˆå§‹åŒ–ç­–ç•¥ï¼Œåªåœ¨éœ€è¦æ™‚å‰µå»º
        """
        try:
            # å‰µå»º LLM é©é…å™¨ï¼ˆå°‡ LangChain ChatModel åŒ…è£æˆ OllamaLLM æ¥å£ï¼‰
            if self.llm_adapter is None:
                print("  - å‰µå»º LLM é©é…å™¨...")
                langchain_llm = get_llm()
                self.llm_adapter = LangChainLLMAdapter(langchain_llm)
                print("    âœ“ LLM é©é…å™¨å‰µå»ºå®Œæˆ")
            
            # ç¢ºä¿æœ‰å¿…è¦çš„çµ„ä»¶
            if not self.rag_pipeline:
                print("  âš ï¸ RAG Pipeline æœªåˆå§‹åŒ–ï¼Œç„¡æ³•å‰µå»ºé€²éš RAG æ–¹æ³•")
                return
            
            if not self.vector_retriever:
                print("  âš ï¸ Vector Retriever æœªåˆå§‹åŒ–ï¼Œç„¡æ³•å‰µå»ºé€²éš RAG æ–¹æ³•")
                return
            
            # åˆå§‹åŒ– SubQuery RAG
            if self.subquery_rag is None:
                try:
                    print("  - åˆå§‹åŒ– SubQuery RAG...")
                    self.subquery_rag = SubQueryDecompositionRAG(
                        rag_pipeline=self.rag_pipeline,
                        llm=self.llm_adapter,
                        max_sub_queries=3,
                        top_k_per_subquery=5,
                        enable_parallel=True
                    )
                    print("    âœ“ SubQuery RAG åˆå§‹åŒ–å®Œæˆ")
                except Exception as e:
                    print(f"    âš ï¸ SubQuery RAG åˆå§‹åŒ–å¤±æ•—: {e}")
            
            # åˆå§‹åŒ– HyDE RAG
            if self.hyde_rag is None:
                try:
                    print("  - åˆå§‹åŒ– HyDE RAG...")
                    self.hyde_rag = HyDERAG(
                        rag_pipeline=self.rag_pipeline,
                        vector_retriever=self.vector_retriever,
                        llm=self.llm_adapter,
                        hypothetical_length=200,
                        temperature=0.7
                    )
                    print("    âœ“ HyDE RAG åˆå§‹åŒ–å®Œæˆ")
                except Exception as e:
                    print(f"    âš ï¸ HyDE RAG åˆå§‹åŒ–å¤±æ•—: {e}")
            
            # åˆå§‹åŒ– Step-back RAG
            if self.step_back_rag is None:
                try:
                    print("  - åˆå§‹åŒ– Step-back RAG...")
                    self.step_back_rag = StepBackRAG(
                        rag_pipeline=self.rag_pipeline,
                        vector_retriever=self.vector_retriever,
                        llm=self.llm_adapter,
                        step_back_temperature=0.3,
                        answer_temperature=0.7,
                        enable_parallel=True
                    )
                    print("    âœ“ Step-back RAG åˆå§‹åŒ–å®Œæˆ")
                except Exception as e:
                    print(f"    âš ï¸ Step-back RAG åˆå§‹åŒ–å¤±æ•—: {e}")
            
            # åˆå§‹åŒ– Hybrid Subquery+HyDE RAG
            if self.hybrid_subquery_hyde_rag is None:
                try:
                    print("  - åˆå§‹åŒ– Hybrid Subquery+HyDE RAG...")
                    self.hybrid_subquery_hyde_rag = HybridSubqueryHyDERAG(
                        rag_pipeline=self.rag_pipeline,
                        vector_retriever=self.vector_retriever,
                        llm=self.llm_adapter,
                        max_sub_queries=3,
                        top_k_per_subquery=5,
                        hypothetical_length=200,
                        temperature_subquery=0.3,
                        temperature_hyde=0.7,
                        enable_parallel=True
                    )
                    print("    âœ“ Hybrid Subquery+HyDE RAG åˆå§‹åŒ–å®Œæˆ")
                except Exception as e:
                    print(f"    âš ï¸ Hybrid Subquery+HyDE RAG åˆå§‹åŒ–å¤±æ•—: {e}")
            
            # åˆå§‹åŒ– Triple Hybrid RAG
            if self.triple_hybrid_rag is None:
                try:
                    print("  - åˆå§‹åŒ– Triple Hybrid RAG...")
                    self.triple_hybrid_rag = TripleHybridRAG(
                        rag_pipeline=self.rag_pipeline,
                        vector_retriever=self.vector_retriever,
                        llm=self.llm_adapter,
                        max_sub_queries=3,
                        top_k_per_subquery=5,
                        hypothetical_length=200,
                        temperature_subquery=0.3,
                        temperature_hyde=0.7,
                        temperature_stepback=0.3,
                        answer_temperature=0.7,
                        enable_parallel=True
                    )
                    print("    âœ“ Triple Hybrid RAG åˆå§‹åŒ–å®Œæˆ")
                except Exception as e:
                    print(f"    âš ï¸ Triple Hybrid RAG åˆå§‹åŒ–å¤±æ•—: {e}")
            
            print("  âœ… æ‰€æœ‰é€²éš RAG æ–¹æ³•åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"  âš ï¸ åˆå§‹åŒ–é€²éš RAG æ–¹æ³•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
    
    def query(
        self,
        query: str,
        top_k: int = 3,
        use_llm: bool = True,
        llm_model: Optional[str] = None,
        conversation_history: Optional[List[Tuple[str, str]]] = None
    ) -> Dict:
        """
        æŸ¥è©¢ RAG ç³»çµ±ä¸¦ç”Ÿæˆå›ç­”
        
        é€™å€‹æ–¹æ³•æœƒåŸ·è¡Œå®Œæ•´çš„ RAG æµç¨‹ï¼š
        1. ä½¿ç”¨æ··åˆæœå°‹ï¼ˆBM25 + å‘é‡æª¢ç´¢ï¼‰æª¢ç´¢ç›¸é—œæ–‡æª”ç‰‡æ®µ
        2. å¯é¸ï¼šä½¿ç”¨é‡æ’åºå™¨é€²ä¸€æ­¥å„ªåŒ–çµæœï¼ˆå¦‚æœå·²åˆå§‹åŒ–ï¼‰
        3. æ ¼å¼åŒ–æª¢ç´¢çµæœç‚º LLM å¯è®€çš„ä¸Šä¸‹æ–‡
        4. ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        
        LLM é¸æ“‡ç­–ç•¥ï¼š
        - è‡ªå‹•ä½¿ç”¨ get_llm() ç²å– LLM å¯¦ä¾‹
        - å„ªå…ˆé †åºï¼šGroq API > Ollama > MLX æœ¬åœ°æ¨¡å‹
        - ç„¡éœ€æ‰‹å‹•æŒ‡å®š LLM é¡å‹ï¼Œç³»çµ±æœƒæ ¹æ“šé…ç½®å’Œå¯ç”¨æ€§è‡ªå‹•é¸æ“‡æœ€åˆé©çš„
        - å¦‚æœç•¶å‰ LLM å¤±æ•—ï¼ˆå¦‚ API é¡åº¦ç”¨å®Œï¼‰ï¼Œæœƒè‡ªå‹•åˆ‡æ›åˆ°å‚™é¸ LLM
        
        Args:
            query: æŸ¥è©¢å•é¡Œï¼ˆç”¨æˆ¶æƒ³è¦å•çš„å•é¡Œï¼‰
                  ä¾‹å¦‚ï¼š"é€™ä»½æ–‡æª”çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ"
            top_k: è¿”å›çš„çµæœæ•¸é‡ï¼ˆæª¢ç´¢åˆ°çš„æ–‡æª”ç‰‡æ®µæ•¸é‡ï¼‰
                  å»ºè­°å€¼ï¼š3-5ï¼ˆå¤ªå°‘å¯èƒ½éºæ¼é‡è¦ä¿¡æ¯ï¼Œå¤ªå¤šå¯èƒ½åŒ…å«ä¸ç›¸é—œå…§å®¹ï¼‰
                  é è¨­å€¼ï¼š3
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
                    True: ä½¿ç”¨ LLM åŸºæ–¼æª¢ç´¢çµæœç”Ÿæˆå®Œæ•´ã€é€£è²«çš„å›ç­”ï¼ˆæ¨è–¦ï¼‰
                    False: åªè¿”å›æª¢ç´¢åˆ°çš„æ–‡æª”ç‰‡æ®µï¼Œä¸ç”Ÿæˆå›ç­”ï¼ˆé©åˆå¿«é€ŸæŸ¥çœ‹ç›¸é—œå…§å®¹ï¼‰
            llm_model: LLM æ¨¡å‹åç¨±ï¼ˆå·²å»¢æ£„ï¼Œä¸å†ä½¿ç”¨ï¼‰
                      ç¾åœ¨çµ±ä¸€ä½¿ç”¨ get_llm() è‡ªå‹•é¸æ“‡ LLMï¼Œéµå¾ª Groq -> Ollama -> MLX çš„å„ªå…ˆé †åº
                      ä¿ç•™æ­¤åƒæ•¸åƒ…ç‚ºå‘å¾Œå…¼å®¹ï¼Œå¯¦éš›ä¸æœƒè¢«ä½¿ç”¨
            
        Returns:
            åŒ…å«ä»¥ä¸‹å…§å®¹çš„å­—å…¸ï¼š
            - success: æ˜¯å¦æˆåŠŸï¼ˆboolï¼‰
            - query: åŸå§‹æŸ¥è©¢å•é¡Œï¼ˆstrï¼‰
            - answer: LLM ç”Ÿæˆçš„å›ç­”ï¼ˆstrï¼Œå¦‚æœ use_llm=True ä¸”æˆåŠŸï¼‰
            - results: æª¢ç´¢åˆ°çš„æ–‡æª”ç‰‡æ®µåˆ—è¡¨ï¼ˆList[Dict]ï¼‰ï¼Œæ¯å€‹ç‰‡æ®µåŒ…å«ï¼š
              * content: æ–‡æª”å…§å®¹ï¼ˆstrï¼‰
              * metadata: å…ƒæ•¸æ“šï¼ˆDictï¼‰ï¼ŒåŒ…å«æ¨™é¡Œã€æ–‡ä»¶è·¯å¾‘ç­‰
              * score: ç›¸é—œæ€§åˆ†æ•¸ï¼ˆfloatï¼‰
            - formatted_context: æ ¼å¼åŒ–å¾Œçš„ä¸Šä¸‹æ–‡ï¼ˆstrï¼‰ï¼Œç”¨æ–¼ LLM ç”Ÿæˆå›ç­”
            - stats: æª¢ç´¢çµ±è¨ˆä¿¡æ¯ï¼ˆDictï¼‰ï¼ŒåŒ…å«ï¼š
              * total_time: ç¸½è€—æ™‚ï¼ˆfloatï¼Œç§’ï¼‰
              * recall_time: å¬å›éšæ®µè€—æ™‚ï¼ˆfloatï¼Œç§’ï¼‰
              * rerank_time: é‡æ’åºéšæ®µè€—æ™‚ï¼ˆfloatï¼Œç§’ï¼Œå¦‚æœæœ‰é‡æ’åºï¼‰
            - document_type: æª¢æ¸¬åˆ°çš„æ–‡æª”é¡å‹ï¼ˆstrï¼‰
                            "paper": å­¸è¡“è«–æ–‡
                            "cv": ç°¡æ­·/å±¥æ­·
                            "general": é€šç”¨æ–‡æª”ï¼ˆé è¨­ï¼‰
            - error: éŒ¯èª¤è¨Šæ¯ï¼ˆstrï¼Œå¦‚æœå¤±æ•—ï¼‰
        """
        if not self.is_initialized:
            return {
                "success": False,
                "error": "RAG ç³»çµ±å°šæœªåˆå§‹åŒ–ï¼Œè«‹å…ˆä¸Šå‚³æ–‡ä»¶"
            }
        
        try:
            # é¸æ“‡ RAG æ–¹æ³•
            selected_method = None
            method_reason = ""
            
            if self.enable_adaptive_selection and self.selected_rag_method is None:
                # è‡ªå‹•é¸æ“‡æœ€ä½³æ–¹æ³•
                query_features = self.rag_selector.analyze_query(query)
                file_features = self.rag_selector.analyze_files(self.current_files, None)
                selected_method = self.rag_selector.select_best_method(
                    query_features, 
                    file_features,
                    enable_advanced=True
                )
                method_reason = self.rag_selector.get_method_reason(
                    selected_method, 
                    query_features, 
                    file_features
                )
                print(f"ğŸ” è‡ªå‹•é¸æ“‡ RAG æ–¹æ³•: {selected_method.value}")
                print(f"   ç†ç”±: {method_reason}")
            elif self.selected_rag_method:
                # æ‰‹å‹•æŒ‡å®šæ–¹æ³•
                try:
                    selected_method = RAGMethod(self.selected_rag_method)
                    method_reason = f"æ‰‹å‹•é¸æ“‡: {selected_method.value}"
                    print(f"ğŸ” ä½¿ç”¨æ‰‹å‹•æŒ‡å®šçš„ RAG æ–¹æ³•: {selected_method.value}")
                except ValueError:
                    print(f"âš ï¸ ç„¡æ•ˆçš„ RAG æ–¹æ³•: {self.selected_rag_method}ï¼Œä½¿ç”¨åŸºç¤æ–¹æ³•")
                    selected_method = RAGMethod.BASIC
                    method_reason = "ç„¡æ•ˆæ–¹æ³•ï¼Œå›é€€åˆ°åŸºç¤æ–¹æ³•"
            else:
                # ä½¿ç”¨åŸºç¤æ–¹æ³•
                selected_method = RAGMethod.BASIC
                method_reason = "ä½¿ç”¨åŸºç¤ RAG æ–¹æ³•"
            
            # æ ¹æ“šé¸æ“‡çš„æ–¹æ³•åŸ·è¡ŒæŸ¥è©¢
            if selected_method == RAGMethod.BASIC:
                # ä½¿ç”¨åŸºç¤ RAG æ–¹æ³•ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
                return self._query_basic(query, top_k, use_llm, conversation_history)
            else:
                # ä½¿ç”¨é€²éš RAG æ–¹æ³•
                return self._query_advanced(query, top_k, use_llm, selected_method, method_reason, conversation_history)
                
        except Exception as e:
            error_msg = f"âŒ æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": error_msg,
                "query": query
            }
    
    def query_stream(
        self,
        query: str,
        top_k: int = 3,
        conversation_history: Optional[List[Tuple[str, str]]] = None
    ):
        """
        æµå¼æŸ¥è©¢ RAG ç³»çµ±ä¸¦é€æ­¥ç”Ÿæˆå›ç­”ï¼ˆé€å­—è¼¸å‡ºï¼‰
        
        é€™å€‹æ–¹æ³•æœƒåŸ·è¡Œå®Œæ•´çš„ RAG æµç¨‹ï¼Œä½†ä½¿ç”¨æµå¼ LLM è¼¸å‡ºï¼š
        1. ä½¿ç”¨æ··åˆæœå°‹ï¼ˆBM25 + å‘é‡æª¢ç´¢ï¼‰æª¢ç´¢ç›¸é—œæ–‡æª”ç‰‡æ®µ
        2. å¯é¸ï¼šä½¿ç”¨é‡æ’åºå™¨é€²ä¸€æ­¥å„ªåŒ–çµæœï¼ˆå¦‚æœå·²åˆå§‹åŒ–ï¼‰
        3. æ ¼å¼åŒ–æª¢ç´¢çµæœç‚º LLM å¯è®€çš„ä¸Šä¸‹æ–‡
        4. ä½¿ç”¨ LLM æµå¼ç”Ÿæˆå›ç­”ï¼ˆé€å­—è¼¸å‡ºï¼‰
        
        Args:
            query: æŸ¥è©¢å•é¡Œï¼ˆç”¨æˆ¶æƒ³è¦å•çš„å•é¡Œï¼‰
            top_k: è¿”å›çš„çµæœæ•¸é‡ï¼ˆæª¢ç´¢åˆ°çš„æ–‡æª”ç‰‡æ®µæ•¸é‡ï¼‰
            conversation_history: å¯é¸çš„å°è©±æ­·å²ï¼Œæ ¼å¼ç‚º List[Tuple[str, str]]
        
        Yields:
            åŒ…å«ä»¥ä¸‹å…§å®¹çš„å­—å…¸ï¼š
            - success: æ˜¯å¦æˆåŠŸï¼ˆboolï¼‰
            - answer: ç•¶å‰ç´¯ç©çš„å›ç­”ï¼ˆstrï¼Œé€æ­¥æ›´æ–°ï¼‰
            - query: åŸå§‹æŸ¥è©¢å•é¡Œï¼ˆstrï¼‰
            - results: æª¢ç´¢åˆ°çš„æ–‡æª”ç‰‡æ®µåˆ—è¡¨ï¼ˆList[Dict]ï¼‰
            - formatted_context: æ ¼å¼åŒ–å¾Œçš„ä¸Šä¸‹æ–‡ï¼ˆstrï¼‰
            - stats: æª¢ç´¢çµ±è¨ˆä¿¡æ¯ï¼ˆDictï¼‰
            - document_type: æª¢æ¸¬åˆ°çš„æ–‡æª”é¡å‹ï¼ˆstrï¼‰
            - rag_method: ä½¿ç”¨çš„ RAG æ–¹æ³•ï¼ˆstrï¼‰
            - method_reason: æ–¹æ³•é¸æ“‡ç†ç”±ï¼ˆstrï¼‰
            - error: éŒ¯èª¤è¨Šæ¯ï¼ˆstrï¼Œå¦‚æœå¤±æ•—ï¼‰
        """
        if not self.is_initialized:
            yield {
                "success": False,
                "error": "RAG ç³»çµ±å°šæœªåˆå§‹åŒ–ï¼Œè«‹å…ˆä¸Šå‚³æ–‡ä»¶",
                "answer": ""
            }
            return
        
        try:
            # é¸æ“‡ RAG æ–¹æ³•ï¼ˆèˆ‡ query æ–¹æ³•ç›¸åŒçš„é‚è¼¯ï¼‰
            selected_method = None
            method_reason = ""
            
            if self.enable_adaptive_selection and self.selected_rag_method is None:
                # è‡ªå‹•é¸æ“‡æœ€ä½³æ–¹æ³•
                query_features = self.rag_selector.analyze_query(query)
                file_features = self.rag_selector.analyze_files(self.current_files, None)
                selected_method = self.rag_selector.select_best_method(
                    query_features, 
                    file_features,
                    enable_advanced=True
                )
                method_reason = self.rag_selector.get_method_reason(
                    selected_method, 
                    query_features, 
                    file_features
                )
                print(f"ğŸ” è‡ªå‹•é¸æ“‡ RAG æ–¹æ³•: {selected_method.value}")
                print(f"   ç†ç”±: {method_reason}")
            elif self.selected_rag_method:
                # æ‰‹å‹•æŒ‡å®šæ–¹æ³•
                try:
                    selected_method = RAGMethod(self.selected_rag_method)
                    method_reason = f"æ‰‹å‹•é¸æ“‡: {selected_method.value}"
                    print(f"ğŸ” ä½¿ç”¨æ‰‹å‹•æŒ‡å®šçš„ RAG æ–¹æ³•: {selected_method.value}")
                except ValueError:
                    print(f"âš ï¸ ç„¡æ•ˆçš„ RAG æ–¹æ³•: {self.selected_rag_method}ï¼Œä½¿ç”¨åŸºç¤æ–¹æ³•")
                    selected_method = RAGMethod.BASIC
                    method_reason = "ç„¡æ•ˆæ–¹æ³•ï¼Œå›é€€åˆ°åŸºç¤æ–¹æ³•"
            else:
                # ä½¿ç”¨åŸºç¤æ–¹æ³•
                selected_method = RAGMethod.BASIC
                method_reason = "ä½¿ç”¨åŸºç¤ RAG æ–¹æ³•"
            
            # ç›®å‰åªæ”¯æŒåŸºç¤æ–¹æ³•çš„æµå¼è¼¸å‡º
            if selected_method != RAGMethod.BASIC:
                # å°æ–¼é€²éšæ–¹æ³•ï¼Œå›é€€åˆ°éæµå¼æŸ¥è©¢
                result = self._query_advanced(query, top_k, True, selected_method, method_reason, conversation_history)
                if result.get("success"):
                    answer = result.get("answer", "")
                    # é€å­—è¼¸å‡º
                    accumulated = ""
                    for char in answer:
                        accumulated += char
                        yield {
                            "success": True,
                            "answer": accumulated,
                            "query": query,
                            "results": result.get("results", []),
                            "formatted_context": result.get("formatted_context", ""),
                            "stats": result.get("stats", {}),
                            "document_type": result.get("document_type", "general"),
                            "rag_method": result.get("rag_method", "basic"),
                            "method_reason": method_reason
                        }
                        time.sleep(0.01)  # æ¯å­—ç¬¦å»¶é² 10 æ¯«ç§’
                else:
                    yield result
                return
            
            # ä½¿ç”¨åŸºç¤ RAG æ–¹æ³•çš„æµå¼è¼¸å‡º
            # æª¢ç´¢ç›¸é—œæ–‡æª”
            if self.rag_pipeline:
                # ä½¿ç”¨å®Œæ•´çš„ RAG ç®¡ç·šï¼ˆåŒ…å«é‡æ’åºï¼‰
                results, stats = self.rag_pipeline.query(
                    text=query,
                    top_k=top_k,
                    enable_rerank=True,
                    return_stats=True
                )
            else:
                # åƒ…ä½¿ç”¨æ··åˆæœå°‹
                results = self.hybrid_search.retrieve(query, top_k=top_k)
                stats = {"total_time": 0, "recall_time": 0, "rerank_time": 0}
            
            if not results:
                yield {
                    "success": False,
                    "error": "æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”ç‰‡æ®µ",
                    "answer": "",
                    "results": [],
                    "rag_method": "basic",
                    "method_reason": "åŸºç¤ RAG æ–¹æ³•"
                }
                return
            
            # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            formatted_context = self.formatter.format_context(
                results,
                format_style="detailed"
            )
            
            # æª¢æ¸¬æ–‡æª”é¡å‹
            document_type = self._detect_document_type(results)
            
            # ä½¿ç”¨æµå¼ LLM ç”Ÿæˆå›ç­”
            try:
                # ä½¿ç”¨ Deep_Agentic_AI_Tool çš„çµ±ä¸€ LLM ç³»çµ±
                llm = get_llm()
                
                # æ§‹å»ºåŒ…å«å°è©±æ­·å²çš„ prompt
                prompt = self._build_prompt_with_history(
                    query,
                    formatted_context,
                    document_type,
                    conversation_history
                )
                
                messages = [HumanMessage(content=prompt)]
                
                # å˜—è©¦ä½¿ç”¨æµå¼è¼¸å‡º
                accumulated_answer = ""
                try:
                    # æª¢æŸ¥ LLM æ˜¯å¦æ”¯æŒ stream æ–¹æ³•
                    if hasattr(llm, 'stream'):
                        # ä½¿ç”¨æµå¼è¼¸å‡º
                        for chunk in llm.stream(messages):
                            if hasattr(chunk, 'content'):
                                chunk_text = chunk.content
                            elif isinstance(chunk, str):
                                chunk_text = chunk
                            else:
                                chunk_text = str(chunk)
                            
                            if chunk_text:
                                accumulated_answer += chunk_text
                                yield {
                                    "success": True,
                                    "answer": accumulated_answer,
                                    "query": query,
                                    "results": results,
                                    "formatted_context": formatted_context,
                                    "stats": stats,
                                    "document_type": document_type,
                                    "rag_method": "basic",
                                    "method_reason": "åŸºç¤ RAG æ–¹æ³•"
                                }
                    else:
                        # å¦‚æœä¸æ”¯æŒæµå¼è¼¸å‡ºï¼Œä½¿ç”¨ invoke ç„¶å¾Œé€å­—è¼¸å‡º
                        response = llm.invoke(messages)
                        answer = response.content if hasattr(response, 'content') else str(response)
                        
                        # é€å­—è¼¸å‡º
                        for char in answer:
                            accumulated_answer += char
                            yield {
                                "success": True,
                                "answer": accumulated_answer,
                                "query": query,
                                "results": results,
                                "formatted_context": formatted_context,
                                "stats": stats,
                                "document_type": document_type,
                                "rag_method": "basic",
                                "method_reason": "åŸºç¤ RAG æ–¹æ³•"
                            }
                            time.sleep(0.01)  # æ¯å­—ç¬¦å»¶é² 10 æ¯«ç§’
                except Exception as stream_error:
                    # å¦‚æœæµå¼è¼¸å‡ºå¤±æ•—ï¼Œå›é€€åˆ°éæµå¼
                    print(f"âš ï¸ æµå¼è¼¸å‡ºå¤±æ•—ï¼Œä½¿ç”¨éæµå¼: {stream_error}")
                    response = llm.invoke(messages)
                    answer = response.content if hasattr(response, 'content') else str(response)
                    
                    # é€å­—è¼¸å‡º
                    for char in answer:
                        accumulated_answer += char
                        yield {
                            "success": True,
                            "answer": accumulated_answer,
                            "query": query,
                            "results": results,
                            "formatted_context": formatted_context,
                            "stats": stats,
                            "document_type": document_type,
                            "rag_method": "basic",
                            "method_reason": "åŸºç¤ RAG æ–¹æ³•"
                        }
                        time.sleep(0.01)  # æ¯å­—ç¬¦å»¶é² 10 æ¯«ç§’
                
            except Exception as e:
                print(f"âš ï¸ LLM ç”Ÿæˆå›ç­”å¤±æ•—: {e}")
                import traceback
                traceback.print_exc()
                yield {
                    "success": False,
                    "error": f"LLM ç”Ÿæˆå›ç­”å¤±æ•—: {str(e)}",
                    "answer": "",
                    "query": query,
                    "rag_method": "basic"
                }
                
        except Exception as e:
            error_msg = f"âŒ æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            yield {
                "success": False,
                "error": error_msg,
                "answer": "",
                "query": query
            }
    
    def _query_basic(self, query: str, top_k: int, use_llm: bool, conversation_history: Optional[List[Tuple[str, str]]] = None) -> Dict:
        """
        ä½¿ç”¨åŸºç¤ RAG æ–¹æ³•æŸ¥è©¢ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
        
        Args:
            query: æŸ¥è©¢å•é¡Œ
            top_k: è¿”å›çµæœæ•¸é‡
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
            conversation_history: å¯é¸çš„å°è©±æ­·å²ï¼Œæ ¼å¼ç‚º List[Tuple[str, str]]ï¼Œæ¯å€‹å…ƒçµ„ç‚º (ç”¨æˆ¶å•é¡Œ, AIå›ç­”)
        """
        try:
            # æª¢ç´¢ç›¸é—œæ–‡æª”
            if self.rag_pipeline:
                # ä½¿ç”¨å®Œæ•´çš„ RAG ç®¡ç·šï¼ˆåŒ…å«é‡æ’åºï¼‰
                results, stats = self.rag_pipeline.query(
                    text=query,
                    top_k=top_k,
                    enable_rerank=True,
                    return_stats=True
                )
            else:
                # åƒ…ä½¿ç”¨æ··åˆæœå°‹
                results = self.hybrid_search.retrieve(query, top_k=top_k)
                stats = {"total_time": 0, "recall_time": 0, "rerank_time": 0}
            
            if not results:
                return {
                    "success": False,
                    "error": "æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”ç‰‡æ®µ",
                    "results": [],
                    "rag_method": "basic",
                    "method_reason": "åŸºç¤ RAG æ–¹æ³•"
                }
            
            # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            formatted_context = self.formatter.format_context(
                results,
                format_style="detailed"
            )
            
            # æª¢æ¸¬æ–‡æª”é¡å‹
            document_type = self._detect_document_type(results)
            
            # ç”Ÿæˆå›ç­”ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            answer = None
            if use_llm:
                try:
                    # ä½¿ç”¨ Deep_Agentic_AI_Tool çš„çµ±ä¸€ LLM ç³»çµ±
                    llm = get_llm()
                    
                    # æ§‹å»ºåŒ…å«å°è©±æ­·å²çš„ prompt
                    prompt = self._build_prompt_with_history(
                        query,
                        formatted_context,
                        document_type,
                        conversation_history
                    )
                    
                    messages = [HumanMessage(content=prompt)]
                    response = llm.invoke(messages)
                    answer = response.content
                    
                except Exception as e:
                    print(f"âš ï¸ LLM ç”Ÿæˆå›ç­”å¤±æ•—: {e}")
                    import traceback
                    traceback.print_exc()
                    answer = None
            
            return {
                "success": True,
                "query": query,
                "answer": answer,
                "results": results,
                "formatted_context": formatted_context,
                "stats": stats,
                "document_type": document_type,
                "rag_method": "basic",
                "method_reason": "åŸºç¤ RAG æ–¹æ³•"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"åŸºç¤ RAG æŸ¥è©¢å¤±æ•—: {str(e)}",
                "query": query,
                "rag_method": "basic"
            }
    
    def _query_advanced(
        self, 
        query: str, 
        top_k: int, 
        use_llm: bool, 
        method: RAGMethod,
        method_reason: str,
        conversation_history: Optional[List[Tuple[str, str]]] = None
    ) -> Dict:
        """
        ä½¿ç”¨é€²éš RAG æ–¹æ³•æŸ¥è©¢
        
        Args:
            query: æŸ¥è©¢å•é¡Œ
            top_k: è¿”å›çµæœæ•¸é‡
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
            method: RAG æ–¹æ³•
            method_reason: æ–¹æ³•é¸æ“‡ç†ç”±
            conversation_history: å¯é¸çš„å°è©±æ­·å²
        """
        try:
            # ç¢ºä¿é€²éšæ–¹æ³•å·²åˆå§‹åŒ–
            if not self.llm_adapter:
                print("âš ï¸ LLM é©é…å™¨æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°åŸºç¤æ–¹æ³•")
                return self._query_basic(query, top_k, use_llm, conversation_history)
            
            # æ ¹æ“šæ–¹æ³•é¸æ“‡å°æ‡‰çš„ RAG å¯¦ä¾‹
            rag_instance = None
            method_name = method.value
            
            if method == RAGMethod.SUBQUERY:
                rag_instance = self.subquery_rag
            elif method == RAGMethod.HYDE:
                rag_instance = self.hyde_rag
            elif method == RAGMethod.STEP_BACK:
                rag_instance = self.step_back_rag
            elif method == RAGMethod.HYBRID_SUBQUERY_HYDE:
                rag_instance = self.hybrid_subquery_hyde_rag
            elif method == RAGMethod.TRIPLE_HYBRID:
                rag_instance = self.triple_hybrid_rag
            
            # å¦‚æœæ–¹æ³•æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°åŸºç¤æ–¹æ³•
            if rag_instance is None:
                print(f"âš ï¸ {method_name} æ–¹æ³•æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°åŸºç¤æ–¹æ³•")
                return self._query_basic(query, top_k, use_llm, conversation_history)
            
            # ä½¿ç”¨é€²éšæ–¹æ³•ç”Ÿæˆå›ç­”
            if use_llm:
                try:
                    result = rag_instance.generate_answer(
                        question=query,
                        formatter=self.formatter,
                        top_k=top_k,
                        document_type=self._detect_document_type([])  # æš«æ™‚ä½¿ç”¨ç©ºåˆ—è¡¨ï¼Œå¯¦éš›æœƒåœ¨æ–¹æ³•å…§éƒ¨æª¢ç´¢
                    )
                    
                    # çµ±ä¸€è¿”å›æ ¼å¼
                    return {
                        "success": True,
                        "query": query,
                        "answer": result.get("answer", ""),
                        "results": result.get("results", []),
                        "formatted_context": result.get("formatted_context", ""),
                        "stats": {
                            "total_time": result.get("elapsed_time", 0),
                            "recall_time": 0,
                            "rerank_time": 0
                        },
                        "document_type": result.get("document_type", "general"),
                        "rag_method": method_name,
                        "method_reason": method_reason,
                        "advanced_details": result  # ä¿ç•™é€²éšæ–¹æ³•çš„é¡å¤–ä¿¡æ¯
                    }
                except Exception as e:
                    print(f"âš ï¸ é€²éš RAG æ–¹æ³•åŸ·è¡Œå¤±æ•—: {e}")
                    import traceback
                    traceback.print_exc()
                    # å›é€€åˆ°åŸºç¤æ–¹æ³•
                    print("   å›é€€åˆ°åŸºç¤ RAG æ–¹æ³•...")
                    return self._query_basic(query, top_k, use_llm, conversation_history)
            else:
                # ä¸ä½¿ç”¨ LLMï¼Œåªæª¢ç´¢
                # ä¸åŒæ–¹æ³•æœ‰ä¸åŒçš„ query æ¥å£ï¼Œé€™è£¡çµ±ä¸€è™•ç†
                if hasattr(rag_instance, 'query'):
                    result = rag_instance.query(query, top_k=top_k)
                    return {
                        "success": True,
                        "query": query,
                        "answer": None,
                        "results": result.get("results", []),
                        "formatted_context": "",
                        "stats": result.get("stats", {}),
                        "document_type": "general",
                        "rag_method": method_name,
                        "method_reason": method_reason
                    }
                else:
                    # å¦‚æœæ–¹æ³•ä¸æ”¯æŒ queryï¼Œå›é€€åˆ°åŸºç¤æ–¹æ³•
                    return self._query_basic(query, top_k, use_llm, conversation_history)
                    
        except Exception as e:
            print(f"âš ï¸ é€²éš RAG æŸ¥è©¢å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            # å›é€€åˆ°åŸºç¤æ–¹æ³•
            return self._query_basic(query, top_k, use_llm, conversation_history)
    
    def _load_conversation_history(self, query: str) -> List[Tuple[str, str]]:
        """
        å¾æ•¸æ“šåº«åŠ è¼‰å°è©±æ­·å²ï¼ˆå¦‚æœå•Ÿç”¨æŒä¹…åŒ–ï¼‰
        
        Args:
            query: ç•¶å‰æŸ¥è©¢å•é¡Œï¼ˆç”¨æ–¼æ™ºèƒ½æª¢ç´¢ï¼‰
            
        Returns:
            å°è©±æ­·å²åˆ—è¡¨
        """
        if not self.enable_conversation_persistence or not self.conversation_db or not self.current_session_id:
            return []
        
        try:
            if self.use_smart_history_retrieval:
                # ä½¿ç”¨æ™ºèƒ½æª¢ç´¢ï¼šæ ¹æ“šæŸ¥è©¢æª¢ç´¢ç›¸é—œæ­·å²
                history = self.conversation_db.search_relevant_history(
                    session_id=self.current_session_id,
                    query=query,
                    limit=self.max_history_rounds,
                    use_semantic_search=True
                )
            else:
                # ä½¿ç”¨ç°¡å–®æª¢ç´¢ï¼šç²å–æœ€è¿‘çš„æ­·å²
                history = self.conversation_db.get_recent_history(
                    session_id=self.current_session_id,
                    limit=self.max_history_rounds
                )
            return history
        except Exception as e:
            print(f"âš ï¸ åŠ è¼‰å°è©±æ­·å²å¤±æ•—: {e}")
            return []
    
    def _save_conversation(self, user_query: str, ai_answer: str):
        """
        ä¿å­˜å°è©±åˆ°æ•¸æ“šåº«ï¼ˆå¦‚æœå•Ÿç”¨æŒä¹…åŒ–ï¼‰
        
        Args:
            user_query: ç”¨æˆ¶å•é¡Œ
            ai_answer: AI å›ç­”
        """
        if not self.enable_conversation_persistence or not self.conversation_db or not self.current_session_id:
            return
        
        try:
            # ä¿å­˜ç”¨æˆ¶å•é¡Œ
            self.conversation_db.save_message(
                session_id=self.current_session_id,
                role="user",
                content=user_query
            )
            # ä¿å­˜ AI å›ç­”
            self.conversation_db.save_message(
                session_id=self.current_session_id,
                role="assistant",
                content=ai_answer
            )
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å°è©±æ­·å²å¤±æ•—: {e}")
    
    def _build_prompt_with_history(
        self,
        query: str,
        formatted_context: str,
        document_type: str,
        conversation_history: Optional[List[Tuple[str, str]]] = None
    ) -> str:
        """
        æ§‹å»ºåŒ…å«å°è©±æ­·å²çš„ prompt
        
        Args:
            query: ç•¶å‰æŸ¥è©¢å•é¡Œ
            formatted_context: æ ¼å¼åŒ–å¾Œçš„ä¸Šä¸‹æ–‡
            document_type: æ–‡æª”é¡å‹
            conversation_history: å¯é¸çš„å°è©±æ­·å²ï¼Œæ ¼å¼ç‚º List[Tuple[str, str]]ï¼Œæ¯å€‹å…ƒçµ„ç‚º (ç”¨æˆ¶å•é¡Œ, AIå›ç­”)
            
        Returns:
            å®Œæ•´çš„ prompt å­—ç¬¦ä¸²
        """
        # ç²å–åŸºç¤ promptï¼ˆä¸åŒ…å«æ­·å²ï¼‰
        base_prompt = self.formatter.create_prompt(
            query,
            formatted_context,
            document_type=document_type
        )
        
        # å¦‚æœæ²’æœ‰å°è©±æ­·å²ï¼Œç›´æ¥è¿”å›åŸºç¤ prompt
        if not conversation_history or len(conversation_history) == 0:
            return base_prompt
        
        # é™åˆ¶æ­·å²é•·åº¦ï¼Œåªä¿ç•™æœ€è¿‘ 10 è¼ªå°è©±ï¼ˆé¿å…ä¸Šä¸‹æ–‡éé•·ï¼‰
        recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history
        
        # æ§‹å»ºæ­·å²å°è©±æ–‡æœ¬
        history_text = ""
        for i, (user_q, ai_a) in enumerate(recent_history, 1):
            if ai_a:  # å¦‚æœæœ‰ AI å›ç­”
                history_text += f"**å°è©± {i}:**\n"
                history_text += f"ç”¨æˆ¶: {user_q}\n"
                history_text += f"AI: {ai_a}\n\n"
            else:  # å¦‚æœåªæœ‰ç”¨æˆ¶å•é¡Œï¼ˆä¸å®Œæ•´å°è©±ï¼‰
                history_text += f"**å°è©± {i}:**\n"
                history_text += f"ç”¨æˆ¶: {user_q}\n\n"
        
        # æª¢æ¸¬èªè¨€
        detected_language = self.formatter.detect_language(query) if self.formatter.auto_detect_language else "zh"
        
        # æ ¹æ“šèªè¨€æ§‹å»ºåŒ…å«æ­·å²çš„ prompt
        if detected_language == "zh":
            history_section = f"""## ä¹‹å‰çš„å°è©±æ­·å²ï¼š

{history_text}---

"""
        else:
            history_section = f"""## Previous Conversation History:

{history_text}---

"""
        
        # å°‡æ­·å²æ’å…¥åˆ°ç³»çµ±æç¤ºè©å’Œæ–‡æª”ç‰‡æ®µä¹‹é–“
        # æ‰¾åˆ° "## ç›¸é—œæ–‡æª”ç‰‡æ®µï¼š" æˆ– "## Relevant Document Excerpts:" çš„ä½ç½®
        if detected_language == "zh":
            marker = "## ç›¸é—œæ–‡æª”ç‰‡æ®µï¼š"
        else:
            marker = "## Relevant Document Excerpts:"
        
        # åœ¨ marker ä¹‹å‰æ’å…¥æ­·å²
        if marker in base_prompt:
            parts = base_prompt.split(marker, 1)
            prompt_with_history = parts[0] + history_section + marker + parts[1]
        else:
            # å¦‚æœæ‰¾ä¸åˆ° markerï¼Œåœ¨é–‹é ­æ·»åŠ æ­·å²
            prompt_with_history = history_section + base_prompt
        
        return prompt_with_history
    
    def _detect_document_type(self, results: List[Dict]) -> str:
        """æª¢æ¸¬æ–‡æª”é¡å‹"""
        if not results:
            return "general"
        
        # æª¢æŸ¥ metadata
        for result in results:
            metadata = result.get("metadata", {})
            file_path = str(metadata.get("file_path", "")).lower()
            
            if any(keyword in file_path for keyword in ["cv", "resume", "å±¥æ­·", "ç°¡æ­·"]):
                return "cv"
            elif any(keyword in file_path for keyword in ["arxiv", "paper", "è«–æ–‡"]):
                return "paper"
        
        return "general"
    
    def clear(self):
        """æ¸…é™¤ç•¶å‰è¼‰å…¥çš„æ–‡ä»¶å’Œ RAG ç³»çµ±"""
        self.current_files = []
        self.is_initialized = False
        self.processor = None
        self.bm25_retriever = None
        self.vector_retriever = None
        self.hybrid_search = None
        self.reranker = None
        self.rag_pipeline = None
        self.formatter = None
        self.shared_embeddings = None


# å…¨å±€å¯¦ä¾‹ï¼ˆç”¨æ–¼ UIï¼‰
_private_rag_instance: Optional[PrivateFileRAG] = None


def get_private_rag_instance() -> PrivateFileRAG:
    """ç²å–å…¨å±€ç§æœ‰æ–‡ä»¶ RAG å¯¦ä¾‹"""
    global _private_rag_instance
    if _private_rag_instance is None:
        _private_rag_instance = PrivateFileRAG()
    return _private_rag_instance


def reset_private_rag_instance():
    """é‡ç½®å…¨å±€å¯¦ä¾‹"""
    global _private_rag_instance
    _private_rag_instance = None

"""
ç§æœ‰æ–‡ä»¶ RAG ç³»çµ±
é›†æˆ Learn_RAG çš„åŠŸèƒ½ï¼Œæ”¯æŒä¸Šå‚³ç§æœ‰æ–‡ä»¶ï¼ˆPDFã€DOCXã€TXTï¼‰ä¸¦ä½¿ç”¨ RAG å›ç­”å•é¡Œ

LLM ä½¿ç”¨ç­–ç•¥ï¼š
- å„ªå…ˆä½¿ç”¨ Groq APIï¼ˆå¦‚æœé…ç½®äº† API é‡‘é‘°ï¼‰
- å…¶æ¬¡ä½¿ç”¨ Ollamaï¼ˆå¦‚æœæœå‹™æ­£åœ¨é‹è¡Œï¼‰
- æœ€å¾Œä½¿ç”¨ MLX æœ¬åœ°æ¨¡å‹ï¼ˆä½œç‚ºå‚™é¸æ–¹æ¡ˆï¼‰
"""
import os
import sys
from pathlib import Path
from typing import Optional, Dict, List, Tuple
import tempfile
import shutil

# å°å…¥ Deep_Agentic_AI_Tool çš„ LLM å·¥å…·
# é€™æ¨£å¯ä»¥ä½¿ç”¨çµ±ä¸€çš„ LLM å„ªå…ˆé †åºç­–ç•¥ï¼ˆGroq -> Ollama -> MLXï¼‰
from ..utils.llm_utils import get_llm
from langchain_core.messages import HumanMessage

# æ·»åŠ  Learn_RAG åˆ° Python è·¯å¾‘
# è¨ˆç®— Learn_RAG çš„è·¯å¾‘ï¼ˆèˆ‡ Deep_Agentic_AI_Tool åœ¨åŒä¸€ç›®éŒ„ä¸‹ï¼‰
current_file = Path(__file__).resolve()
# å¾ deep_agent_rag/rag/private_file_rag.py å‘ä¸Šæ‰¾åˆ° Deep_Agentic_AI_Tool æ ¹ç›®éŒ„
deep_agent_root = current_file.parent.parent.parent.parent
learn_rag_path = deep_agent_root.parent / "Learn_RAG"

# å¦‚æœ Learn_RAG ä¸åœ¨é æœŸä½ç½®ï¼Œå˜—è©¦å…¶ä»–å¯èƒ½çš„ä½ç½®
if not learn_rag_path.exists():
    # å˜—è©¦ç•¶å‰å·¥ä½œç›®éŒ„çš„çˆ¶ç›®éŒ„
    cwd = Path.cwd()
    learn_rag_path = cwd.parent / "Learn_RAG"
    
    if not learn_rag_path.exists():
        # å˜—è©¦ç›´æ¥ä½¿ç”¨çµ•å°è·¯å¾‘
        learn_rag_path = Path("/Users/matthuang/Desktop/Learn_RAG")

# å°‡ Learn_RAG ç›®éŒ„æ·»åŠ åˆ° Python è·¯å¾‘ï¼ˆé€™æ¨£å¯ä»¥å°å…¥ src æ¨¡çµ„ï¼‰
# æ³¨æ„ï¼šéœ€è¦å°‡ Learn_RAG ç›®éŒ„æœ¬èº«æ·»åŠ åˆ°è·¯å¾‘ï¼Œå› ç‚º src æ¨¡çµ„åœ¨ Learn_RAG/src/ ä¸‹
if learn_rag_path.exists() and learn_rag_path.is_dir():
    if str(learn_rag_path) not in sys.path:
        sys.path.insert(0, str(learn_rag_path))
    print(f"âœ“ æ‰¾åˆ° Learn_RAG é …ç›®: {learn_rag_path}")
    print(f"  Python è·¯å¾‘å·²æ·»åŠ : {learn_rag_path}")
else:
    print(f"âš ï¸ ç„¡æ³•æ‰¾åˆ° Learn_RAG é …ç›®")
    print(f"   å˜—è©¦çš„è·¯å¾‘: {learn_rag_path}")
    print(f"   è«‹ç¢ºä¿ Learn_RAG é …ç›®åœ¨: {deep_agent_root.parent / 'Learn_RAG'}")

# å˜—è©¦å°å…¥ Learn_RAG æ¨¡çµ„
# æ³¨æ„ï¼šdocument_processor.py åœ¨é ‚å±¤å°å…¥äº† arxivï¼Œæ‰€ä»¥éœ€è¦å…ˆå®‰è£ä¾è³´
try:
    # å…ˆæª¢æŸ¥å¿…è¦çš„ä¾è³´æ˜¯å¦å·²å®‰è£
    import importlib
    
    required_deps = {
        "arxiv": "arxiv",
        "langchain_community": "langchain-community",
        "langchain_text_splitters": "langchain-text-splitters",
        "chromadb": "chromadb",
        "sentence_transformers": "sentence-transformers",
        "rank_bm25": "rank-bm25",
        "pypdf": "pypdf",
    }
    
    missing_deps = []
    for module_name, package_name in required_deps.items():
        try:
            importlib.import_module(module_name)
        except ImportError:
            missing_deps.append(package_name)
    
    if missing_deps:
        print(f"âš ï¸ ç¼ºå°‘ä»¥ä¸‹ä¾è³´åŒ…: {', '.join(missing_deps)}")
        print(f"\nğŸ’¡ è«‹å®‰è£ Learn_RAG é …ç›®çš„ä¾è³´:")
        print(f"   æ–¹æ³• 1: ä½¿ç”¨ pip")
        print(f"   pip install {' '.join(missing_deps)}")
        print(f"\n   æ–¹æ³• 2: ä½¿ç”¨ uv (æ¨è–¦ï¼Œå¦‚æœ Learn_RAG ä½¿ç”¨ uv)")
        print(f"   cd {learn_rag_path}")
        print(f"   uv sync")
        print(f"\n   æ–¹æ³• 3: å®‰è£æ‰€æœ‰ Learn_RAG ä¾è³´")
        print(f"   pip install arxiv langchain-community langchain-text-splitters chromadb sentence-transformers rank-bm25 pypdf docx2txt langchain-experimental")
        LEARN_RAG_AVAILABLE = False
    else:
        # æ‰€æœ‰ä¾è³´éƒ½å·²å®‰è£ï¼Œå˜—è©¦å°å…¥æ¨¡çµ„
        from src.document_processor import DocumentProcessor
        from src.retrievers.bm25_retriever import BM25Retriever
        from src.retrievers.vector_retriever import VectorRetriever
        from src.retrievers.hybrid_search import HybridSearch
        from src.retrievers.reranker import Reranker, RAGPipeline
        from src.prompt_formatter import PromptFormatter
        # ä¸å†éœ€è¦å°å…¥ OllamaLLMï¼Œå› ç‚ºæˆ‘å€‘ä½¿ç”¨ Deep_Agentic_AI_Tool çš„çµ±ä¸€ LLM ç³»çµ±ï¼ˆget_llm()ï¼‰
        # from src.llm_integration import OllamaLLM
        LEARN_RAG_AVAILABLE = True
        print("âœ“ æˆåŠŸå°å…¥ Learn_RAG æ¨¡çµ„")
        
except ImportError as e:
    error_msg = str(e)
    print(f"âš ï¸ ç„¡æ³•å°å…¥ Learn_RAG æ¨¡çµ„: {error_msg}")
    print(f"\nğŸ’¡ è«‹å®‰è£ Learn_RAG é …ç›®çš„ä¾è³´:")
    print(f"   pip install arxiv langchain-community langchain-text-splitters chromadb sentence-transformers rank-bm25 pypdf docx2txt langchain-experimental")
    print(f"\n   æˆ–è€…:")
    print(f"   cd {learn_rag_path}")
    print(f"   uv sync")
    LEARN_RAG_AVAILABLE = False
except Exception as e:
    error_msg = str(e)
    print(f"âš ï¸ å°å…¥ Learn_RAG æ¨¡çµ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {error_msg}")
    print(f"   ç•¶å‰ Python è·¯å¾‘: {sys.path[:3]}")
    print(f"   Learn_RAG è·¯å¾‘: {learn_rag_path}")
    LEARN_RAG_AVAILABLE = False


class PrivateFileRAG:
    """
    ç§æœ‰æ–‡ä»¶ RAG ç³»çµ±ç®¡ç†å™¨
    
    é€™å€‹é¡è² è²¬ç®¡ç†ç§æœ‰æ–‡ä»¶çš„ RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰ç³»çµ±ï¼ŒåŒ…æ‹¬ï¼š
    - æ–‡ä»¶è™•ç†å’Œåˆ†å¡Šï¼ˆæ”¯æŒå­—ç¬¦åˆ†å¡Šå’Œèªç¾©åˆ†å¡Šï¼‰
    - æª¢ç´¢ç³»çµ±åˆå§‹åŒ–ï¼ˆBM25 + å‘é‡æª¢ç´¢ + æ··åˆæœå°‹ï¼‰
    - RAG æŸ¥è©¢å’Œå›ç­”ç”Ÿæˆ
    
    LLM ä½¿ç”¨ç­–ç•¥ï¼š
    - ä½¿ç”¨ Deep_Agentic_AI_Tool çš„çµ±ä¸€ LLM ç³»çµ±ï¼ˆget_llm()ï¼‰
    - è‡ªå‹•éµå¾ªå„ªå…ˆé †åºï¼šGroq API > Ollama > MLX æœ¬åœ°æ¨¡å‹
    - ç„¡éœ€æ‰‹å‹•æŒ‡å®š LLM é¡å‹ï¼Œç³»çµ±æœƒæ ¹æ“šé…ç½®å’Œå¯ç”¨æ€§è‡ªå‹•é¸æ“‡æœ€åˆé©çš„ LLM
    - å¦‚æœ Groq API é¡åº¦ç”¨å®Œæˆ–æœå‹™ä¸å¯ç”¨ï¼Œæœƒè‡ªå‹•åˆ‡æ›åˆ°å‚™é¸ LLM
    """
    
    def __init__(
        self,
        use_semantic_chunking: bool = False,
        chunk_size: int = 500,  # é è¨­æ”¹ç‚º 500ï¼Œæä¾›æ›´ç´°çš„ç²’åº¦
        chunk_overlap: int = 100,  # é è¨­æ”¹ç‚º 100ï¼Œä¿æŒ 20% çš„é‡ç–Šæ¯”ä¾‹
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        persist_directory: str = "./chroma_db_private",
        # èªç¾©åˆ†å¡Šåƒæ•¸
        semantic_threshold: float = 1.0,  # é è¨­æ”¹ç‚º 1.0ï¼Œæä¾›æ›´ç´°çš„ç²’åº¦ï¼ˆåŸç‚º 1.5ï¼‰
        semantic_min_chunk_size: int = 100  # èªç¾©åˆ†å¡Šçš„æœ€å° chunk å¤§å°ï¼ˆå­—ç¬¦æ•¸ï¼‰
    ):
        """
        åˆå§‹åŒ–ç§æœ‰æ–‡ä»¶ RAG ç³»çµ±
        
        Args:
            use_semantic_chunking: æ˜¯å¦ä½¿ç”¨èªç¾©åˆ†å¡Š
                                  True: ä½¿ç”¨èªç¾©åˆ†å¡Šï¼ˆä¿æŒèªç¾©å®Œæ•´æ€§ï¼Œä¸æœƒåœ¨å¥å­ä¸­é–“åˆ‡æ–·ï¼Œä½†è™•ç†æ™‚é–“è¼ƒé•·ï¼‰
                                  False: ä½¿ç”¨å­—ç¬¦åˆ†å¡Šï¼ˆå¿«é€Ÿï¼Œä½†å¯èƒ½åœ¨å¥å­ä¸­é–“åˆ‡æ–·ï¼Œé è¨­ï¼‰
            chunk_size: å­—ç¬¦åˆ†å¡Šå¤§å°ï¼ˆåƒ…ç”¨æ–¼å­—ç¬¦åˆ†å¡Šæ¨¡å¼ï¼‰
                       æ¯å€‹ chunk çš„å­—ç¬¦æ•¸ï¼Œé è¨­ 500ï¼ˆè¼ƒç´°çš„ç²’åº¦ï¼‰
                       å»ºè­°å€¼ï¼š
                       - 300-500ï¼šç´°ç²’åº¦ï¼Œé©åˆç²¾ç¢ºæª¢ç´¢ï¼Œä½†å¯èƒ½éºæ¼ä¸Šä¸‹æ–‡
                       - 500-800ï¼šä¸­ç­‰ç²’åº¦ï¼Œå¹³è¡¡ç²¾ç¢ºåº¦å’Œä¸Šä¸‹æ–‡ï¼ˆæ¨è–¦ï¼‰
                       - 800-1200ï¼šç²—ç²’åº¦ï¼ŒåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†å¯èƒ½åŒ…å«ä¸ç›¸é—œå…§å®¹
                       è¼ƒå¤§çš„å€¼å¯ä»¥åŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†å¯èƒ½åŒ…å«ä¸ç›¸é—œå…§å®¹
                       è¼ƒå°çš„å€¼æ›´ç²¾ç¢ºï¼Œä½†å¯èƒ½éºæ¼é‡è¦ä¿¡æ¯
            chunk_overlap: å­—ç¬¦åˆ†å¡Šé‡ç–Šå¤§å°ï¼ˆåƒ…ç”¨æ–¼å­—ç¬¦åˆ†å¡Šæ¨¡å¼ï¼‰
                          ç›¸é„° chunks ä¹‹é–“çš„é‡ç–Šå­—ç¬¦æ•¸ï¼Œé è¨­ 100ï¼ˆç´„ç‚º chunk_size çš„ 20%ï¼‰
                          å»ºè­°å€¼ï¼šchunk_size çš„ 15-25%
                          é‡ç–Šå¯ä»¥å¹«åŠ©ä¿æŒä¸Šä¸‹æ–‡é€£è²«æ€§ï¼Œé¿å…åœ¨é‡è¦ä¿¡æ¯é‚Šç•Œè™•åˆ‡æ–·
                          è¼ƒå¤§çš„é‡ç–Šå¯ä»¥æ›´å¥½åœ°ä¿æŒä¸Šä¸‹æ–‡ï¼Œä½†æœƒå¢åŠ  chunks æ•¸é‡
            embedding_model: Embedding æ¨¡å‹åç¨±
                            ç”¨æ–¼å‘é‡æª¢ç´¢å’Œèªç¾©åˆ†å¡Šçš„ embedding æ¨¡å‹
                            é è¨­ä½¿ç”¨ "sentence-transformers/all-MiniLM-L6-v2"ï¼ˆè¼•é‡ç´šã€å¿«é€Ÿã€æ•ˆæœå¥½ï¼‰
                            å¦‚æœéœ€è¦æ›´å¥½çš„æ•ˆæœï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼Œä½†æœƒå¢åŠ è¨ˆç®—æ™‚é–“å’Œå…§å­˜ä½¿ç”¨
            persist_directory: å‘é‡è³‡æ–™åº«æŒä¹…åŒ–ç›®éŒ„
                               ChromaDB æœƒå°‡å‘é‡è³‡æ–™åº«ä¿å­˜åœ¨æ­¤ç›®éŒ„ï¼Œä¸‹æ¬¡ä½¿ç”¨æ™‚å¯ä»¥ç›´æ¥è¼‰å…¥
                               é è¨­ç‚º "./chroma_db_private"
                               å¦‚æœç›®éŒ„å·²å­˜åœ¨ï¼Œæœƒè‡ªå‹•è¼‰å…¥å·²æœ‰çš„å‘é‡è³‡æ–™åº«
            semantic_threshold: èªç¾©åˆ†å¡Šçš„æ•æ„Ÿåº¦é–¾å€¼ï¼ˆæ¨™æº–å·®å€æ•¸ï¼‰
                               æ•¸å€¼è¶Šå°ï¼Œåˆ†å¡Šè¶Šå¤šã€è¶Šç´°ï¼ˆchunks è¶Šå°ï¼‰
                               æ•¸å€¼è¶Šå¤§ï¼Œåˆ†å¡Šè¶Šå°‘ã€è¶Šç²—ï¼ˆchunks è¶Šå¤§ï¼‰
                               å»ºè­°ç¯„åœï¼š
                               - 0.8-1.2ï¼šç´°ç²’åº¦ï¼Œé©åˆéœ€è¦ç²¾ç¢ºæª¢ç´¢çš„å ´æ™¯ï¼ˆæ¨è–¦ï¼‰
                               - 1.2-1.8ï¼šä¸­ç­‰ç²’åº¦ï¼Œå¹³è¡¡ç²¾ç¢ºåº¦å’Œä¸Šä¸‹æ–‡
                               - 1.8-2.5ï¼šç²—ç²’åº¦ï¼ŒåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†å¯èƒ½åŒ…å«ä¸ç›¸é—œå…§å®¹
                               é è¨­å€¼ï¼š1.0ï¼ˆå·²å„ªåŒ–ç‚ºæ›´ç´°çš„ç²’åº¦ï¼ŒåŸç‚º 1.5ï¼‰
            semantic_min_chunk_size: èªç¾©åˆ†å¡Šçš„æœ€å° chunk å¤§å°ï¼ˆå­—ç¬¦æ•¸ï¼‰
                                    å°æ–¼æ­¤å¤§å°çš„ chunks æœƒè¢«åˆä½µåˆ°ç›¸é„°çš„ chunks
                                    é è¨­å€¼ï¼š100 å­—ç¬¦
                                    å»ºè­°å€¼ï¼š50-200ï¼Œæ ¹æ“šæ–‡æª”é¡å‹èª¿æ•´
                                    è¼ƒå°çš„å€¼å¯ä»¥ä¿ç•™æ›´å¤šç´°ç¯€ï¼Œä½†å¯èƒ½ç”¢ç”Ÿéå¤šçš„å° chunks
        """
        if not LEARN_RAG_AVAILABLE:
            raise ImportError("Learn_RAG æ¨¡çµ„ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥å®‰è£")
        
        self.use_semantic_chunking = use_semantic_chunking
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.embedding_model = embedding_model
        self.persist_directory = persist_directory
        # èªç¾©åˆ†å¡Šåƒæ•¸
        self.semantic_threshold = semantic_threshold
        self.semantic_min_chunk_size = semantic_min_chunk_size
        
        # çµ„ä»¶
        self.processor = None
        self.bm25_retriever = None
        self.vector_retriever = None
        self.hybrid_search = None
        self.reranker = None
        self.rag_pipeline = None
        self.formatter = None
        self.shared_embeddings = None
        
        # ç•¶å‰è¼‰å…¥çš„æ–‡ä»¶
        self.current_files = []
        self.is_initialized = False
    
    def _init_embeddings(self):
        """
        åˆå§‹åŒ–å…±ç”¨çš„ Embedding æ¨¡å‹ï¼ˆç”¨æ–¼èªç¾©åˆ†å¡Šï¼‰
        
        é€™å€‹æ–¹æ³•æœƒå‰µå»ºä¸€å€‹ HuggingFace Embeddings æ¨¡å‹ï¼Œç”¨æ–¼ï¼š
        - èªç¾©åˆ†å¡Šï¼šè¨ˆç®—æ–‡æœ¬çš„èªç¾©ç›¸ä¼¼åº¦ï¼Œåœ¨èªç¾©é‚Šç•Œè™•åˆ‡åˆ†
        - å‘é‡æª¢ç´¢ï¼šå°‡æ–‡æª”è½‰æ›ç‚ºå‘é‡ï¼Œç”¨æ–¼èªç¾©æœå°‹
        
        å¦‚æœåˆå§‹åŒ–å¤±æ•—ï¼Œæœƒè‡ªå‹•å›é€€åˆ°å­—ç¬¦åˆ†å¡Šæ¨¡å¼ã€‚
        
        Returns:
            HuggingFaceEmbeddings å¯¦ä¾‹ï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› None
        """
        # å¦‚æœä¸éœ€è¦èªç¾©åˆ†å¡Šï¼Œç›´æ¥è¿”å› None
        if not self.use_semantic_chunking:
            return None
        
        try:
            from langchain_community.embeddings import HuggingFaceEmbeddings
            from src.retrievers.vector_retriever import get_device
            
            # ç²å– Hugging Face æ¨¡å‹ç·©å­˜ç›®éŒ„ï¼ˆå¦‚æœè¨­ç½®äº†ç’°å¢ƒè®Šæ•¸ï¼‰
            # é€™å°æ–¼ä½¿ç”¨å¤–æ¥ç¡¬ç¢Ÿå­˜å„²æ¨¡å‹å¾ˆæœ‰ç”¨
            hf_cache_dir = os.getenv("HF_CACHE_DIR", None)
            
            # è‡ªå‹•æª¢æ¸¬å¯ç”¨çš„è¨­å‚™ï¼ˆMPS/CUDA/CPUï¼‰
            # MPS: macOS GPU, CUDA: NVIDIA GPU, CPU: å‚™é¸
            device = get_device()
            
            # æ§‹å»ºæ¨¡å‹åƒæ•¸
            model_kwargs = {'device': device}
            if hf_cache_dir:
                model_kwargs['cache_dir'] = hf_cache_dir
            
            # å‰µå»º HuggingFace Embeddings æ¨¡å‹
            # normalize_embeddings=True æœƒå°‡å‘é‡æ­£è¦åŒ–ï¼Œæœ‰åŠ©æ–¼æå‡æª¢ç´¢æ•ˆæœ
            self.shared_embeddings = HuggingFaceEmbeddings(
                model_name=self.embedding_model,
                model_kwargs=model_kwargs,
                encode_kwargs={'normalize_embeddings': True}
            )
            return self.shared_embeddings
        except Exception as e:
            # å¦‚æœåˆå§‹åŒ–å¤±æ•—ï¼Œè¨˜éŒ„éŒ¯èª¤ä¸¦å›é€€åˆ°å­—ç¬¦åˆ†å¡Šæ¨¡å¼
            print(f"âš ï¸ åˆå§‹åŒ– Embedding æ¨¡å‹å¤±æ•—: {e}")
            print("   å°‡å›é€€åˆ°å­—ç¬¦åˆ†å¡Šæ¨¡å¼")
            self.use_semantic_chunking = False
            return None
    
    def process_files(self, file_paths: List[str]) -> Tuple[List[Dict], str]:
        """
        è™•ç†ä¸Šå‚³çš„æ–‡ä»¶
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾‘åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²è·¯å¾‘æˆ– Gradio æ–‡ä»¶å°è±¡ï¼‰
            
        Returns:
            (documents, status_message) å…ƒçµ„
        """
        if not file_paths:
            return [], "âŒ æœªæä¾›æ–‡ä»¶è·¯å¾‘"
        
        try:
            # è™•ç†æ–‡ä»¶è·¯å¾‘ï¼ˆå¯èƒ½æ˜¯ Gradio æ–‡ä»¶å°è±¡ï¼‰
            actual_paths = []
            for file_path in file_paths:
                if hasattr(file_path, 'name'):
                    # Gradio æ–‡ä»¶å°è±¡
                    actual_path = file_path.name
                else:
                    # å­—ç¬¦ä¸²è·¯å¾‘
                    actual_path = file_path
                
                if os.path.exists(actual_path):
                    actual_paths.append(actual_path)
                else:
                    print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {actual_path}")
            
            if not actual_paths:
                return [], "âŒ æ²’æœ‰æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾‘"
            
            # åˆå§‹åŒ– Embeddingï¼ˆå¦‚æœéœ€è¦èªç¾©åˆ†å¡Šï¼‰
            if self.use_semantic_chunking:
                self._init_embeddings()
            
            # åˆå§‹åŒ–æ–‡æª”è™•ç†å™¨
            if self.use_semantic_chunking and self.shared_embeddings:
                # ä½¿ç”¨å¯èª¿æ•´çš„èªç¾©åˆ†å¡Šåƒæ•¸
                self.processor = DocumentProcessor(
                    embeddings=self.shared_embeddings,
                    use_semantic_chunking=True,
                    breakpoint_threshold_amount=self.semantic_threshold,  # ä½¿ç”¨å¯èª¿æ•´çš„é–¾å€¼
                    min_chunk_size=self.semantic_min_chunk_size  # ä½¿ç”¨å¯èª¿æ•´çš„æœ€å° chunk å¤§å°
                )
                print(f"ğŸ“ ä½¿ç”¨èªç¾©åˆ†å¡Šï¼šthreshold={self.semantic_threshold}, min_chunk_size={self.semantic_min_chunk_size}")
            else:
                self.processor = DocumentProcessor(
                    chunk_size=self.chunk_size,
                    chunk_overlap=self.chunk_overlap
                )
            
            # è™•ç†æ‰€æœ‰æ–‡ä»¶
            all_documents = []
            for file_path in actual_paths:
                print(f"è™•ç†æ–‡ä»¶: {file_path}")
                documents = self.processor.process_file(file_path)
                all_documents.extend(documents)
                print(f"  âœ“ å‰µå»ºäº† {len(documents)} å€‹ chunks")
            
            if not all_documents:
                return [], "âŒ è™•ç†å¾Œæ²’æœ‰æ–‡æª”å…§å®¹"
            
            self.current_files = actual_paths
            
            # åˆå§‹åŒ–æª¢ç´¢ç³»çµ±
            status_msg = self._init_retrievers(all_documents)
            
            return all_documents, status_msg
            
        except Exception as e:
            error_msg = f"âŒ è™•ç†æ–‡ä»¶å¤±æ•—: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return [], error_msg
    
    def _init_retrievers(self, documents: List[Dict]) -> str:
        """
        åˆå§‹åŒ–æª¢ç´¢å™¨
        
        Args:
            documents: æ–‡æª”åˆ—è¡¨
            
        Returns:
            ç‹€æ…‹è¨Šæ¯
        """
        try:
            # åˆå§‹åŒ– BM25 æª¢ç´¢å™¨
            print("  - åˆå§‹åŒ– BM25 æª¢ç´¢å™¨...")
            self.bm25_retriever = BM25Retriever(documents)
            
            # åˆå§‹åŒ–å‘é‡æª¢ç´¢å™¨
            print("  - åˆå§‹åŒ–å‘é‡æª¢ç´¢å™¨...")
            self.vector_retriever = VectorRetriever(
                documents,
                embedding_model=self.embedding_model,
                persist_directory=self.persist_directory,
                embeddings=self.shared_embeddings
            )
            
            # åˆå§‹åŒ–æ··åˆæœå°‹
            print("  - åˆå§‹åŒ–æ··åˆæœå°‹...")
            self.hybrid_search = HybridSearch(
                sparse_retriever=self.bm25_retriever,
                dense_retriever=self.vector_retriever,
                fusion_method="rrf",
                rrf_k=60
            )
            
            # å˜—è©¦åˆå§‹åŒ–é‡æ’åºå™¨ï¼ˆå¯é¸ï¼‰
            try:
                print("  - åˆå§‹åŒ–é‡æ’åºå™¨...")
                self.reranker = Reranker(
                    model_name="BAAI/bge-reranker-base",
                    batch_size=16
                )
                
                # åˆå§‹åŒ– RAG ç®¡ç·š
                print("  - åˆå§‹åŒ– RAG ç®¡ç·š...")
                self.rag_pipeline = RAGPipeline(
                    hybrid_search=self.hybrid_search,
                    reranker=self.reranker,
                    recall_k=20,
                    adaptive_recall=True
                )
                
                # åˆå§‹åŒ– Prompt æ ¼å¼åŒ–å™¨
                self.formatter = PromptFormatter(format_style="detailed")
                
                self.is_initialized = True
                return f"âœ… æˆåŠŸè™•ç† {len(self.current_files)} å€‹æ–‡ä»¶ï¼Œå‰µå»ºäº† {len(documents)} å€‹ chunksï¼ŒRAG ç³»çµ±å·²åˆå§‹åŒ–ï¼ˆåŒ…å«é‡æ’åºï¼‰"
                
            except Exception as e:
                print(f"  âš ï¸ é‡æ’åºå™¨åˆå§‹åŒ–å¤±æ•—: {e}")
                print("   å°‡ä½¿ç”¨æ··åˆæœå°‹ï¼ˆä¸é€²è¡Œé‡æ’åºï¼‰")
                self.formatter = PromptFormatter(format_style="detailed")
                self.is_initialized = True
                return f"âœ… æˆåŠŸè™•ç† {len(self.current_files)} å€‹æ–‡ä»¶ï¼Œå‰µå»ºäº† {len(documents)} å€‹ chunksï¼ŒRAG ç³»çµ±å·²åˆå§‹åŒ–ï¼ˆç„¡é‡æ’åºï¼‰"
                
        except Exception as e:
            error_msg = f"âŒ æª¢ç´¢ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return error_msg
    
    def query(
        self,
        query: str,
        top_k: int = 3,
        use_llm: bool = True,
        llm_model: Optional[str] = None
    ) -> Dict:
        """
        æŸ¥è©¢ RAG ç³»çµ±ä¸¦ç”Ÿæˆå›ç­”
        
        é€™å€‹æ–¹æ³•æœƒåŸ·è¡Œå®Œæ•´çš„ RAG æµç¨‹ï¼š
        1. ä½¿ç”¨æ··åˆæœå°‹ï¼ˆBM25 + å‘é‡æª¢ç´¢ï¼‰æª¢ç´¢ç›¸é—œæ–‡æª”ç‰‡æ®µ
        2. å¯é¸ï¼šä½¿ç”¨é‡æ’åºå™¨é€²ä¸€æ­¥å„ªåŒ–çµæœï¼ˆå¦‚æœå·²åˆå§‹åŒ–ï¼‰
        3. æ ¼å¼åŒ–æª¢ç´¢çµæœç‚º LLM å¯è®€çš„ä¸Šä¸‹æ–‡
        4. ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        
        LLM é¸æ“‡ç­–ç•¥ï¼š
        - è‡ªå‹•ä½¿ç”¨ get_llm() ç²å– LLM å¯¦ä¾‹
        - å„ªå…ˆé †åºï¼šGroq API > Ollama > MLX æœ¬åœ°æ¨¡å‹
        - ç„¡éœ€æ‰‹å‹•æŒ‡å®š LLM é¡å‹ï¼Œç³»çµ±æœƒæ ¹æ“šé…ç½®å’Œå¯ç”¨æ€§è‡ªå‹•é¸æ“‡æœ€åˆé©çš„
        - å¦‚æœç•¶å‰ LLM å¤±æ•—ï¼ˆå¦‚ API é¡åº¦ç”¨å®Œï¼‰ï¼Œæœƒè‡ªå‹•åˆ‡æ›åˆ°å‚™é¸ LLM
        
        Args:
            query: æŸ¥è©¢å•é¡Œï¼ˆç”¨æˆ¶æƒ³è¦å•çš„å•é¡Œï¼‰
                  ä¾‹å¦‚ï¼š"é€™ä»½æ–‡æª”çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ"
            top_k: è¿”å›çš„çµæœæ•¸é‡ï¼ˆæª¢ç´¢åˆ°çš„æ–‡æª”ç‰‡æ®µæ•¸é‡ï¼‰
                  å»ºè­°å€¼ï¼š3-5ï¼ˆå¤ªå°‘å¯èƒ½éºæ¼é‡è¦ä¿¡æ¯ï¼Œå¤ªå¤šå¯èƒ½åŒ…å«ä¸ç›¸é—œå…§å®¹ï¼‰
                  é è¨­å€¼ï¼š3
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
                    True: ä½¿ç”¨ LLM åŸºæ–¼æª¢ç´¢çµæœç”Ÿæˆå®Œæ•´ã€é€£è²«çš„å›ç­”ï¼ˆæ¨è–¦ï¼‰
                    False: åªè¿”å›æª¢ç´¢åˆ°çš„æ–‡æª”ç‰‡æ®µï¼Œä¸ç”Ÿæˆå›ç­”ï¼ˆé©åˆå¿«é€ŸæŸ¥çœ‹ç›¸é—œå…§å®¹ï¼‰
            llm_model: LLM æ¨¡å‹åç¨±ï¼ˆå·²å»¢æ£„ï¼Œä¸å†ä½¿ç”¨ï¼‰
                      ç¾åœ¨çµ±ä¸€ä½¿ç”¨ get_llm() è‡ªå‹•é¸æ“‡ LLMï¼Œéµå¾ª Groq -> Ollama -> MLX çš„å„ªå…ˆé †åº
                      ä¿ç•™æ­¤åƒæ•¸åƒ…ç‚ºå‘å¾Œå…¼å®¹ï¼Œå¯¦éš›ä¸æœƒè¢«ä½¿ç”¨
            
        Returns:
            åŒ…å«ä»¥ä¸‹å…§å®¹çš„å­—å…¸ï¼š
            - success: æ˜¯å¦æˆåŠŸï¼ˆboolï¼‰
            - query: åŸå§‹æŸ¥è©¢å•é¡Œï¼ˆstrï¼‰
            - answer: LLM ç”Ÿæˆçš„å›ç­”ï¼ˆstrï¼Œå¦‚æœ use_llm=True ä¸”æˆåŠŸï¼‰
            - results: æª¢ç´¢åˆ°çš„æ–‡æª”ç‰‡æ®µåˆ—è¡¨ï¼ˆList[Dict]ï¼‰ï¼Œæ¯å€‹ç‰‡æ®µåŒ…å«ï¼š
              * content: æ–‡æª”å…§å®¹ï¼ˆstrï¼‰
              * metadata: å…ƒæ•¸æ“šï¼ˆDictï¼‰ï¼ŒåŒ…å«æ¨™é¡Œã€æ–‡ä»¶è·¯å¾‘ç­‰
              * score: ç›¸é—œæ€§åˆ†æ•¸ï¼ˆfloatï¼‰
            - formatted_context: æ ¼å¼åŒ–å¾Œçš„ä¸Šä¸‹æ–‡ï¼ˆstrï¼‰ï¼Œç”¨æ–¼ LLM ç”Ÿæˆå›ç­”
            - stats: æª¢ç´¢çµ±è¨ˆä¿¡æ¯ï¼ˆDictï¼‰ï¼ŒåŒ…å«ï¼š
              * total_time: ç¸½è€—æ™‚ï¼ˆfloatï¼Œç§’ï¼‰
              * recall_time: å¬å›éšæ®µè€—æ™‚ï¼ˆfloatï¼Œç§’ï¼‰
              * rerank_time: é‡æ’åºéšæ®µè€—æ™‚ï¼ˆfloatï¼Œç§’ï¼Œå¦‚æœæœ‰é‡æ’åºï¼‰
            - document_type: æª¢æ¸¬åˆ°çš„æ–‡æª”é¡å‹ï¼ˆstrï¼‰
                            "paper": å­¸è¡“è«–æ–‡
                            "cv": ç°¡æ­·/å±¥æ­·
                            "general": é€šç”¨æ–‡æª”ï¼ˆé è¨­ï¼‰
            - error: éŒ¯èª¤è¨Šæ¯ï¼ˆstrï¼Œå¦‚æœå¤±æ•—ï¼‰
        """
        if not self.is_initialized:
            return {
                "success": False,
                "error": "RAG ç³»çµ±å°šæœªåˆå§‹åŒ–ï¼Œè«‹å…ˆä¸Šå‚³æ–‡ä»¶"
            }
        
        try:
            # æª¢ç´¢ç›¸é—œæ–‡æª”
            if self.rag_pipeline:
                # ä½¿ç”¨å®Œæ•´çš„ RAG ç®¡ç·šï¼ˆåŒ…å«é‡æ’åºï¼‰
                results, stats = self.rag_pipeline.query(
                    text=query,
                    top_k=top_k,
                    enable_rerank=True,
                    return_stats=True
                )
            else:
                # åƒ…ä½¿ç”¨æ··åˆæœå°‹
                results = self.hybrid_search.retrieve(query, top_k=top_k)
                stats = {"total_time": 0, "recall_time": 0, "rerank_time": 0}
            
            if not results:
                return {
                    "success": False,
                    "error": "æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”ç‰‡æ®µ",
                    "results": []
                }
            
            # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            formatted_context = self.formatter.format_context(
                results,
                format_style="detailed"
            )
            
            # æª¢æ¸¬æ–‡æª”é¡å‹
            document_type = self._detect_document_type(results)
            
            # ç”Ÿæˆå›ç­”ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            answer = None
            if use_llm:
                try:
                    # ä½¿ç”¨ Deep_Agentic_AI_Tool çš„çµ±ä¸€ LLM ç³»çµ±
                    # å„ªå…ˆé †åºï¼šGroq API > Ollama > MLX æœ¬åœ°æ¨¡å‹
                    # é€™æ¨£å¯ä»¥è‡ªå‹•è™•ç† API é¡åº¦ç”¨å®Œã€æœå‹™ä¸å¯ç”¨ç­‰æƒ…æ³
                    llm = get_llm()
                    
                    # ä½¿ç”¨ PromptFormatter å‰µå»ºæ ¼å¼åŒ–çš„ prompt
                    # é€™å€‹ prompt åŒ…å«äº†æŸ¥è©¢å•é¡Œå’Œæª¢ç´¢åˆ°çš„ç›¸é—œæ–‡æª”ç‰‡æ®µ
                    prompt = self.formatter.create_prompt(
                        query,
                        formatted_context,
                        document_type=document_type
                    )
                    
                    # å°‡ prompt è½‰æ›ç‚º LangChain çš„æ¶ˆæ¯æ ¼å¼
                    # LangChain çš„ ChatModel ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨è€Œä¸æ˜¯ç´”æ–‡å­— prompt
                    messages = [HumanMessage(content=prompt)]
                    
                    # ä½¿ç”¨ LangChain çš„ invoke æ–¹æ³•ç”Ÿæˆå›ç­”
                    # invoke æ–¹æ³•æœƒæ ¹æ“š LLM é¡å‹ï¼ˆGroq/Ollama/MLXï¼‰è‡ªå‹•é¸æ“‡åˆé©çš„èª¿ç”¨æ–¹å¼
                    response = llm.invoke(messages)
                    
                    # å¾ LangChain çš„ AIMessage ä¸­æå–å›ç­”å…§å®¹
                    answer = response.content
                except Exception as e:
                    # LLM ç”Ÿæˆå›ç­”å¤±æ•—æ™‚çš„éŒ¯èª¤è™•ç†
                    # å¯èƒ½çš„åŸå› ï¼šAPI é¡åº¦ç”¨å®Œã€æœå‹™ä¸å¯ç”¨ã€ç¶²çµ¡å•é¡Œç­‰
                    print(f"âš ï¸ LLM ç”Ÿæˆå›ç­”å¤±æ•—: {e}")
                    print("   ç³»çµ±æœƒè‡ªå‹•å˜—è©¦åˆ‡æ›åˆ°å‚™é¸ LLMï¼ˆå¦‚æœå¯ç”¨ï¼‰")
                    import traceback
                    traceback.print_exc()
                    answer = None
            
            return {
                "success": True,
                "query": query,
                "answer": answer,
                "results": results,
                "formatted_context": formatted_context,
                "stats": stats,
                "document_type": document_type
            }
            
        except Exception as e:
            error_msg = f"âŒ æŸ¥è©¢å¤±æ•—: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": error_msg
            }
    
    def _detect_document_type(self, results: List[Dict]) -> str:
        """æª¢æ¸¬æ–‡æª”é¡å‹"""
        if not results:
            return "general"
        
        # æª¢æŸ¥ metadata
        for result in results:
            metadata = result.get("metadata", {})
            file_path = str(metadata.get("file_path", "")).lower()
            
            if any(keyword in file_path for keyword in ["cv", "resume", "å±¥æ­·", "ç°¡æ­·"]):
                return "cv"
            elif any(keyword in file_path for keyword in ["arxiv", "paper", "è«–æ–‡"]):
                return "paper"
        
        return "general"
    
    def clear(self):
        """æ¸…é™¤ç•¶å‰è¼‰å…¥çš„æ–‡ä»¶å’Œ RAG ç³»çµ±"""
        self.current_files = []
        self.is_initialized = False
        self.processor = None
        self.bm25_retriever = None
        self.vector_retriever = None
        self.hybrid_search = None
        self.reranker = None
        self.rag_pipeline = None
        self.formatter = None
        self.shared_embeddings = None


# å…¨å±€å¯¦ä¾‹ï¼ˆç”¨æ–¼ UIï¼‰
_private_rag_instance: Optional[PrivateFileRAG] = None


def get_private_rag_instance() -> PrivateFileRAG:
    """ç²å–å…¨å±€ç§æœ‰æ–‡ä»¶ RAG å¯¦ä¾‹"""
    global _private_rag_instance
    if _private_rag_instance is None:
        _private_rag_instance = PrivateFileRAG()
    return _private_rag_instance


def reset_private_rag_instance():
    """é‡ç½®å…¨å±€å¯¦ä¾‹"""
    global _private_rag_instance
    _private_rag_instance = None


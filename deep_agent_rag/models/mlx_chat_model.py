"""
MLX æ¨¡å‹åŒ…è£å™¨
å°‡ MLX æ¨¡å‹æ•´åˆåˆ° LangChain ç”Ÿæ…‹ç³»çµ±ä¸­
"""
from typing import List, Optional, Any
import mlx.core as mx
from mlx_lm import load, generate as mlx_generate

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage

from ..config import MLX_MODEL_ID, MLX_MAX_TOKENS, MLX_TEMPERATURE


class MLXChatModel(BaseChatModel):
    """
    MLX æ¨¡å‹çš„ LangChain åŒ…è£å™¨
    å°‡ MLX æ¨¡å‹æ•´åˆåˆ° LangChain ç”Ÿæ…‹ç³»çµ±ä¸­
    """
    model: Any = None
    tokenizer: Any = None
    max_tokens: int = MLX_MAX_TOKENS
    temperature: float = MLX_TEMPERATURE
    
    def __init__(self, model, tokenizer, max_tokens=MLX_MAX_TOKENS, temperature=MLX_TEMPERATURE, **kwargs):
        super().__init__(**kwargs)
        self.model = model
        self.tokenizer = tokenizer
        self.max_tokens = max_tokens
        self.temperature = temperature
    
    @property
    def _llm_type(self) -> str:
        return "mlx"
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """ç”Ÿæˆå›ç­”"""
        # å°‡ LangChain æ¶ˆæ¯è½‰æ›ç‚ºæ¨¡å‹æ ¼å¼
        formatted_messages = []
        for msg in messages:
            if isinstance(msg, SystemMessage):
                formatted_messages.append({"role": "system", "content": msg.content})
            elif isinstance(msg, HumanMessage):
                formatted_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                formatted_messages.append({"role": "assistant", "content": msg.content})
        
        # ä½¿ç”¨ tokenizer æ ¼å¼åŒ–å°è©±
        try:
            prompt = self.tokenizer.apply_chat_template(
                formatted_messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except Exception:
            # å¦‚æœ apply_chat_template å¤±æ•—ï¼Œä½¿ç”¨æ‰‹å‹•æ ¼å¼
            prompt_parts = []
            for msg in formatted_messages:
                role = msg["role"]
                content = msg["content"]
                if role == "system":
                    prompt_parts.append(f"<|im_start|>system\n{content}<|im_end|>")
                elif role == "user":
                    prompt_parts.append(f"<|im_start|>user\n{content}<|im_end|>")
                elif role == "assistant":
                    prompt_parts.append(f"<|im_start|>assistant\n{content}<|im_end|>")
            prompt_parts.append("<|im_start|>assistant\n")
            prompt = "\n".join(prompt_parts)
        
        # ä½¿ç”¨ MLX çš„ generate å‡½æ•¸ä¸€æ¬¡æ€§ç”Ÿæˆï¼ˆæ›´å¿«ï¼‰
        # æ³¨æ„ï¼šMLX çš„ generate ä¸æ”¯æ´ temperature åƒæ•¸ï¼Œä½†é€Ÿåº¦æ›´å¿«
        try:
            response_text = mlx_generate(
                self.model,
                self.tokenizer,
                prompt=prompt,
                max_tokens=self.max_tokens,
                verbose=False
            )# ã€ä¿®å¾©ã€‘æ¸…ç†è¼¸å‡ºä¸­çš„ç‰¹æ®Šæ¨™è¨˜
            response_text = response_text.strip()
            # ç§»é™¤ <|im_end|> å’Œ <|im_start|> æ¨™è¨˜
            response_text = response_text.replace("<|im_end|>", "").replace("<|im_start|>", "")
            # ç§»é™¤å¤šé¤˜çš„ç©ºç™½è¡Œ
            response_text = "\n".join(line for line in response_text.split("\n") if line.strip())
        except Exception as e:
            # å¦‚æœ generate å¤±æ•—ï¼Œå›é€€åˆ°é€å€‹ token ç”Ÿæˆ
            print(f"   âš ï¸ MLX generate å¤±æ•—ï¼Œä½¿ç”¨é€å€‹ token ç”Ÿæˆ: {e}")
            tokens = self.tokenizer.encode(prompt)
            tokens = mx.array(tokens)
            
            generated_tokens = []
            for _ in range(self.max_tokens):
                # å‰å‘å‚³æ’­
                logits = self.model(tokens[None, :])
                logits = logits[0, -1, :]
                
                # ä½¿ç”¨è²ªå©ªè§£ç¢¼ï¼ˆæœ€å¿«ï¼‰
                next_token = mx.argmax(logits)
                next_token = int(next_token.item())
                
                # æª¢æŸ¥çµæŸç¬¦
                if next_token == self.tokenizer.eos_token_id:
                    break
                
                generated_tokens.append(next_token)
                tokens = mx.concatenate([tokens, mx.array([next_token])])
            
            # è§£ç¢¼å›ç­”
            response_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        # ã€é¡å¤–ä¿éšªã€‘å†æ¬¡æ¸…ç†è¼¸å‡ºï¼Œç¢ºä¿æ²’æœ‰éºæ¼çš„ç‰¹æ®Šæ¨™è¨˜
        response_text = response_text.strip()
        response_text = response_text.replace("<|im_end|>", "").replace("<|im_start|>", "")
        response_text = response_text.strip()
        
        # å‰µå»º ChatResult
        message = AIMessage(content=response_text)
        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])
    
    def bind_tools(self, tools: List[Any], **kwargs: Any):
        """
        ç¶å®šå·¥å…·ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
        æ³¨æ„ï¼šMLX æ¨¡å‹å¯èƒ½ä¸ç›´æ¥æ”¯æ´å·¥å…·èª¿ç”¨ï¼Œé€™è£¡è¿”å›è‡ªèº«
        å¦‚æœéœ€è¦å·¥å…·èª¿ç”¨ï¼Œå¯èƒ½éœ€è¦é¡å¤–çš„å¾Œè™•ç†
        """
        # å°‡å·¥å…·ä¿¡æ¯æ·»åŠ åˆ°ç³»çµ±æç¤ºä¸­
        self._tools = tools
        return self


# å…¨åŸŸ MLX æ¨¡å‹è®Šæ•¸ï¼ˆå»¶é²è¼‰å…¥ï¼‰
_mlx_model = None
_mlx_tokenizer = None


def load_mlx_model():
    """è¼‰å…¥ MLX æ¨¡å‹ï¼ˆåªè¼‰å…¥ä¸€æ¬¡ï¼‰"""
    global _mlx_model, _mlx_tokenizer
    
    if _mlx_model is None or _mlx_tokenizer is None:
        print(f"ğŸ“¦ æ­£åœ¨è¼‰å…¥ MLX æ¨¡å‹ {MLX_MODEL_ID}...")
        _mlx_model, _mlx_tokenizer = load(MLX_MODEL_ID)
        print("âœ… MLX æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    
    return _mlx_model, _mlx_tokenizer


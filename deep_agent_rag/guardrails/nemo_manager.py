"""
Hybrid Guardrail Manager
æ··åˆå¼å…§å®¹éæ¿¾ç®¡ç†å™¨ï¼Œå— NeMo Guardrails å•Ÿç™¼

æ•´åˆé—œéµå­—å¯†åº¦æª¢æŸ¥ï¼ˆå¿«é€Ÿå±¤ï¼‰å’Œèªç¾©ä¸»é¡Œéæ¿¾ï¼ˆæ·±åº¦å±¤ï¼‰
æ”¯æ´è¼¸å…¥/è¼¸å‡ºé›™å‘éæ¿¾
"""

import os
import yaml
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import numpy as np
from sentence_transformers import SentenceTransformer
import jieba

# ç²å–é…ç½®æ–‡ä»¶è·¯å¾‘
GUARDRAILS_CONFIG_DIR = Path(__file__).parent / "config"
CONFIG_FILE = GUARDRAILS_CONFIG_DIR / "config.yml"
RAILS_FILE = GUARDRAILS_CONFIG_DIR / "rails.txt"


class SemanticTopic:
    """èªç¾©ä¸»é¡Œå®šç¾©"""
    def __init__(self, name: str, display_name: str, examples: List[str], blocked_message: str):
        self.name = name
        self.display_name = display_name
        self.examples = examples
        self.blocked_message = blocked_message
        self.embeddings: Optional[np.ndarray] = None


class HybridGuardrailManager:
    """
    æ··åˆå¼ Guardrail ç®¡ç†å™¨
    
    åŠŸèƒ½ï¼š
    1. å¿«é€Ÿé—œéµå­—å¯†åº¦æª¢æŸ¥ï¼ˆæ¯«ç§’ç´šï¼‰
    2. èªç¾©ä¸»é¡ŒåŒ¹é…ï¼ˆä½¿ç”¨ sentence-transformersï¼‰
    3. è¼¸å…¥/è¼¸å‡ºé›™å‘éæ¿¾
    4. å¯é…ç½®çš„å•Ÿç”¨/åœç”¨é¸é …
    """
    
    def __init__(self, config_path: Optional[Path] = None):
        """
        åˆå§‹åŒ– Guardrail ç®¡ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾‘ï¼ˆé»˜èªä½¿ç”¨å…§å»ºé…ç½®ï¼‰
        """
        self.config_path = config_path or CONFIG_FILE
        self.config: Dict = {}
        self.topics: List[SemanticTopic] = []
        self.model: Optional[SentenceTransformer] = None
        self._initialized = False
        
        # è¼‰å…¥é…ç½®
        self._load_config()
        
        # åˆå§‹åŒ– jieba
        self._init_jieba()
        
        # æ‡¶åŠ è¼‰ embedding æ¨¡å‹ï¼ˆåªåœ¨éœ€è¦æ™‚åˆå§‹åŒ–ï¼‰
        if self.config.get("enabled", {}).get("semantic_filter", False):
            self._init_semantic_model()
    
    def _load_config(self):
        """è¼‰å…¥é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            print(f"âœ… è¼‰å…¥ Guardrails é…ç½®: {self.config_path}")
        except Exception as e:
            print(f"âš ï¸  ç„¡æ³•è¼‰å…¥ Guardrails é…ç½®ï¼Œä½¿ç”¨é»˜èªè¨­å®š: {e}")
            self._load_default_config()
    
    def _load_default_config(self):
        """è¼‰å…¥é»˜èªé…ç½®"""
        self.config = {
            "enabled": {
                "keyword_filter": True,
                "semantic_filter": False,  # é»˜èªé—œé–‰èªç¾©éæ¿¾
                "input_rails": True,
                "output_rails": True
            },
            "keyword_filter": {
                "threshold": 0.05,
                "blocked_keywords": [
                    "ä¼Šæ–¯è˜­æ•™", "é˜¿æ‹‰", "å›æ•™å¾’", "é»˜ç½•é»˜å¾·",
                    "Islam", "Allah", "Muslim", "Muhammad"
                ],
                "blocked_message": "æŠ±æ­‰ï¼Œæ‚¨çš„å•é¡ŒåŒ…å«æ•æ„Ÿå…§å®¹ï¼Œç„¡æ³•å›ç­”ã€‚è«‹æ›å€‹è©±é¡Œæˆ–é‡æ–°è¡¨è¿°æ‚¨çš„å•é¡Œã€‚"
            },
            "semantic_filter": {
                "similarity_threshold": 0.75,
                "topics": []
            },
            "embeddings": {
                "model": "sentence-transformers/all-MiniLM-L6-v2",
                "cache_embeddings": True
            }
        }
    
    def _init_jieba(self):
        """åˆå§‹åŒ– jieba åˆ†è©"""
        keywords = self.config.get("keyword_filter", {}).get("blocked_keywords", [])
        for keyword in keywords:
            jieba.add_word(keyword, freq=10000, tag='sensitive')
    
    def _init_semantic_model(self):
        """åˆå§‹åŒ–èªç¾©æ¨¡å‹ï¼ˆæ‡¶åŠ è¼‰ï¼‰"""
        if self._initialized:
            return
        
        try:
            model_name = self.config.get("embeddings", {}).get("model", "sentence-transformers/all-MiniLM-L6-v2")
            print(f"ğŸ”„ æ­£åœ¨è¼‰å…¥èªç¾©æ¨¡å‹: {model_name}")
            self.model = SentenceTransformer(model_name)
            
            # è¼‰å…¥ä¸»é¡Œå®šç¾©
            self._load_topics()
            
            # é è¨ˆç®—ä¸»é¡Œ embeddings
            self._precompute_topic_embeddings()
            
            self._initialized = True
            print(f"âœ… èªç¾©æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œå…± {len(self.topics)} å€‹ä¸»é¡Œ")
        except Exception as e:
            print(f"âš ï¸  ç„¡æ³•è¼‰å…¥èªç¾©æ¨¡å‹: {e}")
            self.config["enabled"]["semantic_filter"] = False
    
    def _load_topics(self):
        """å¾é…ç½®è¼‰å…¥ä¸»é¡Œå®šç¾©"""
        self.topics = []
        
        # å¾ YAML é…ç½®è¼‰å…¥
        topics_config = self.config.get("semantic_filter", {}).get("topics", [])
        for topic_data in topics_config:
            topic = SemanticTopic(
                name=topic_data.get("name", ""),
                display_name=topic_data.get("display_name", ""),
                examples=topic_data.get("examples", []),
                blocked_message=topic_data.get("blocked_message", "æŠ±æ­‰ï¼Œç„¡æ³•å›ç­”æ­¤å•é¡Œã€‚")
            )
            self.topics.append(topic)
        
        print(f"ğŸ“‹ è¼‰å…¥äº† {len(self.topics)} å€‹èªç¾©ä¸»é¡Œ")
    
    def _precompute_topic_embeddings(self):
        """é è¨ˆç®—æ‰€æœ‰ä¸»é¡Œçš„ embeddings"""
        if not self.model:
            return
        
        for topic in self.topics:
            if topic.examples:
                topic.embeddings = self.model.encode(topic.examples, convert_to_numpy=True)
    
    def _check_keyword_density(self, text: str) -> Tuple[bool, float, str]:
        """
        æª¢æŸ¥é—œéµå­—å¯†åº¦
        
        Returns:
            (should_block, density, message)
        """
        if not text or not text.strip():
            return False, 0.0, ""
        
        # ä½¿ç”¨ jieba é€²è¡Œæ–·è©
        words = list(jieba.cut(text))
        total_words = len(words)
        
        if total_words == 0:
            return False, 0.0, ""
        
        # å»ºç«‹å°å¯«æ•æ„Ÿè©é›†åˆ
        blocked_keywords = self.config.get("keyword_filter", {}).get("blocked_keywords", [])
        blocked_keywords_lower = {k.lower() for k in blocked_keywords}
        
        # è¨ˆç®—æ•æ„Ÿè©æ•¸é‡
        sensitive_word_count = sum(
            1 for word in words
            if word.strip().lower() in blocked_keywords_lower
        )
        
        # è¨ˆç®—å¯†åº¦
        density = sensitive_word_count / total_words
        threshold = self.config.get("keyword_filter", {}).get("threshold", 0.05)
        
        should_block = density >= threshold
        message = self.config.get("keyword_filter", {}).get("blocked_message", "") if should_block else ""
        
        return should_block, density, message
    
    def _check_semantic_topic(self, text: str) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        æª¢æŸ¥èªç¾©ä¸»é¡ŒåŒ¹é…
        
        Returns:
            (should_block, topic_name, blocked_message)
        """
        if not self.model or not self.topics:
            return False, None, None
        
        # è¨ˆç®—è¼¸å…¥æ–‡æœ¬çš„ embedding
        text_embedding = self.model.encode([text], convert_to_numpy=True)[0]
        
        # ç²å–ç›¸ä¼¼åº¦é–€æª»
        threshold = self.config.get("semantic_filter", {}).get("similarity_threshold", 0.75)
        
        # æª¢æŸ¥æ¯å€‹ä¸»é¡Œ
        for topic in self.topics:
            if topic.embeddings is None or len(topic.embeddings) == 0:
                continue
            
            # è¨ˆç®—èˆ‡æ‰€æœ‰ç¯„ä¾‹çš„ç›¸ä¼¼åº¦
            similarities = np.dot(topic.embeddings, text_embedding) / (
                np.linalg.norm(topic.embeddings, axis=1) * np.linalg.norm(text_embedding)
            )
            
            # å–æœ€å¤§ç›¸ä¼¼åº¦
            max_similarity = np.max(similarities)
            
            # å¦‚æœè¶…éé–€æª»ï¼Œé˜»æ“‹
            if max_similarity >= threshold:
                print(f"ğŸš« èªç¾©åŒ¹é…: {topic.display_name} (ç›¸ä¼¼åº¦: {max_similarity:.2%})")
                return True, topic.name, topic.blocked_message
        
        return False, None, None
    
    def check_input(self, text: str) -> Tuple[bool, str]:
        """
        æª¢æŸ¥ç”¨æˆ¶è¼¸å…¥
        
        Args:
            text: ç”¨æˆ¶è¼¸å…¥æ–‡æœ¬
        
        Returns:
            (should_block, message): æ˜¯å¦é˜»æ“‹, é˜»æ“‹è¨Šæ¯ï¼ˆå¦‚æœé˜»æ“‹ï¼‰
        """
        if not self.config.get("enabled", {}).get("input_rails", True):
            return False, ""
        
        # 1. å¿«é€Ÿé—œéµå­—æª¢æŸ¥
        if self.config.get("enabled", {}).get("keyword_filter", True):
            should_block, density, message = self._check_keyword_density(text)
            if should_block:
                print(f"ğŸš« é—œéµå­—éæ¿¾: å¯†åº¦ {density:.2%}")
                return True, message
        
        # 2. èªç¾©ä¸»é¡Œæª¢æŸ¥
        if self.config.get("enabled", {}).get("semantic_filter", False):
            should_block, topic, message = self._check_semantic_topic(text)
            if should_block:
                return True, message or "æŠ±æ­‰ï¼Œç„¡æ³•å›ç­”æ­¤å•é¡Œã€‚"
        
        return False, ""
    
    def check_output(self, text: str) -> Tuple[bool, str]:
        """
        æª¢æŸ¥ LLM è¼¸å‡º
        
        Args:
            text: LLM è¼¸å‡ºæ–‡æœ¬
        
        Returns:
            (should_block, filtered_text): æ˜¯å¦é˜»æ“‹, éæ¿¾å¾Œçš„æ–‡æœ¬
        """
        if not self.config.get("enabled", {}).get("output_rails", True):
            return False, text
        
        # 1. å¿«é€Ÿé—œéµå­—æª¢æŸ¥
        if self.config.get("enabled", {}).get("keyword_filter", True):
            should_block, density, message = self._check_keyword_density(text)
            if should_block:
                print(f"ğŸš« è¼¸å‡ºéæ¿¾: å¯†åº¦ {density:.2%}")
                return True, message
        
        # 2. èªç¾©ä¸»é¡Œæª¢æŸ¥
        if self.config.get("enabled", {}).get("semantic_filter", False):
            should_block, topic, message = self._check_semantic_topic(text)
            if should_block:
                return True, message or "æŠ±æ­‰ï¼Œç„¡æ³•æä¾›æ­¤å›æ‡‰ã€‚"
        
        return False, text
    
    def get_status(self) -> Dict:
        """ç²å–ç•¶å‰ Guardrails ç‹€æ…‹"""
        return {
            "enabled": self.config.get("enabled", {}),
            "keyword_filter": {
                "threshold": self.config.get("keyword_filter", {}).get("threshold", 0.05),
                "keywords_count": len(self.config.get("keyword_filter", {}).get("blocked_keywords", []))
            },
            "semantic_filter": {
                "initialized": self._initialized,
                "topics_count": len(self.topics),
                "threshold": self.config.get("semantic_filter", {}).get("similarity_threshold", 0.75)
            }
        }
    
    def get_topics_info(self) -> List[Dict]:
        """ç²å–ä¸»é¡Œè³‡è¨Š"""
        return [
            {
                "name": topic.name,
                "display_name": topic.display_name,
                "examples_count": len(topic.examples)
            }
            for topic in self.topics
        ]


# å…¨å±€å–®ä¾‹
_guardrail_manager: Optional[HybridGuardrailManager] = None


def get_guardrail_manager() -> HybridGuardrailManager:
    """ç²å–å…¨å±€ Guardrail ç®¡ç†å™¨å–®ä¾‹"""
    global _guardrail_manager
    if _guardrail_manager is None:
        _guardrail_manager = HybridGuardrailManager()
    return _guardrail_manager

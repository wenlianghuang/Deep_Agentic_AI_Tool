"""
Agent å·¥å…·å®šç¾©
åŒ…å«è‚¡ç¥¨æŸ¥è©¢ã€ç¶²è·¯æœå°‹ã€PDF çŸ¥è­˜åº«æŸ¥è©¢ã€arXiv è«–æ–‡æœå°‹ç­‰å·¥å…·
"""
import yfinance as yf
from langchain_core.tools import tool
from langchain_community.tools.tavily_search import TavilySearchResults


@tool
def get_company_deep_info(ticker: str) -> str:
    """æŸ¥è©¢è‚¡ç¥¨çš„è©³ç´°ç‡Ÿé‹ç‹€æ³ï¼ŒåŒ…æ‹¬ç¾åƒ¹ã€å¸‚å€¼ã€æœ¬ç›Šæ¯”ã€ç‡Ÿæ”¶å¢é•·ç­‰æ·±åº¦æ•¸æ“šã€‚"""
    try:
        stock = yf.Ticker(ticker)
        info = stock.info
        summary = (
            f"è‚¡ç¥¨: {info.get('longName')} ({ticker})\n"
            f"ç¾åƒ¹: {info.get('currentPrice')} {info.get('currency')}\n"
            f"å¸‚å€¼: {info.get('marketCap')}\n"
            f"æœ¬ç›Šæ¯” (PE): {info.get('trailingPE')}\n"
            f"ç‡Ÿæ”¶å¢é•·: {info.get('revenueGrowth')}\n"
            f"æ¥­å‹™æ‘˜è¦: {info.get('longBusinessSummary')[:500]}..."
        )
        return summary
    except Exception as e:
        return f"æ•¸æ“šæŸ¥è©¢å¤±æ•—: {e}"


@tool
def search_web(query: str) -> str:
    """æœå°‹ç¶²éš›ç¶²è·¯ä»¥ç²å–æœ€æ–°æ–°èæˆ–ä¸€èˆ¬çŸ¥è­˜ã€‚"""
    try:
        tool = TavilySearchResults(k=5)  # å¢åŠ æœå°‹é‡ä»¥ç²å–æ·±åº¦è³‡è¨Š
        return str(tool.invoke(query))
    except Exception as e:
        return f"æœå°‹éŒ¯èª¤: {e}"


def get_product_names_from_files(data_dir: str = "data") -> list:
    """
    å¾ data æ–‡ä»¶å¤¾ä¸­çš„ PDF æ–‡ä»¶åå‹•æ…‹æå–ç”¢å“åç¨±ã€‚
    
    Args:
        data_dir: PDF æ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¾è·¯å¾‘
        
    Returns:
        ç”¢å“åç¨±åˆ—è¡¨ï¼ˆåŒ…å«å¸¶ç ´æŠ˜è™Ÿå’Œç©ºæ ¼çš„ç‰ˆæœ¬ï¼‰
    """
    import os
    
    product_names = []
    
    try:
        # ç²å–çµ•å°è·¯å¾‘
        if not os.path.isabs(data_dir):
            # å‡è¨­ç›¸å°æ–¼å°ˆæ¡ˆæ ¹ç›®éŒ„
            base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            data_dir = os.path.join(base_dir, data_dir)
        
        if not os.path.exists(data_dir):
            print(f"   âš ï¸ è³‡æ–™å¤¾ä¸å­˜åœ¨: {data_dir}")
            return []
        
        # æƒæ PDF æ–‡ä»¶
        for filename in os.listdir(data_dir):
            if filename.endswith('.pdf'):
                # å¾æ–‡ä»¶åä¸­æå–ç”¢å“åç¨±ï¼ˆä¾‹å¦‚ "Lumina-Grid æ™ºæ…§èƒ½æºæ§åˆ¶å™¨.pdf" -> "Lumina-Grid"ï¼‰
                # æå–ç¬¬ä¸€å€‹ç©ºæ ¼å‰çš„éƒ¨åˆ†ä½œç‚ºç”¢å“åç¨±
                product_name = filename.split()[0] if ' ' in filename else filename.replace('.pdf', '')
                
                # ç§»é™¤å¯èƒ½çš„æ“´å±•åï¼ˆå¦‚æœæ²’æœ‰ç©ºæ ¼çš„è©±ï¼‰
                product_name = product_name.replace('.pdf', '')
                
                if product_name:
                    # æ·»åŠ åŸå§‹åç¨±ï¼ˆå¸¶ç ´æŠ˜è™Ÿï¼‰
                    product_names.append(product_name)
                    
                    # æ·»åŠ ç©ºæ ¼ç‰ˆæœ¬ï¼ˆå°‡ç ´æŠ˜è™Ÿæ›¿æ›ç‚ºç©ºæ ¼ï¼‰
                    if '-' in product_name:
                        product_names.append(product_name.replace('-', ' '))
        
        if product_names:
            print(f"   âœ… å¾ {len(set(product_names))//2} å€‹ PDF æ–‡ä»¶ä¸­æå–ç”¢å“åç¨±: {', '.join(set([p for p in product_names if '-' in p]))}")
        
    except Exception as e:
        print(f"   âš ï¸ è®€å–ç”¢å“åç¨±å¤±æ•—: {e}")
        # è¿”å›ç©ºåˆ—è¡¨ï¼Œè®“èª¿ç”¨è€…æ±ºå®šæ˜¯å¦ä½¿ç”¨å‚™ç”¨åˆ—è¡¨
    
    return product_names


def query_pdf_knowledge(query: str, rag_retriever=None) -> str:
    """
    æŸ¥è©¢ PDF çŸ¥è­˜åº«ä¸­çš„ç›¸é—œè³‡è¨Šã€‚
    ç•¶å•é¡Œæ¶‰åŠè«–æ–‡å…§å®¹ã€ç ”ç©¶æ¦‚å¿µã€æ–¹æ³•è«–æˆ–å­¸è¡“ç†è«–æ™‚ä½¿ç”¨æ­¤å·¥å…·ã€‚
    
    ç¾åœ¨ä½¿ç”¨ Private File RAG ç³»çµ±ï¼Œæ”¯æŒå¤šæ–‡ä»¶ã€é€²éš RAG æ–¹æ³•ã€‚
    
    é€™å€‹å‡½æ•¸æœƒæ™ºèƒ½æ“´å±•æŸ¥è©¢ï¼š
    1. å¦‚æœæŸ¥è©¢ä¸­æ²’æœ‰æ˜ç¢ºçš„ç”¢å“åç¨±ï¼Œæœƒå…ˆé€²è¡Œåˆæ­¥æª¢ç´¢
    2. å¾åˆæ­¥æª¢ç´¢çµæœå’ŒæŸ¥è©¢æœ¬èº«æ¨æ–·å¯èƒ½çš„ç”¢å“åç¨±
    3. ä½¿ç”¨æ“´å±•å¾Œçš„æŸ¥è©¢é€²è¡Œå®Œæ•´æª¢ç´¢
    """
    if not rag_retriever:
        return "PDF çŸ¥è­˜åº«æœªè¼‰å…¥ï¼Œç„¡æ³•æŸ¥è©¢ã€‚"
    
    try:
        print(f"   ğŸ” [RAG] æ­£åœ¨æŸ¥è©¢ PDF çŸ¥è­˜åº«: {query}")
        
        # æª¢æŸ¥æ˜¯å¦æ˜¯ Private File RAG å¯¦ä¾‹
        from ..rag.private_file_rag import PrivateFileRAG
        from ..utils.llm_utils import get_llm
        from langchain_core.messages import HumanMessage
        
        if not isinstance(rag_retriever, PrivateFileRAG):
            return "PDF çŸ¥è­˜åº«æ ¼å¼ä¸æ­£ç¢ºï¼Œè«‹é‡æ–°åˆå§‹åŒ–ã€‚"
        
        # å·²çŸ¥çš„ç”¢å“åç¨±åˆ—è¡¨ - å¾ data æ–‡ä»¶å¤¾å‹•æ…‹è¼‰å…¥
        product_names = get_product_names_from_files()
        
        # å¦‚æœå‹•æ…‹è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨åˆ—è¡¨
        if not product_names:
            print("   âš ï¸ ç„¡æ³•å¾æ–‡ä»¶è¼‰å…¥ç”¢å“åç¨±ï¼Œä½¿ç”¨å‚™ç”¨åˆ—è¡¨")
            product_names = [
                "Lumina-Grid", "Gaia-7", "Nebula-X", "Deep-Void", "Synapse-Link",
                "Lumina Grid", "Gaia 7", "Nebula X", "Deep Void", "Synapse Link"
            ]
        
        # æª¢æŸ¥æŸ¥è©¢ä¸­æ˜¯å¦å·²ç¶“åŒ…å«ç”¢å“åç¨±
        query_lower = query.lower()
        has_product_in_query = any(
            name.lower() in query_lower for name in product_names
        )
        
        # å¦‚æœæŸ¥è©¢ä¸­æ²’æœ‰ç”¢å“åç¨±ï¼Œå˜—è©¦æ™ºèƒ½æ“´å±•
        expanded_query = query
        if not has_product_in_query:
            print(f"   ğŸ” [æŸ¥è©¢æ“´å±•] æŸ¥è©¢ä¸­æ²’æœ‰æ˜ç¢ºçš„ç”¢å“åç¨±ï¼Œå˜—è©¦æ™ºèƒ½æ“´å±•...")
            
            # ç­–ç•¥ 1: å…ˆé€²è¡Œä¸€æ¬¡åˆæ­¥æª¢ç´¢ï¼ŒæŸ¥çœ‹ PDF å…§å®¹
            # ä½¿ç”¨è¼ƒå¤§çš„ top_k ä¾†ç²å–æ›´å¤šå€™é¸çµæœ
            preliminary_result = rag_retriever.query(
                query=query,
                top_k=10,  # ç²å–æ›´å¤šçµæœä»¥ä¾¿åˆ†æ
                use_llm=False  # åªæª¢ç´¢ï¼Œä¸ç”Ÿæˆå›ç­”
            )
            
            if preliminary_result.get("success") and preliminary_result.get("results"):
                # å¾åˆæ­¥æª¢ç´¢çµæœä¸­æå–æ–‡æœ¬
                contexts = []
                for res in preliminary_result.get("results", [])[:5]:  # åªå–å‰5å€‹çµæœ
                    contexts.append(res.get("content", ""))
                
                combined_context = "\n\n".join(contexts)
                # é™åˆ¶é•·åº¦é¿å…éé•·
                context_snippet = combined_context[:2000]
                
                # ä½¿ç”¨ LLM å¾æŸ¥è©¢å’Œæª¢ç´¢çµæœä¸­æ¨æ–·ç”¢å“åç¨±
                try:
                    llm = get_llm()
                    infer_prompt = f"""æ ¹æ“šä»¥ä¸‹æŸ¥è©¢å’Œ PDF å…§å®¹ç‰‡æ®µï¼Œæ¨æ–·ç”¨æˆ¶å¯èƒ½æƒ³æŸ¥è©¢å“ªå€‹ç”¢å“çš„ä¿¡æ¯ã€‚

æŸ¥è©¢ï¼š{query}

PDF å…§å®¹ç‰‡æ®µï¼š
{context_snippet}

å·²çŸ¥ç”¢å“åˆ—è¡¨ï¼š{', '.join(product_names)}

è«‹æ ¹æ“šæŸ¥è©¢å…§å®¹å’Œ PDF ç‰‡æ®µæ¨æ–·æœ€å¯èƒ½çš„ç”¢å“åç¨±ã€‚
å¦‚æœèƒ½å¤ ç¢ºå®šç”¢å“åç¨±ï¼Œè«‹åªè¿”å›ç”¢å“åç¨±ï¼ˆä¾‹å¦‚ï¼š"Lumina-Grid"ï¼‰ã€‚
å¦‚æœç„¡æ³•ç¢ºå®šï¼Œè«‹è¿”å› "ç„¡"ã€‚
åªè¿”å›ç”¢å“åç¨±æˆ–"ç„¡"ï¼Œä¸è¦å…¶ä»–è§£é‡‹ã€‚"""
                    
                    messages = [HumanMessage(content=infer_prompt)]
                    response = llm.invoke(messages)
                    inferred_product = response.content.strip() if hasattr(response, 'content') else str(response).strip()
                    
                    # æª¢æŸ¥æ¨æ–·çš„ç”¢å“æ˜¯å¦åœ¨å·²çŸ¥åˆ—è¡¨ä¸­
                    if inferred_product and inferred_product.lower() not in ["ç„¡", "æ— ", "none", "no", ""]:
                        # æ‰¾åˆ°åŒ¹é…çš„ç”¢å“åç¨±
                        matched_product = None
                        for name in product_names:
                            if name.lower() in inferred_product.lower() or inferred_product.lower() in name.lower():
                                matched_product = name
                                break
                        
                        if matched_product:
                            expanded_query = f"{matched_product} {query}"
                            print(f"   âœ… [æŸ¥è©¢æ“´å±•] å¾ PDF å…§å®¹æ¨æ–·ç”¢å“åç¨± '{matched_product}'ï¼Œæ“´å±•æŸ¥è©¢ç‚ºï¼š{expanded_query}")
                        else:
                            # å¦‚æœæ¨æ–·çš„ç”¢å“ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œä½†çœ‹èµ·ä¾†åƒç”¢å“åç¨±ï¼Œä¹Ÿå¯ä»¥å˜—è©¦
                            # æª¢æŸ¥æ˜¯å¦åŒ…å«å¸¸è¦‹çš„ç”¢å“åç¨±æ¨¡å¼
                            for name in product_names:
                                if any(word.lower() in inferred_product.lower() for word in name.split() if len(word) > 2):
                                    expanded_query = f"{name} {query}"
                                    print(f"   âœ… [æŸ¥è©¢æ“´å±•] å¾æ¨æ–·çµæœ '{inferred_product}' åŒ¹é…åˆ°ç”¢å“ '{name}'ï¼Œæ“´å±•æŸ¥è©¢ç‚ºï¼š{expanded_query}")
                                    break
                except Exception as e:
                    print(f"   âš ï¸ [æŸ¥è©¢æ“´å±•] LLM æ¨æ–·ç”¢å“åç¨±å¤±æ•—: {e}ï¼Œä½¿ç”¨åŸå§‹æŸ¥è©¢")
            
            # ç­–ç•¥ 2: å¦‚æœåˆæ­¥æª¢ç´¢æ²’æœ‰å¹«åŠ©ï¼Œå˜—è©¦ç›´æ¥å¾æŸ¥è©¢æ¨æ–·
            if expanded_query == query:
                try:
                    llm = get_llm()
                    # æª¢æŸ¥æŸ¥è©¢ä¸­æ˜¯å¦åŒ…å«ç‰ˆæœ¬è™Ÿã€æŠ€è¡“è¦æ ¼ç­‰é—œéµè©
                    version_keywords = ["ç‰ˆæœ¬", "version", "v1", "v2", "v3", "v1.", "v2.", "v3.", "v4", "v5"]
                    spec_keywords = ["æ™‚è„ˆ", "é »ç‡", "clock", "GHz", "æ ¸å¿ƒ", "æ™¶ç‰‡", "chip", "core", "èƒ½æº", "è½‰æ›ç‡"]
                    
                    has_version_or_spec = any(
                        keyword in query_lower for keyword in version_keywords + spec_keywords
                    )
                    
                    if has_version_or_spec:
                        infer_prompt = f"""æ ¹æ“šä»¥ä¸‹æŸ¥è©¢ï¼Œæ¨æ–·ç”¨æˆ¶å¯èƒ½æƒ³æŸ¥è©¢å“ªå€‹ç”¢å“çš„ä¿¡æ¯ã€‚

æŸ¥è©¢ï¼š{query}

å·²çŸ¥ç”¢å“åˆ—è¡¨ï¼š{', '.join(product_names)}

è«‹æ ¹æ“šæŸ¥è©¢å…§å®¹æ¨æ–·æœ€å¯èƒ½çš„ç”¢å“åç¨±ã€‚å¦‚æœæŸ¥è©¢ä¸­æ²’æœ‰æ˜ç¢ºçš„ç”¢å“ä¿¡æ¯ï¼Œè«‹è¿”å› "ç„¡"ã€‚
åªè¿”å›ç”¢å“åç¨±æˆ–"ç„¡"ï¼Œä¸è¦å…¶ä»–è§£é‡‹ã€‚"""
                        
                        messages = [HumanMessage(content=infer_prompt)]
                        response = llm.invoke(messages)
                        inferred_product = response.content.strip() if hasattr(response, 'content') else str(response).strip()
                        
                        if inferred_product and inferred_product.lower() not in ["ç„¡", "æ— ", "none", "no", ""]:
                            # æ‰¾åˆ°åŒ¹é…çš„ç”¢å“åç¨±
                            matched_product = None
                            for name in product_names:
                                if name.lower() in inferred_product.lower() or inferred_product.lower() in name.lower():
                                    matched_product = name
                                    break
                            
                            if matched_product:
                                expanded_query = f"{matched_product} {query}"
                                print(f"   âœ… [æŸ¥è©¢æ“´å±•] å¾æŸ¥è©¢æ¨æ–·ç”¢å“åç¨± '{matched_product}'ï¼Œæ“´å±•æŸ¥è©¢ç‚ºï¼š{expanded_query}")
                except Exception as e:
                    print(f"   âš ï¸ [æŸ¥è©¢æ“´å±•] å¾æŸ¥è©¢æ¨æ–·ç”¢å“åç¨±å¤±æ•—: {e}ï¼Œä½¿ç”¨åŸå§‹æŸ¥è©¢")
        
        # ä½¿ç”¨æ“´å±•å¾Œçš„æŸ¥è©¢é€²è¡Œå®Œæ•´æª¢ç´¢
        result = rag_retriever.query(
            query=expanded_query,  # ä½¿ç”¨æ“´å±•å¾Œçš„æŸ¥è©¢
            top_k=5,  # æª¢ç´¢å‰ 5 å€‹ç›¸é—œç‰‡æ®µ
            use_llm=True  # ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
        )
        
        if result.get("success"):
            answer = result.get("answer", "")
            if answer:
                # å¯é¸ï¼šæ·»åŠ ä½¿ç”¨çš„ RAG æ–¹æ³•ä¿¡æ¯ï¼ˆç”¨æ–¼èª¿è©¦ï¼‰
                rag_method = result.get("rag_method", "basic")
                if rag_method != "basic":
                    print(f"   ğŸ“Š [RAG] ä½¿ç”¨ {rag_method} æ–¹æ³•")
                return answer
            else:
                return "åœ¨ PDF çŸ¥è­˜åº«ä¸­æœªæ‰¾åˆ°ç›¸é—œè³‡è¨Šã€‚"
        else:
            error = result.get("error", "æœªçŸ¥éŒ¯èª¤")
            return f"PDF çŸ¥è­˜åº«æŸ¥è©¢å¤±æ•—: {error}"
            
    except Exception as e:
        return f"PDF çŸ¥è­˜åº«æŸ¥è©¢å¤±æ•—: {e}"


def extract_keywords_from_pdf(query: str, rag_retriever=None) -> str:
    """
    å¾ PDF çŸ¥è­˜åº«ä¸­æå–å­¸è¡“é—œéµå­—ï¼Œç”¨æ–¼ arXiv æœå°‹ã€‚
    ç•¶éœ€è¦æŸ¥æ‰¾ç›¸é—œå­¸è¡“è«–æ–‡æ™‚ä½¿ç”¨æ­¤å·¥å…·ã€‚
    
    Args:
        query: æŸ¥è©¢å•é¡Œ
        rag_retriever: RAG æª¢ç´¢å™¨ï¼ˆPrivateFileRAG å¯¦ä¾‹ï¼‰
    
    Returns:
        æå–çš„é—œéµå­—åˆ—è¡¨ï¼ˆJSON æ ¼å¼ï¼‰
    """
    if not rag_retriever:
        return "PDF çŸ¥è­˜åº«æœªè¼‰å…¥ï¼Œç„¡æ³•æå–é—œéµå­—ã€‚"
    
    try:
        from ..rag.private_file_rag import PrivateFileRAG
        from ..utils.llm_utils import get_llm
        from langchain_core.messages import HumanMessage
        import json
        
        if not isinstance(rag_retriever, PrivateFileRAG):
            return "PDF çŸ¥è­˜åº«æ ¼å¼ä¸æ­£ç¢ºã€‚"
        
        # å…ˆæŸ¥è©¢ PDF ç²å–ç›¸é—œå…§å®¹
        result = rag_retriever.query(
            query=query,
            top_k=10,
            use_llm=False  # åªæª¢ç´¢ï¼Œä¸ç”Ÿæˆå›ç­”
        )
        
        if not result.get("success") or not result.get("results"):
            return "åœ¨ PDF ä¸­æœªæ‰¾åˆ°ç›¸é—œå…§å®¹ï¼Œç„¡æ³•æå–é—œéµå­—ã€‚"
        
        # å¾æª¢ç´¢çµæœä¸­æå–æ–‡æœ¬
        contexts = []
        for res in result.get("results", [])[:3]:
            contexts.append(res.get("content", ""))
        
        combined_context = "\n\n".join(contexts)
        
        # ä½¿ç”¨ LLM æå–é—œéµå­—
        llm = get_llm()
        prompt = f"""å¾ä»¥ä¸‹ PDF å…§å®¹ä¸­æå–å­¸è¡“é—œéµå­—ï¼Œé€™äº›é—œéµå­—å°‡ç”¨æ–¼åœ¨ arXiv ä¸Šæœå°‹ç›¸é—œè«–æ–‡ã€‚

PDF å…§å®¹ï¼š
{combined_context}

åŸå§‹æŸ¥è©¢ï¼š{query}

è«‹æå–ï¼š
1. æ ¸å¿ƒå­¸è¡“æ¦‚å¿µå’Œè¡“èªï¼ˆè‹±æ–‡ï¼‰
2. ç ”ç©¶æ–¹æ³•å’ŒæŠ€è¡“åç¨±
3. ç›¸é—œé ˜åŸŸé—œéµè©

è¿”å›æ ¼å¼ï¼šJSON é™£åˆ—ï¼Œä¾‹å¦‚ï¼š["keyword1", "keyword2", "keyword3"]
åªè¿”å› JSON é™£åˆ—ï¼Œä¸è¦å…¶ä»–è§£é‡‹ã€‚"""
        
        messages = [HumanMessage(content=prompt)]
        response = llm.invoke(messages)
        keywords_text = response.content if hasattr(response, 'content') else str(response)
        
        # å˜—è©¦è§£æ JSON
        try:
            # æ¸…ç†éŸ¿æ‡‰ï¼Œæå– JSON éƒ¨åˆ†
            keywords_text = keywords_text.strip()
            if keywords_text.startswith("```"):
                # ç§»é™¤ç¨‹å¼ç¢¼å¡Šæ¨™è¨˜
                keywords_text = keywords_text.split("```")[1]
                if keywords_text.startswith("json"):
                    keywords_text = keywords_text[4:]
            keywords_text = keywords_text.strip()
            
            keywords = json.loads(keywords_text)
            if isinstance(keywords, list) and keywords:
                return json.dumps(keywords, ensure_ascii=False)
            else:
                return "æœªèƒ½æå–æœ‰æ•ˆé—œéµå­—ã€‚"
        except json.JSONDecodeError:
            # å¦‚æœ JSON è§£æå¤±æ•—ï¼Œå˜—è©¦æå–å¼•è™Ÿä¸­çš„å…§å®¹
            import re
            keywords = re.findall(r'"([^"]+)"', keywords_text)
            if keywords:
                return json.dumps(keywords, ensure_ascii=False)
            return f"é—œéµå­—æå–å¤±æ•—ï¼ŒLLM è¿”å›ï¼š{keywords_text}"
            
    except Exception as e:
        return f"æå–é—œéµå­—å¤±æ•—: {e}"


def search_arxiv_papers(keywords_json: str, max_results: int = 5) -> str:
    """
    ä½¿ç”¨ arXiv API æœå°‹ç›¸é—œè«–æ–‡ã€‚
    
    Args:
        keywords_json: é—œéµå­— JSON é™£åˆ—å­—ä¸²ï¼Œä¾‹å¦‚ï¼š'["machine learning", "neural networks"]'
        max_results: æœ€å¤§è¿”å›çµæœæ•¸
    
    Returns:
        è«–æ–‡åˆ—è¡¨çš„æ ¼å¼åŒ–å­—ä¸²
    """
    try:
        import json
        import arxiv
        from src.document_processor import DocumentProcessor
        
        # è§£æé—œéµå­—
        keywords = json.loads(keywords_json)
        if not isinstance(keywords, list) or not keywords:
            return "ç„¡æ•ˆçš„é—œéµå­—æ ¼å¼ã€‚"
        
        # æ§‹å»º arXiv æœå°‹æŸ¥è©¢
        # ä½¿ç”¨ OR é€£æ¥å¤šå€‹é—œéµå­—
        query = " OR ".join([f'all:"{kw}"' for kw in keywords[:5]])  # é™åˆ¶æœ€å¤š 5 å€‹é—œéµå­—
        
        print(f"   ğŸ“š [arXiv] æ­£åœ¨æœå°‹è«–æ–‡ï¼Œé—œéµå­—: {', '.join(keywords[:5])}")
        
        # æœå°‹è«–æ–‡
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.Relevance
        )
        
        papers = []
        for paper in search.results():
            papers.append({
                "title": paper.title,
                "authors": [author.name for author in paper.authors],
                "summary": paper.summary[:500],  # é™åˆ¶æ‘˜è¦é•·åº¦
                "published": str(paper.published),
                "arxiv_id": paper.entry_id.split('/')[-1],
                "arxiv_url": paper.entry_id,
                "pdf_url": paper.pdf_url,
                "categories": [str(cat) for cat in paper.categories],
            })
        
        if not papers:
            return "æœªæ‰¾åˆ°ç›¸é—œè«–æ–‡ã€‚"
        
        # è¿”å›æ ¼å¼åŒ–çš„è«–æ–‡åˆ—è¡¨
        result_text = f"æ‰¾åˆ° {len(papers)} ç¯‡ç›¸é—œè«–æ–‡ï¼š\n\n"
        for i, paper in enumerate(papers, 1):
            result_text += f"{i}. {paper['title']}\n"
            result_text += f"   arXiv ID: {paper['arxiv_id']}\n"
            result_text += f"   ä½œè€…: {', '.join(paper['authors'][:3])}"
            if len(paper['authors']) > 3:
                result_text += f" ç­‰ {len(paper['authors'])} ä½ä½œè€…"
            result_text += "\n"
            result_text += f"   æ‘˜è¦: {paper['summary']}...\n"
            result_text += f"   é€£çµ: {paper['pdf_url']}\n"
            result_text += f"   åˆ†é¡: {', '.join(paper['categories'][:3])}\n\n"
        
        print(f"   âœ… [arXiv] æˆåŠŸæ‰¾åˆ° {len(papers)} ç¯‡è«–æ–‡")
        return result_text
        
    except Exception as e:
        return f"arXiv æœå°‹å¤±æ•—: {e}"


def add_arxiv_papers_to_rag(arxiv_ids_json: str, rag_retriever=None) -> str:
    """
    ä¸‹è¼‰ arXiv è«–æ–‡ä¸¦æ·»åŠ åˆ° RAG ç³»çµ±ä¸­ã€‚
    
    Args:
        arxiv_ids_json: arXiv ID çš„ JSON é™£åˆ—ï¼Œä¾‹å¦‚ï¼š'["2305.10601", "2301.12345"]'
        rag_retriever: RAG æª¢ç´¢å™¨ï¼ˆPrivateFileRAG å¯¦ä¾‹ï¼‰
    
    Returns:
        æ·»åŠ çµæœçš„ç‹€æ…‹è³‡è¨Š
    """
    if not rag_retriever:
        return "RAG ç³»çµ±æœªåˆå§‹åŒ–ã€‚"
    
    try:
        import json
        import arxiv
        import tempfile
        import os
        from ..rag.private_file_rag import PrivateFileRAG
        
        if not isinstance(rag_retriever, PrivateFileRAG):
            return "RAG ç³»çµ±æ ¼å¼ä¸æ­£ç¢ºã€‚"
        
        # è§£æ arXiv IDs
        arxiv_ids = json.loads(arxiv_ids_json)
        if not isinstance(arxiv_ids, list) or not arxiv_ids:
            return "ç„¡æ•ˆçš„ arXiv ID æ ¼å¼ã€‚"
        
        print(f"   ğŸ“¥ [arXiv] æ­£åœ¨ä¸‹è¼‰ {len(arxiv_ids)} ç¯‡è«–æ–‡...")
        
        # ä¸‹è¼‰è«–æ–‡ PDF
        downloaded_files = []
        for arxiv_id in arxiv_ids[:5]:  # é™åˆ¶æœ€å¤š 5 ç¯‡
            try:
                # æœå°‹è«–æ–‡
                search = arxiv.Search(id_list=[arxiv_id])
                paper = next(search.results(), None)
                
                if not paper:
                    print(f"   âš ï¸ æ‰¾ä¸åˆ°è«–æ–‡ {arxiv_id}")
                    continue
                
                # ä¸‹è¼‰ PDF åˆ°è‡¨æ™‚æª”æ¡ˆ
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    paper.download_pdf(dirpath=os.path.dirname(tmp_file.name), filename=os.path.basename(tmp_file.name))
                    downloaded_files.append(tmp_file.name)
                    print(f"   âœ“ å·²ä¸‹è¼‰è«–æ–‡ {arxiv_id}: {paper.title[:50]}...")
                    
            except Exception as e:
                print(f"   âš ï¸ ä¸‹è¼‰è«–æ–‡ {arxiv_id} å¤±æ•—: {e}")
                continue
        
        if not downloaded_files:
            return "æœªèƒ½ä¸‹è¼‰ä»»ä½•è«–æ–‡ã€‚"
        
        print(f"   ğŸ”„ [RAG] æ­£åœ¨å°‡ {len(downloaded_files)} ç¯‡è«–æ–‡æ·»åŠ åˆ° RAG ç³»çµ±...")
        
        # å°‡è«–æ–‡æ·»åŠ åˆ° RAG ç³»çµ±
        documents, status_msg = rag_retriever.process_files(downloaded_files)
        
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        for file_path in downloaded_files:
            try:
                os.unlink(file_path)
            except:
                pass
        
        if documents:
            return f"âœ… æˆåŠŸæ·»åŠ  {len(downloaded_files)} ç¯‡è«–æ–‡åˆ° RAG ç³»çµ±ã€‚{status_msg}"
        else:
            return f"âš ï¸ è«–æ–‡ä¸‹è¼‰æˆåŠŸä½†è™•ç†å¤±æ•—ï¼š{status_msg}"
            
    except Exception as e:
        return f"æ·»åŠ è«–æ–‡å¤±æ•—: {e}"


def get_tools_list(rag_retriever=None):
    """
    ç²å–å·¥å…·åˆ—è¡¨
    æ³¨æ„ï¼šéƒ¨åˆ†å·¥å…·éœ€è¦ rag_retrieverï¼Œæ‰€ä»¥éœ€è¦å‹•æ…‹å‰µå»º
    """
    # å‰µå»ºå¸¶æœ‰ rag_retriever çš„å·¥å…·åŒ…è£å™¨
    if rag_retriever:
        def query_pdf_wrapper(query: str) -> str:
            """
            æŸ¥è©¢ PDF çŸ¥è­˜åº«ä¸­çš„ç›¸é—œè³‡è¨Šã€‚
            ç•¶å•é¡Œæ¶‰åŠè«–æ–‡å…§å®¹ã€ç ”ç©¶æ¦‚å¿µã€æ–¹æ³•è«–æˆ–å­¸è¡“ç†è«–æ™‚ä½¿ç”¨æ­¤å·¥å…·ã€‚
            æ”¯æŒå¤šæ–‡ä»¶æª¢ç´¢å’Œé€²éš RAG æ–¹æ³•ã€‚
            
            Args:
                query: æŸ¥è©¢å•é¡Œ
            
            Returns:
                åŸºæ–¼ PDF çŸ¥è­˜åº«çš„å›ç­”
            """
            return query_pdf_knowledge(query, rag_retriever=rag_retriever)
        
        def extract_keywords_wrapper(query: str) -> str:
            """
            å¾ PDF çŸ¥è­˜åº«ä¸­æå–å­¸è¡“é—œéµå­—ï¼Œç”¨æ–¼ arXiv æœå°‹ã€‚
            ç•¶éœ€è¦æŸ¥æ‰¾ç›¸é—œå­¸è¡“è«–æ–‡æ™‚ä½¿ç”¨æ­¤å·¥å…·ã€‚
            
            Args:
                query: æŸ¥è©¢å•é¡Œ
            
            Returns:
                é—œéµå­— JSON é™£åˆ—å­—ä¸²
            """
            return extract_keywords_from_pdf(query, rag_retriever=rag_retriever)
        
        def add_arxiv_papers_wrapper(arxiv_ids_json: str) -> str:
            """
            ä¸‹è¼‰ arXiv è«–æ–‡ä¸¦æ·»åŠ åˆ° RAG ç³»çµ±ä¸­ã€‚
            
            Args:
                arxiv_ids_json: arXiv ID çš„ JSON é™£åˆ—å­—ä¸²
            
            Returns:
                æ·»åŠ çµæœçš„ç‹€æ…‹è³‡è¨Š
            """
            return add_arxiv_papers_to_rag(arxiv_ids_json, rag_retriever=rag_retriever)
        
        # å‰µå»ºå·¥å…·
        pdf_tool = tool(query_pdf_wrapper)
        pdf_tool.name = "query_pdf_knowledge"
        
        keywords_tool = tool(extract_keywords_wrapper)
        keywords_tool.name = "extract_keywords_from_pdf"
        
        arxiv_search_tool = tool(search_arxiv_papers)
        arxiv_search_tool.name = "search_arxiv_papers"
        
        add_papers_tool = tool(add_arxiv_papers_wrapper)
        add_papers_tool.name = "add_arxiv_papers_to_rag"
        
        return [
            get_company_deep_info, 
            search_web, 
            pdf_tool,
            keywords_tool,
            arxiv_search_tool,
            add_papers_tool
        ]
    else:
        return [get_company_deep_info, search_web]


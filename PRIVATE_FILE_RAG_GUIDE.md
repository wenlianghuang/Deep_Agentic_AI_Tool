# 私有檔案 RAG 功能使用指南

## 總覽

本功能整合了 `Learn_RAG` 專案的 RAG 系統，讓使用者可以上傳私有檔案（如 PDF、DOCX、TXT），並基於這些檔案內容進行智慧問答。系統採用了先進的混合檢索與 LLM 技術，以提供準確、相關的回答。

## 功能特色

- ✅ **支援多種檔案格式**：PDF、DOCX、DOC、TXT。
- ✅ **支援多檔案上傳**：可一次上傳並處理多個檔案。
- ✅ **混合檢索**：結合 BM25（關鍵字檢索）與向量檢索（語義檢索），大幅提升檢索準確度。
- ✅ **可選重排序**：使用 BGE Reranker 模型進一步優化檢索結果的相關性。
- ✅ **兩種分塊模式**：
    - **語義分塊（推薦）**：智慧切分文件，確保語義完整性，不會在句子中間斷開，提升檢索品質。
    - **字元分塊**：依固定字數切分，處理速度快，適合快速測試。
- ✅ **智慧回答生成**：
    - **LLM 自動切換策略**：系統會自動根據可用性選擇最佳的 LLM，優先順序為：**Groq API > Ollama > MLX 本地模型**。
    - **自動化提示工程**：自動偵測檔案類型（如學術論文、履歷、通用文件）以調整提問風格，生成更貼切的回答。
- ✅ **支援中英文問答**。

## 使用方法

### 1. 準備工作

確保 `Learn_RAG` 專案與本專案 (`Deep_Agentic_AI_Tool`) 位於**同一個父目錄**下。正確的目錄結構應如下：

```
/some_parent_directory/
├─── Deep_Agentic_AI_Tool/ (本專案)
└─── Learn_RAG/
```

此外，請確保 `Learn_RAG` 的依賴已安裝。您可以在 `Learn_RAG` 目錄下執行 `uv sync` 或 `pip install -r requirements.txt`。

### 2. 啟動系統

執行主程式以啟動 Gradio 網頁介面：
```bash
python Deep_Agent_Gradio_RAG_localLLM_main.py
```

### 3. 使用步驟

1.  **開啟介面**：在瀏覽器中開啟 Gradio 介面（預設為 `http://0.0.0.0:7860`），並點擊 **"📚 Private File RAG"** 標籤頁。

2.  **上傳檔案**：點擊或拖曳檔案至 **"📁 上傳檔案"** 區域，可選擇一個或多個 PDF、DOCX 或 TXT 檔案。

3.  **設定分塊模式**：
    - **使用語義分塊（推薦）**：勾選此選項以獲得最佳的檢索品質。處理時間較長，但效果最好。
    - **調整參數（可選）**：介面提供了對兩種分塊模式的進階參數調整，您可以根據需求調整，或直接使用已優化的預設值。

4.  **處理檔案**：點擊 **"📝 處理檔案"** 按鈕。系統會根據您的設定進行分塊、建立索引並初始化 RAG 系統。請等待處理狀態顯示 "✅ 文件處理完成"。
    - **注意**：首次使用時，系統需要下載 Embedding 模型，可能需要數分鐘時間，請耐心等候。

5.  **提出問題**：在 **"❓ 請輸入您的問題"** 輸入框中，輸入您想詢問關於檔案內容的問題。

6.  **調整查詢選項**：
    - **返回結果數量**：可調整檢索到的相關文件片段數量（預設為 3）。
    - **使用 LLM 生成回答**：勾選此項，AI 會總結檢索到的內容並生成流暢的回答。若取消勾選，則僅顯示原始的文件片段。

7.  **執行查詢**：點擊 **"🔍 查詢"** 按鈕。

8.  **檢視結果**：
    - **💬 AI 回答**：顯示由 LLM 生成的最終回答。
    - **📄 檢索到的文件片段**：顯示用於生成回答的原始文件內容、來源及相關性分數，方便您查證。

9.  **清除與重置**：點擊 **"🗑️ 清除"** 按鈕可重設當前會話，讓您重新上傳檔案。

## 技術細節

### LLM 使用策略

本系統採用彈性的 LLM 調度策略，無需手動設定：
1.  🥇 **Groq API**：若您在環境變數中設定了 `GROQ_API_KEY`，系統會優先使用速度極快的 Groq API。
2.  🥈 **Ollama**：若 Groq API 不可用或額度用盡，系統會自動切換至本地運行的 Ollama 模型（如 Llama3.2）。
3.  🥉 **MLX**：若前兩者皆不可用，系統會使用 Apple MLX 在本地運行模型（如 Qwen2.5）作為最終備案。

### 分塊模式詳解

- **字元分塊**（預設 `chunk_size: 500`, `chunk_overlap: 100`）：
  - **優點**：處理速度快。
  - **適用場景**：快速測試、對語義完整性要求不高的文件。
  - **參數說明**：
    - `分塊大小`：每個區塊的字元數。較小值粒度更細，較大值上下文更完整。
    - `分塊重疊`：相鄰區塊間重疊的字元數，有助於保持上下文連貫。

- **語義分塊**（預設 `threshold: 1.0`, `min_chunk_size: 100`）：
  - **優點**：根據語義邊界切分，能保持句子和段落的完整性，檢索品質更高。
  - **適用場景**：專業文件、報告、論文等需要精準理解上下文的場景。
  - **參數說明**：
    - `語義分塊閾值`：控制分塊的敏感度。數值越小，分塊越細。建議值為 0.8-1.2。
    - `最小分塊大小`：過小的區塊會被合併，以避免碎片化。

### 檢索系統

1.  **Embedding 模型**：使用 `sentence-transformers/all-MiniLM-L6-v2` 將文字轉換為向量，此模型輕量且高效。
2.  **向量資料庫**：使用 `ChromaDB` 儲存並索引向量，資料庫會持久化儲存於 `./chroma_db_private` 目錄。
3.  **混合檢索**：結合 `BM25`（基於關鍵字）和 `向量檢索`（基於語義），並透過 `RRF` (Reciprocal Rank Fusion) 演算法融合結果，兼顧關鍵字匹配和語義相似度。
4.  **重排序器（Reranker）**：使用 `BAAI/bge-reranker-base` 模型對混合檢索的結果進行二次排序，將最相關的片段排在最前面，極大化提升了最終答案的品質。

## 常見問題

### Q: 處理檔案時出錯或提示 `Learn_RAG` 模組不可用？
**A:** 請檢查：
1.  **專案位置**：確保 `Learn_RAG` 專案目錄與 `Deep_Agentic_AI_Tool` 位於同一父目錄下。
2.  **依賴安裝**：確認您已安裝 `Learn_RAG` 的所有 Python 依賴。最簡單的方式是進入 `Learn_RAG` 目錄並執行 `uv sync`。
3.  **檔案格式**：確認您上傳的是支援的格式（PDF, DOCX, DOC, TXT）且檔案未損毀。

### Q: AI 無法生成回答，或回答很慢？
**A:** 請檢查：
1.  **LLM 服務**：如果您想使用 Ollama，請確保 Ollama 服務正在本地運行（可透過終端機執行 `ollama serve`）。
2.  **模型下載**：確認 Ollama 需要的模型已經下載（如 `ollama pull llama3.2:3b`）。
3.  **LLM 狀態**：系統會自動從 Groq 切換至本地模型。若切換至 MLX，處理速度會較慢，請耐心等待。若不需生成式回答，可取消勾選 "使用 LLM 生成回答" 以直接查看檢索結果。

### Q: "清除" 按鈕的功能是什麼？
**A:** "清除" 按鈕會重設當前 Gradio 會話中的 RAG 系統，清空已上傳的檔案和記憶體中的索引。這讓您可以重新上傳並處理新的一批檔案。
**注意**：此按鈕**不會**刪除磁碟上 `./chroma_db_private` 目錄中持久化的向量資料庫。若要完全清空所有資料，您需要手動刪除該目錄。

### Q: 處理檔案速度很慢？
**A:** 
1.  首次使用時，系統需要下載數百 MB 的 Embedding 模型，請耐心等待。
2.  語義分塊模式會進行複雜的計算，處理時間比字元分塊長，但效果更好。
3.  檔案越大、數量越多，處理時間越長。

## 依賴項

- `Learn_RAG` 專案及其所有依賴項。
- `langchain-community`, `sentence-transformers`, `chromadb`, `rank_bm25`, `pypdf`, `docx2txt` 等。
- `ollama` (若使用本地 Ollama LLM)。